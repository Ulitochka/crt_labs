2018-08-10 02:06:54,352:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': '009',
               'momentum': 0.2,
               'type': 'LBFGS',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': False,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 1, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 02:08:31,921:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'ExponentialLR',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': '009',
               'momentum': 0.2,
               'type': 'LBFGS',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': False,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 1, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 02:11:50,143:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'ReduceLROnPlateau',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': '009',
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': False,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 1, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 02:15:24,696:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': '009',
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': False,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 1, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 02:19:41,642:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 1, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 02:20:45,979:CRITICAL:EmoV2_step40: iteration: 0: Loss: 1.003193, lr: 0.000965
2018-08-10 02:28:50,422:CRITICAL:EmoV2_step40: validate. Iteration: 0: Accuracy (valence, arousal): 26.991% 3.097%
2018-08-10 02:28:50,423:CRITICAL:EmoV2_step40: validate. Iteration: 0: Loss: 1.000109
2018-08-10 02:29:21,823:CRITICAL:EmoV2_step40: iteration: 100: Loss: 1.166441, lr: 0.001030
2018-08-10 02:29:53,314:CRITICAL:EmoV2_step40: iteration: 200: Loss: 1.076861, lr: 0.001094
2018-08-10 02:30:24,799:CRITICAL:EmoV2_step40: iteration: 300: Loss: 0.829984, lr: 0.001159
2018-08-10 02:30:56,098:CRITICAL:EmoV2_step40: iteration: 400: Loss: 0.586444, lr: 0.001224
2018-08-10 02:31:27,445:CRITICAL:EmoV2_step40: iteration: 500: Loss: 0.868674, lr: 0.001289
2018-08-10 02:31:58,919:CRITICAL:EmoV2_step40: iteration: 600: Loss: 0.523545, lr: 0.001354
2018-08-10 02:32:30,326:CRITICAL:EmoV2_step40: iteration: 700: Loss: 0.904411, lr: 0.001418
2018-08-10 02:33:01,661:CRITICAL:EmoV2_step40: iteration: 800: Loss: 0.776901, lr: 0.001483
2018-08-10 02:33:33,211:CRITICAL:EmoV2_step40: iteration: 900: Loss: 0.626423, lr: 0.001548
2018-08-10 02:34:04,669:CRITICAL:EmoV2_step40: iteration: 1000: Loss: 0.904032, lr: 0.001613
2018-08-10 02:42:10,263:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Accuracy (valence, arousal): 18.142% 16.372%
2018-08-10 02:42:10,263:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Loss: 0.999578
2018-08-10 02:42:41,731:CRITICAL:EmoV2_step40: iteration: 1100: Loss: 0.752743, lr: 0.001678
2018-08-10 02:43:13,170:CRITICAL:EmoV2_step40: iteration: 1200: Loss: 0.746295, lr: 0.001742
2018-08-10 02:43:44,670:CRITICAL:EmoV2_step40: iteration: 1300: Loss: 0.822244, lr: 0.001807
2018-08-10 02:44:16,136:CRITICAL:EmoV2_step40: iteration: 1400: Loss: 0.426014, lr: 0.001872
2018-08-10 02:44:47,592:CRITICAL:EmoV2_step40: iteration: 1500: Loss: 0.571451, lr: 0.001937
2018-08-10 02:45:18,919:CRITICAL:EmoV2_step40: iteration: 1600: Loss: 0.942434, lr: 0.002002
2018-08-10 02:45:50,339:CRITICAL:EmoV2_step40: iteration: 1700: Loss: 0.481774, lr: 0.002066
2018-08-10 02:46:21,824:CRITICAL:EmoV2_step40: iteration: 1800: Loss: 0.761885, lr: 0.002131
2018-08-10 02:46:53,242:CRITICAL:EmoV2_step40: iteration: 1900: Loss: 0.791509, lr: 0.002196
2018-08-10 02:47:24,537:CRITICAL:EmoV2_step40: iteration: 2000: Loss: 0.757879, lr: 0.002261
2018-08-10 02:55:30,453:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Accuracy (valence, arousal): 9.292% 12.832%
2018-08-10 02:55:30,453:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Loss: 0.999842
2018-08-10 02:56:01,938:CRITICAL:EmoV2_step40: iteration: 2100: Loss: 0.585888, lr: 0.002326
2018-08-10 02:56:33,437:CRITICAL:EmoV2_step40: iteration: 2200: Loss: 0.608979, lr: 0.002390
2018-08-10 02:57:04,864:CRITICAL:EmoV2_step40: iteration: 2300: Loss: 0.532929, lr: 0.002455
2018-08-10 02:57:36,173:CRITICAL:EmoV2_step40: iteration: 2400: Loss: 0.879000, lr: 0.002520
2018-08-10 02:58:07,691:CRITICAL:EmoV2_step40: iteration: 2500: Loss: 0.647069, lr: 0.002585
2018-08-10 02:58:39,065:CRITICAL:EmoV2_step40: iteration: 2600: Loss: 0.616963, lr: 0.002650
2018-08-10 02:59:10,406:CRITICAL:EmoV2_step40: iteration: 2700: Loss: 0.507229, lr: 0.002714
2018-08-10 02:59:41,906:CRITICAL:EmoV2_step40: iteration: 2800: Loss: 0.582922, lr: 0.002779
2018-08-10 03:00:13,393:CRITICAL:EmoV2_step40: iteration: 2900: Loss: 0.591309, lr: 0.002844
2018-08-10 03:00:44,868:CRITICAL:EmoV2_step40: iteration: 3000: Loss: 0.572807, lr: 0.002909
2018-08-10 03:08:50,134:CRITICAL:EmoV2_step40: validate. Iteration: 3000: Accuracy (valence, arousal): 11.062% 21.681%
2018-08-10 03:08:50,134:CRITICAL:EmoV2_step40: validate. Iteration: 3000: Loss: 0.999921
2018-08-10 03:09:21,630:CRITICAL:EmoV2_step40: iteration: 3100: Loss: 0.486522, lr: 0.002974
2018-08-10 03:09:53,184:CRITICAL:EmoV2_step40: iteration: 3200: Loss: 0.315019, lr: 0.003038
2018-08-10 03:10:24,609:CRITICAL:EmoV2_step40: iteration: 3300: Loss: 0.747888, lr: 0.003103
2018-08-10 03:10:55,912:CRITICAL:EmoV2_step40: iteration: 3400: Loss: 0.794875, lr: 0.003168
2018-08-10 03:11:27,308:CRITICAL:EmoV2_step40: iteration: 3500: Loss: 0.524496, lr: 0.003233
2018-08-10 03:11:58,607:CRITICAL:EmoV2_step40: iteration: 3600: Loss: 0.724262, lr: 0.003298
2018-08-10 03:12:30,090:CRITICAL:EmoV2_step40: iteration: 3700: Loss: 0.738490, lr: 0.003362
2018-08-10 03:13:01,567:CRITICAL:EmoV2_step40: iteration: 3800: Loss: 0.748686, lr: 0.003427
2018-08-10 03:13:33,076:CRITICAL:EmoV2_step40: iteration: 3900: Loss: 0.512982, lr: 0.003492
2018-08-10 03:14:04,604:CRITICAL:EmoV2_step40: iteration: 4000: Loss: 0.595031, lr: 0.003557
2018-08-10 03:22:10,939:CRITICAL:EmoV2_step40: validate. Iteration: 4000: Accuracy (valence, arousal): 15.929% 14.602%
2018-08-10 03:22:10,939:CRITICAL:EmoV2_step40: validate. Iteration: 4000: Loss: 0.999151
2018-08-10 03:22:42,275:CRITICAL:EmoV2_step40: iteration: 4100: Loss: 0.678100, lr: 0.003622
2018-08-10 03:23:13,418:CRITICAL:EmoV2_step40: iteration: 4200: Loss: 0.449108, lr: 0.003686
2018-08-10 03:23:44,923:CRITICAL:EmoV2_step40: iteration: 4300: Loss: 0.545216, lr: 0.003751
2018-08-10 03:24:16,501:CRITICAL:EmoV2_step40: iteration: 4400: Loss: 1.001667, lr: 0.003816
2018-08-10 03:24:47,930:CRITICAL:EmoV2_step40: iteration: 4500: Loss: 0.760783, lr: 0.003881
2018-08-10 03:25:19,403:CRITICAL:EmoV2_step40: iteration: 4600: Loss: 0.765148, lr: 0.003946
2018-08-10 03:25:50,943:CRITICAL:EmoV2_step40: iteration: 4700: Loss: 0.641453, lr: 0.004010
2018-08-10 03:26:22,368:CRITICAL:EmoV2_step40: iteration: 4800: Loss: 0.442016, lr: 0.004075
2018-08-10 03:26:53,476:CRITICAL:EmoV2_step40: iteration: 4900: Loss: 0.450945, lr: 0.004140
2018-08-10 03:27:24,882:CRITICAL:EmoV2_step40: iteration: 5000: Loss: 0.648426, lr: 0.004205
2018-08-10 03:35:30,984:CRITICAL:EmoV2_step40: validate. Iteration: 5000: Accuracy (valence, arousal): 7.965% 5.310%
2018-08-10 03:35:30,984:CRITICAL:EmoV2_step40: validate. Iteration: 5000: Loss: 0.999748
2018-08-10 03:36:02,429:CRITICAL:EmoV2_step40: iteration: 5100: Loss: 0.468563, lr: 0.004270
2018-08-10 03:36:33,653:CRITICAL:EmoV2_step40: iteration: 5200: Loss: 0.667267, lr: 0.004334
2018-08-10 03:37:05,127:CRITICAL:EmoV2_step40: iteration: 5300: Loss: 0.873249, lr: 0.004399
2018-08-10 03:37:36,554:CRITICAL:EmoV2_step40: iteration: 5400: Loss: 0.788574, lr: 0.004464
2018-08-10 03:38:08,000:CRITICAL:EmoV2_step40: iteration: 5500: Loss: 0.391074, lr: 0.004529
2018-08-10 03:38:39,423:CRITICAL:EmoV2_step40: iteration: 5600: Loss: 0.674609, lr: 0.004594
2018-08-10 03:39:10,873:CRITICAL:EmoV2_step40: iteration: 5700: Loss: 0.765231, lr: 0.004658
2018-08-10 03:39:42,313:CRITICAL:EmoV2_step40: iteration: 5800: Loss: 0.597784, lr: 0.004723
2018-08-10 03:40:13,765:CRITICAL:EmoV2_step40: iteration: 5900: Loss: 0.564205, lr: 0.004788
2018-08-10 03:40:45,292:CRITICAL:EmoV2_step40: iteration: 6000: Loss: 0.581769, lr: 0.004853
2018-08-10 03:48:51,542:CRITICAL:EmoV2_step40: validate. Iteration: 6000: Accuracy (valence, arousal): 27.876% 11.504%
2018-08-10 03:48:51,543:CRITICAL:EmoV2_step40: validate. Iteration: 6000: Loss: 0.999716
2018-08-10 03:49:22,995:CRITICAL:EmoV2_step40: iteration: 6100: Loss: 0.578820, lr: 0.004918
2018-08-10 03:49:54,133:CRITICAL:EmoV2_step40: iteration: 6200: Loss: 0.643633, lr: 0.004982
2018-08-10 03:50:25,433:CRITICAL:EmoV2_step40: iteration: 6300: Loss: 0.644307, lr: 0.005047
2018-08-10 03:50:56,828:CRITICAL:EmoV2_step40: iteration: 6400: Loss: 0.552381, lr: 0.005112
2018-08-10 03:51:28,283:CRITICAL:EmoV2_step40: iteration: 6500: Loss: 0.626733, lr: 0.005177
2018-08-10 03:51:59,748:CRITICAL:EmoV2_step40: iteration: 6600: Loss: 0.621752, lr: 0.005242
2018-08-10 03:52:31,237:CRITICAL:EmoV2_step40: iteration: 6700: Loss: 0.786060, lr: 0.005306
2018-08-10 03:53:02,697:CRITICAL:EmoV2_step40: iteration: 6800: Loss: 0.857171, lr: 0.005371
2018-08-10 03:53:34,126:CRITICAL:EmoV2_step40: iteration: 6900: Loss: 0.947319, lr: 0.005436
2018-08-10 03:54:05,264:CRITICAL:EmoV2_step40: iteration: 7000: Loss: 0.556042, lr: 0.005501
2018-08-10 04:02:11,676:CRITICAL:EmoV2_step40: validate. Iteration: 7000: Accuracy (valence, arousal): 28.319% 19.027%
2018-08-10 04:02:11,677:CRITICAL:EmoV2_step40: validate. Iteration: 7000: Loss: 0.999446
2018-08-10 04:02:43,109:CRITICAL:EmoV2_step40: iteration: 7100: Loss: 0.835305, lr: 0.005566
2018-08-10 04:03:14,561:CRITICAL:EmoV2_step40: iteration: 7200: Loss: 0.488833, lr: 0.005630
2018-08-10 04:03:46,076:CRITICAL:EmoV2_step40: iteration: 7300: Loss: 0.450332, lr: 0.005695
2018-08-10 04:04:17,510:CRITICAL:EmoV2_step40: iteration: 7400: Loss: 0.502291, lr: 0.005760
2018-08-10 04:04:49,024:CRITICAL:EmoV2_step40: iteration: 7500: Loss: 0.541577, lr: 0.005825
2018-08-10 04:05:20,451:CRITICAL:EmoV2_step40: iteration: 7600: Loss: 0.500994, lr: 0.005890
2018-08-10 04:05:52,008:CRITICAL:EmoV2_step40: iteration: 7700: Loss: 0.451977, lr: 0.005954
2018-08-10 04:06:23,457:CRITICAL:EmoV2_step40: iteration: 7800: Loss: 0.488978, lr: 0.006019
2018-08-10 04:06:54,885:CRITICAL:EmoV2_step40: iteration: 7900: Loss: 0.770007, lr: 0.006084
2018-08-10 04:07:26,396:CRITICAL:EmoV2_step40: iteration: 8000: Loss: 0.854848, lr: 0.006149
2018-08-10 04:15:32,331:CRITICAL:EmoV2_step40: validate. Iteration: 8000: Accuracy (valence, arousal): 29.646% 11.947%
2018-08-10 04:15:32,332:CRITICAL:EmoV2_step40: validate. Iteration: 8000: Loss: 0.999474
2018-08-10 04:16:03,665:CRITICAL:EmoV2_step40: iteration: 8100: Loss: 0.398297, lr: 0.006214
2018-08-10 04:16:35,158:CRITICAL:EmoV2_step40: iteration: 8200: Loss: 0.736941, lr: 0.006278
2018-08-10 04:17:06,696:CRITICAL:EmoV2_step40: iteration: 8300: Loss: 0.371528, lr: 0.006343
2018-08-10 04:17:38,165:CRITICAL:EmoV2_step40: iteration: 8400: Loss: 0.582964, lr: 0.006408
2018-08-10 04:18:09,593:CRITICAL:EmoV2_step40: iteration: 8500: Loss: 0.446310, lr: 0.006473
2018-08-10 04:18:41,052:CRITICAL:EmoV2_step40: iteration: 8600: Loss: 0.789405, lr: 0.006538
2018-08-10 04:19:12,491:CRITICAL:EmoV2_step40: iteration: 8700: Loss: 0.724455, lr: 0.006602
2018-08-10 04:19:43,993:CRITICAL:EmoV2_step40: iteration: 8800: Loss: 0.359078, lr: 0.006667
2018-08-10 04:20:15,162:CRITICAL:EmoV2_step40: iteration: 8900: Loss: 0.367524, lr: 0.006732
2018-08-10 04:20:46,514:CRITICAL:EmoV2_step40: iteration: 9000: Loss: 0.398295, lr: 0.006797
2018-08-10 04:28:52,793:CRITICAL:EmoV2_step40: validate. Iteration: 9000: Accuracy (valence, arousal): 20.354% 23.009%
2018-08-10 04:28:52,794:CRITICAL:EmoV2_step40: validate. Iteration: 9000: Loss: 0.999232
2018-08-10 04:29:24,203:CRITICAL:EmoV2_step40: iteration: 9100: Loss: 0.345164, lr: 0.006862
2018-08-10 04:29:55,685:CRITICAL:EmoV2_step40: iteration: 9200: Loss: 0.802285, lr: 0.006926
2018-08-10 04:30:27,161:CRITICAL:EmoV2_step40: iteration: 9300: Loss: 0.368170, lr: 0.006991
2018-08-10 04:30:58,690:CRITICAL:EmoV2_step40: iteration: 9400: Loss: 0.496062, lr: 0.007056
2018-08-10 04:31:30,064:CRITICAL:EmoV2_step40: iteration: 9500: Loss: 0.526971, lr: 0.007121
2018-08-10 04:32:01,229:CRITICAL:EmoV2_step40: iteration: 9600: Loss: 0.498567, lr: 0.007186
2018-08-10 04:32:32,677:CRITICAL:EmoV2_step40: iteration: 9700: Loss: 0.555915, lr: 0.007250
2018-08-10 04:33:04,232:CRITICAL:EmoV2_step40: iteration: 9800: Loss: 0.671554, lr: 0.007315
2018-08-10 04:33:35,677:CRITICAL:EmoV2_step40: iteration: 9900: Loss: 0.411837, lr: 0.007380
2018-08-10 04:34:49,805:CRITICAL:EmoV2_step40: iteration: 10000: Loss: 0.635009, lr: 0.007445
2018-08-10 04:42:55,550:CRITICAL:EmoV2_step40: validate. Iteration: 10000: Accuracy (valence, arousal): 18.142% 23.451%
2018-08-10 04:42:55,551:CRITICAL:EmoV2_step40: validate. Iteration: 10000: Loss: 0.999715
2018-08-10 04:43:27,023:CRITICAL:EmoV2_step40: iteration: 10100: Loss: 0.462245, lr: 0.007510
2018-08-10 04:43:58,442:CRITICAL:EmoV2_step40: iteration: 10200: Loss: 0.359302, lr: 0.007574
2018-08-10 04:44:29,849:CRITICAL:EmoV2_step40: iteration: 10300: Loss: 0.220934, lr: 0.007639
2018-08-10 04:45:01,362:CRITICAL:EmoV2_step40: iteration: 10400: Loss: 0.570678, lr: 0.007704
2018-08-10 04:45:32,892:CRITICAL:EmoV2_step40: iteration: 10500: Loss: 0.438502, lr: 0.007769
2018-08-10 04:46:04,405:CRITICAL:EmoV2_step40: iteration: 10600: Loss: 0.282928, lr: 0.007834
2018-08-10 04:46:35,642:CRITICAL:EmoV2_step40: iteration: 10700: Loss: 0.852080, lr: 0.007898
2018-08-10 04:47:06,736:CRITICAL:EmoV2_step40: iteration: 10800: Loss: 0.440383, lr: 0.007963
2018-08-10 04:47:37,858:CRITICAL:EmoV2_step40: iteration: 10900: Loss: 0.466841, lr: 0.008028
2018-08-10 04:48:08,965:CRITICAL:EmoV2_step40: iteration: 11000: Loss: 0.373986, lr: 0.008093
2018-08-10 04:56:15,336:CRITICAL:EmoV2_step40: validate. Iteration: 11000: Accuracy (valence, arousal): 15.487% 19.469%
2018-08-10 04:56:15,336:CRITICAL:EmoV2_step40: validate. Iteration: 11000: Loss: 0.999107
2018-08-10 04:56:46,903:CRITICAL:EmoV2_step40: iteration: 11100: Loss: 0.214799, lr: 0.008158
2018-08-10 04:57:18,152:CRITICAL:EmoV2_step40: iteration: 11200: Loss: 0.339375, lr: 0.008222
2018-08-10 04:57:49,257:CRITICAL:EmoV2_step40: iteration: 11300: Loss: 0.354869, lr: 0.008287
2018-08-10 04:58:20,544:CRITICAL:EmoV2_step40: iteration: 11400: Loss: 0.314714, lr: 0.008352
2018-08-10 04:58:52,223:CRITICAL:EmoV2_step40: iteration: 11500: Loss: 0.261410, lr: 0.008417
2018-08-10 04:59:23,708:CRITICAL:EmoV2_step40: iteration: 11600: Loss: 0.253473, lr: 0.008482
2018-08-10 04:59:55,135:CRITICAL:EmoV2_step40: iteration: 11700: Loss: 0.475894, lr: 0.008546
2018-08-10 05:00:26,568:CRITICAL:EmoV2_step40: iteration: 11800: Loss: 0.566119, lr: 0.008611
2018-08-10 05:00:57,747:CRITICAL:EmoV2_step40: iteration: 11900: Loss: 0.450171, lr: 0.008676
2018-08-10 05:01:28,939:CRITICAL:EmoV2_step40: iteration: 12000: Loss: 0.359572, lr: 0.008741
2018-08-10 05:09:35,124:CRITICAL:EmoV2_step40: validate. Iteration: 12000: Accuracy (valence, arousal): 26.106% 21.239%
2018-08-10 05:09:35,125:CRITICAL:EmoV2_step40: validate. Iteration: 12000: Loss: 0.999516
2018-08-10 05:10:06,590:CRITICAL:EmoV2_step40: iteration: 12100: Loss: 0.225055, lr: 0.008806
2018-08-10 05:10:37,720:CRITICAL:EmoV2_step40: iteration: 12200: Loss: 0.733303, lr: 0.008870
2018-08-10 05:11:09,112:CRITICAL:EmoV2_step40: iteration: 12300: Loss: 0.405735, lr: 0.008935
2018-08-10 05:11:40,582:CRITICAL:EmoV2_step40: iteration: 12400: Loss: 0.318805, lr: 0.009000
2018-08-10 05:12:12,022:CRITICAL:EmoV2_step40: iteration: 12500: Loss: 0.470289, lr: 0.008984
2018-08-10 05:12:43,482:CRITICAL:EmoV2_step40: iteration: 12600: Loss: 0.498726, lr: 0.008968
2018-08-10 05:13:14,993:CRITICAL:EmoV2_step40: iteration: 12700: Loss: 0.436670, lr: 0.008951
2018-08-10 05:13:46,505:CRITICAL:EmoV2_step40: iteration: 12800: Loss: 0.232873, lr: 0.008935
2018-08-10 05:14:18,036:CRITICAL:EmoV2_step40: iteration: 12900: Loss: 0.302169, lr: 0.008919
2018-08-10 05:14:49,608:CRITICAL:EmoV2_step40: iteration: 13000: Loss: 0.338367, lr: 0.008903
2018-08-10 05:22:56,246:CRITICAL:EmoV2_step40: validate. Iteration: 13000: Accuracy (valence, arousal): 29.646% 21.239%
2018-08-10 05:22:56,247:CRITICAL:EmoV2_step40: validate. Iteration: 13000: Loss: 0.999126
2018-08-10 05:23:27,501:CRITICAL:EmoV2_step40: iteration: 13100: Loss: 0.388962, lr: 0.008887
2018-08-10 05:23:58,887:CRITICAL:EmoV2_step40: iteration: 13200: Loss: 0.373915, lr: 0.008870
2018-08-10 05:24:30,016:CRITICAL:EmoV2_step40: iteration: 13300: Loss: 0.378859, lr: 0.008854
2018-08-10 05:25:01,196:CRITICAL:EmoV2_step40: iteration: 13400: Loss: 0.324791, lr: 0.008838
2018-08-10 05:25:32,440:CRITICAL:EmoV2_step40: iteration: 13500: Loss: 0.350580, lr: 0.008822
2018-08-10 05:26:03,941:CRITICAL:EmoV2_step40: iteration: 13600: Loss: 0.859775, lr: 0.008806
2018-08-10 05:26:35,456:CRITICAL:EmoV2_step40: iteration: 13700: Loss: 0.320195, lr: 0.008789
2018-08-10 05:27:06,937:CRITICAL:EmoV2_step40: iteration: 13800: Loss: 0.298928, lr: 0.008773
2018-08-10 05:27:38,459:CRITICAL:EmoV2_step40: iteration: 13900: Loss: 0.319061, lr: 0.008757
2018-08-10 05:28:09,966:CRITICAL:EmoV2_step40: iteration: 14000: Loss: 0.416310, lr: 0.008741
2018-08-10 05:36:15,865:CRITICAL:EmoV2_step40: validate. Iteration: 14000: Accuracy (valence, arousal): 30.973% 18.142%
2018-08-10 05:36:15,866:CRITICAL:EmoV2_step40: validate. Iteration: 14000: Loss: 0.998712
2018-08-10 05:36:47,371:CRITICAL:EmoV2_step40: iteration: 14100: Loss: 0.219693, lr: 0.008725
2018-08-10 05:37:18,817:CRITICAL:EmoV2_step40: iteration: 14200: Loss: 0.345805, lr: 0.008708
2018-08-10 05:37:50,098:CRITICAL:EmoV2_step40: iteration: 14300: Loss: 0.386504, lr: 0.008692
2018-08-10 05:38:21,515:CRITICAL:EmoV2_step40: iteration: 14400: Loss: 0.556535, lr: 0.008676
2018-08-10 05:38:52,985:CRITICAL:EmoV2_step40: iteration: 14500: Loss: 0.475585, lr: 0.008660
2018-08-10 05:39:24,469:CRITICAL:EmoV2_step40: iteration: 14600: Loss: 0.438595, lr: 0.008644
2018-08-10 05:39:56,079:CRITICAL:EmoV2_step40: iteration: 14700: Loss: 0.327552, lr: 0.008627
2018-08-10 05:40:27,558:CRITICAL:EmoV2_step40: iteration: 14800: Loss: 0.240556, lr: 0.008611
2018-08-10 05:40:59,082:CRITICAL:EmoV2_step40: iteration: 14900: Loss: 0.288744, lr: 0.008595
2018-08-10 05:41:30,542:CRITICAL:EmoV2_step40: iteration: 15000: Loss: 0.513686, lr: 0.008579
2018-08-10 05:49:36,751:CRITICAL:EmoV2_step40: validate. Iteration: 15000: Accuracy (valence, arousal): 28.319% 19.912%
2018-08-10 05:49:36,751:CRITICAL:EmoV2_step40: validate. Iteration: 15000: Loss: 0.998791
2018-08-10 05:50:08,308:CRITICAL:EmoV2_step40: iteration: 15100: Loss: 0.358190, lr: 0.008563
2018-08-10 05:50:40,013:CRITICAL:EmoV2_step40: iteration: 15200: Loss: 0.276657, lr: 0.008546
2018-08-10 05:51:11,609:CRITICAL:EmoV2_step40: iteration: 15300: Loss: 0.372767, lr: 0.008530
2018-08-10 05:51:43,079:CRITICAL:EmoV2_step40: iteration: 15400: Loss: 0.263400, lr: 0.008514
2018-08-10 05:52:14,570:CRITICAL:EmoV2_step40: iteration: 15500: Loss: 0.187482, lr: 0.008498
2018-08-10 05:52:46,035:CRITICAL:EmoV2_step40: iteration: 15600: Loss: 0.294688, lr: 0.008482
2018-08-10 05:53:17,274:CRITICAL:EmoV2_step40: iteration: 15700: Loss: 0.291631, lr: 0.008465
2018-08-10 05:53:48,541:CRITICAL:EmoV2_step40: iteration: 15800: Loss: 0.272092, lr: 0.008449
2018-08-10 05:54:20,147:CRITICAL:EmoV2_step40: iteration: 15900: Loss: 0.283734, lr: 0.008433
2018-08-10 05:54:51,629:CRITICAL:EmoV2_step40: iteration: 16000: Loss: 0.366158, lr: 0.008417
2018-08-10 06:02:58,199:CRITICAL:EmoV2_step40: validate. Iteration: 16000: Accuracy (valence, arousal): 40.265% 20.354%
2018-08-10 06:02:58,200:CRITICAL:EmoV2_step40: validate. Iteration: 16000: Loss: 0.998851
2018-08-10 06:03:29,520:CRITICAL:EmoV2_step40: iteration: 16100: Loss: 0.372275, lr: 0.008401
2018-08-10 06:04:00,831:CRITICAL:EmoV2_step40: iteration: 16200: Loss: 0.286077, lr: 0.008384
2018-08-10 06:04:32,319:CRITICAL:EmoV2_step40: iteration: 16300: Loss: 0.450056, lr: 0.008368
2018-08-10 06:05:03,499:CRITICAL:EmoV2_step40: iteration: 16400: Loss: 0.324138, lr: 0.008352
2018-08-10 06:05:34,722:CRITICAL:EmoV2_step40: iteration: 16500: Loss: 0.323255, lr: 0.008336
2018-08-10 06:06:06,055:CRITICAL:EmoV2_step40: iteration: 16600: Loss: 0.202479, lr: 0.008320
2018-08-10 06:06:37,152:CRITICAL:EmoV2_step40: iteration: 16700: Loss: 0.528006, lr: 0.008303
2018-08-10 06:07:08,242:CRITICAL:EmoV2_step40: iteration: 16800: Loss: 0.193408, lr: 0.008287
2018-08-10 06:07:39,356:CRITICAL:EmoV2_step40: iteration: 16900: Loss: 0.550169, lr: 0.008271
2018-08-10 06:08:10,450:CRITICAL:EmoV2_step40: iteration: 17000: Loss: 0.355363, lr: 0.008255
2018-08-10 06:16:16,478:CRITICAL:EmoV2_step40: validate. Iteration: 17000: Accuracy (valence, arousal): 29.646% 17.699%
2018-08-10 06:16:16,479:CRITICAL:EmoV2_step40: validate. Iteration: 17000: Loss: 0.998372
2018-08-10 06:16:47,903:CRITICAL:EmoV2_step40: iteration: 17100: Loss: 0.490261, lr: 0.008239
2018-08-10 06:17:19,477:CRITICAL:EmoV2_step40: iteration: 17200: Loss: 0.340707, lr: 0.008222
2018-08-10 06:17:50,933:CRITICAL:EmoV2_step40: iteration: 17300: Loss: 0.293335, lr: 0.008206
2018-08-10 06:18:22,500:CRITICAL:EmoV2_step40: iteration: 17400: Loss: 0.333016, lr: 0.008190
2018-08-10 06:18:53,998:CRITICAL:EmoV2_step40: iteration: 17500: Loss: 0.391383, lr: 0.008174
2018-08-10 06:19:25,501:CRITICAL:EmoV2_step40: iteration: 17600: Loss: 0.226364, lr: 0.008158
2018-08-10 06:19:57,047:CRITICAL:EmoV2_step40: iteration: 17700: Loss: 0.410840, lr: 0.008141
2018-08-10 06:20:28,452:CRITICAL:EmoV2_step40: iteration: 17800: Loss: 0.446125, lr: 0.008125
2018-08-10 06:20:59,861:CRITICAL:EmoV2_step40: iteration: 17900: Loss: 0.382786, lr: 0.008109
2018-08-10 06:21:31,328:CRITICAL:EmoV2_step40: iteration: 18000: Loss: 0.487433, lr: 0.008093
2018-08-10 06:29:37,671:CRITICAL:EmoV2_step40: validate. Iteration: 18000: Accuracy (valence, arousal): 31.858% 22.124%
2018-08-10 06:29:37,671:CRITICAL:EmoV2_step40: validate. Iteration: 18000: Loss: 0.998643
2018-08-10 06:30:09,110:CRITICAL:EmoV2_step40: iteration: 18100: Loss: 0.349397, lr: 0.008077
2018-08-10 06:30:40,658:CRITICAL:EmoV2_step40: iteration: 18200: Loss: 0.684039, lr: 0.008060
2018-08-10 06:31:12,140:CRITICAL:EmoV2_step40: iteration: 18300: Loss: 0.416162, lr: 0.008044
2018-08-10 06:31:43,603:CRITICAL:EmoV2_step40: iteration: 18400: Loss: 0.478177, lr: 0.008028
2018-08-10 06:32:15,086:CRITICAL:EmoV2_step40: iteration: 18500: Loss: 0.425410, lr: 0.008012
2018-08-10 06:32:46,646:CRITICAL:EmoV2_step40: iteration: 18600: Loss: 0.350688, lr: 0.007996
2018-08-10 06:33:18,242:CRITICAL:EmoV2_step40: iteration: 18700: Loss: 0.287060, lr: 0.007979
2018-08-10 06:33:49,646:CRITICAL:EmoV2_step40: iteration: 18800: Loss: 0.332090, lr: 0.007963
2018-08-10 06:34:20,739:CRITICAL:EmoV2_step40: iteration: 18900: Loss: 0.336792, lr: 0.007947
2018-08-10 06:34:51,850:CRITICAL:EmoV2_step40: iteration: 19000: Loss: 0.313843, lr: 0.007931
2018-08-10 06:42:58,771:CRITICAL:EmoV2_step40: validate. Iteration: 19000: Accuracy (valence, arousal): 34.513% 21.681%
2018-08-10 06:42:58,771:CRITICAL:EmoV2_step40: validate. Iteration: 19000: Loss: 0.998915
2018-08-10 06:43:30,258:CRITICAL:EmoV2_step40: iteration: 19100: Loss: 0.425185, lr: 0.007915
2018-08-10 06:44:01,639:CRITICAL:EmoV2_step40: iteration: 19200: Loss: 0.321564, lr: 0.007898
2018-08-10 06:44:32,894:CRITICAL:EmoV2_step40: iteration: 19300: Loss: 0.414118, lr: 0.007882
2018-08-10 06:45:04,008:CRITICAL:EmoV2_step40: iteration: 19400: Loss: 0.232720, lr: 0.007866
2018-08-10 06:45:35,102:CRITICAL:EmoV2_step40: iteration: 19500: Loss: 0.582453, lr: 0.007850
2018-08-10 06:46:06,490:CRITICAL:EmoV2_step40: iteration: 19600: Loss: 0.275059, lr: 0.007834
2018-08-10 06:46:37,881:CRITICAL:EmoV2_step40: iteration: 19700: Loss: 0.541731, lr: 0.007817
2018-08-10 06:47:09,352:CRITICAL:EmoV2_step40: iteration: 19800: Loss: 0.240831, lr: 0.007801
2018-08-10 06:47:40,820:CRITICAL:EmoV2_step40: iteration: 19900: Loss: 0.309173, lr: 0.007785
2018-08-10 06:48:53,653:CRITICAL:EmoV2_step40: iteration: 20000: Loss: 0.273308, lr: 0.007769
2018-08-10 06:56:59,521:CRITICAL:EmoV2_step40: validate. Iteration: 20000: Accuracy (valence, arousal): 34.513% 23.451%
2018-08-10 06:56:59,521:CRITICAL:EmoV2_step40: validate. Iteration: 20000: Loss: 0.998955
2018-08-10 06:57:31,022:CRITICAL:EmoV2_step40: iteration: 20100: Loss: 0.370354, lr: 0.007753
2018-08-10 06:58:02,518:CRITICAL:EmoV2_step40: iteration: 20200: Loss: 0.245480, lr: 0.007736
2018-08-10 06:58:33,934:CRITICAL:EmoV2_step40: iteration: 20300: Loss: 0.193805, lr: 0.007720
2018-08-10 06:59:05,354:CRITICAL:EmoV2_step40: iteration: 20400: Loss: 0.315053, lr: 0.007704
2018-08-10 06:59:36,788:CRITICAL:EmoV2_step40: iteration: 20500: Loss: 0.662829, lr: 0.007688
2018-08-10 07:00:08,227:CRITICAL:EmoV2_step40: iteration: 20600: Loss: 0.431377, lr: 0.007672
2018-08-10 07:00:39,600:CRITICAL:EmoV2_step40: iteration: 20700: Loss: 0.477528, lr: 0.007655
2018-08-10 07:01:11,085:CRITICAL:EmoV2_step40: iteration: 20800: Loss: 0.265802, lr: 0.007639
2018-08-10 07:01:42,597:CRITICAL:EmoV2_step40: iteration: 20900: Loss: 0.398773, lr: 0.007623
2018-08-10 07:02:14,046:CRITICAL:EmoV2_step40: iteration: 21000: Loss: 0.647122, lr: 0.007607
2018-08-10 07:10:19,633:CRITICAL:EmoV2_step40: validate. Iteration: 21000: Accuracy (valence, arousal): 31.416% 20.796%
2018-08-10 07:10:19,634:CRITICAL:EmoV2_step40: validate. Iteration: 21000: Loss: 0.998902
2018-08-10 07:10:51,203:CRITICAL:EmoV2_step40: iteration: 21100: Loss: 0.348315, lr: 0.007591
2018-08-10 07:11:22,856:CRITICAL:EmoV2_step40: iteration: 21200: Loss: 0.252446, lr: 0.007574
2018-08-10 07:11:54,292:CRITICAL:EmoV2_step40: iteration: 21300: Loss: 0.202769, lr: 0.007558
2018-08-10 07:12:25,723:CRITICAL:EmoV2_step40: iteration: 21400: Loss: 0.365208, lr: 0.007542
2018-08-10 07:12:57,126:CRITICAL:EmoV2_step40: iteration: 21500: Loss: 0.747688, lr: 0.007526
2018-08-10 07:13:28,495:CRITICAL:EmoV2_step40: iteration: 21600: Loss: 0.450248, lr: 0.007510
2018-08-10 07:13:59,980:CRITICAL:EmoV2_step40: iteration: 21700: Loss: 0.197600, lr: 0.007493
2018-08-10 07:14:31,466:CRITICAL:EmoV2_step40: iteration: 21800: Loss: 0.566551, lr: 0.007477
2018-08-10 07:15:02,966:CRITICAL:EmoV2_step40: iteration: 21900: Loss: 0.300567, lr: 0.007461
2018-08-10 07:15:34,407:CRITICAL:EmoV2_step40: iteration: 22000: Loss: 0.264401, lr: 0.007445
2018-08-10 07:23:40,485:CRITICAL:EmoV2_step40: validate. Iteration: 22000: Accuracy (valence, arousal): 31.416% 18.584%
2018-08-10 07:23:40,486:CRITICAL:EmoV2_step40: validate. Iteration: 22000: Loss: 0.998881
2018-08-10 07:24:11,984:CRITICAL:EmoV2_step40: iteration: 22100: Loss: 0.479169, lr: 0.007429
2018-08-10 07:24:43,534:CRITICAL:EmoV2_step40: iteration: 22200: Loss: 0.232768, lr: 0.007412
2018-08-10 07:25:14,908:CRITICAL:EmoV2_step40: iteration: 22300: Loss: 0.238927, lr: 0.007396
2018-08-10 07:25:46,422:CRITICAL:EmoV2_step40: iteration: 22400: Loss: 0.720724, lr: 0.007380
2018-08-10 07:26:17,946:CRITICAL:EmoV2_step40: iteration: 22500: Loss: 0.300401, lr: 0.007364
2018-08-10 07:26:49,377:CRITICAL:EmoV2_step40: iteration: 22600: Loss: 0.221169, lr: 0.007348
2018-08-10 20:33:39,477:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 20:35:25,601:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 20:44:13,733:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 20:45:45,142:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 20:49:34,300:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 20:55:25,383:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 20:57:50,862:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:01:07,013:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:04:56,261:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 8,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:10:39,826:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:13:12,284:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:15:20,979:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:17:36,899:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:19:53,860:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:22:51,580:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:31:30,191:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:32:36,825:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-10 21:34:22,597:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 4, 'height': 171, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 01:39:52,670:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 01:41:33,836:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 01:44:22,371:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 01:54:46,271:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 01:59:47,002:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:00:58,829:CRITICAL:EmoV2_step40: iteration: 0: Loss: 1.122048, lr: 0.000965
2018-08-11 02:01:18,244:CRITICAL:EmoV2_step40: validate. Iteration: 0: Accuracy (valence, arousal): 0.442% 0.885%
2018-08-11 02:01:18,244:CRITICAL:EmoV2_step40: validate. Iteration: 0: Loss: 1.002476
2018-08-11 02:01:37,077:CRITICAL:EmoV2_step40: iteration: 100: Loss: 0.978341, lr: 0.001030
2018-08-11 02:01:54,160:CRITICAL:EmoV2_step40: iteration: 200: Loss: 0.993575, lr: 0.001094
2018-08-11 02:02:11,598:CRITICAL:EmoV2_step40: iteration: 300: Loss: 0.986531, lr: 0.001159
2018-08-11 02:02:28,574:CRITICAL:EmoV2_step40: iteration: 400: Loss: 0.986426, lr: 0.001224
2018-08-11 02:02:45,957:CRITICAL:EmoV2_step40: iteration: 500: Loss: 1.004532, lr: 0.001289
2018-08-11 02:03:02,402:CRITICAL:EmoV2_step40: iteration: 600: Loss: 0.987163, lr: 0.001354
2018-08-11 02:03:19,685:CRITICAL:EmoV2_step40: iteration: 700: Loss: 0.967085, lr: 0.001418
2018-08-11 02:03:36,870:CRITICAL:EmoV2_step40: iteration: 800: Loss: 1.005597, lr: 0.001483
2018-08-11 02:03:54,181:CRITICAL:EmoV2_step40: iteration: 900: Loss: 0.993300, lr: 0.001548
2018-08-11 02:04:09,860:CRITICAL:EmoV2_step40: iteration: 1000: Loss: 1.020549, lr: 0.001613
2018-08-11 02:04:24,734:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Accuracy (valence, arousal): 0.000% 0.000%
2018-08-11 02:04:24,734:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Loss: 0.999955
2018-08-11 02:04:39,652:CRITICAL:EmoV2_step40: iteration: 1100: Loss: 0.993705, lr: 0.001678
2018-08-11 02:04:52,475:CRITICAL:EmoV2_step40: iteration: 1200: Loss: 1.024207, lr: 0.001742
2018-08-11 02:05:06,499:CRITICAL:EmoV2_step40: iteration: 1300: Loss: 0.989459, lr: 0.001807
2018-08-11 02:05:19,722:CRITICAL:EmoV2_step40: iteration: 1400: Loss: 1.010337, lr: 0.001872
2018-08-11 02:05:32,184:CRITICAL:EmoV2_step40: iteration: 1500: Loss: 1.005104, lr: 0.001937
2018-08-11 02:05:46,025:CRITICAL:EmoV2_step40: iteration: 1600: Loss: 1.026127, lr: 0.002002
2018-08-11 02:05:58,724:CRITICAL:EmoV2_step40: iteration: 1700: Loss: 0.999351, lr: 0.002066
2018-08-11 02:06:11,930:CRITICAL:EmoV2_step40: iteration: 1800: Loss: 0.997847, lr: 0.002131
2018-08-11 02:06:26,881:CRITICAL:EmoV2_step40: iteration: 1900: Loss: 0.995432, lr: 0.002196
2018-08-11 02:06:39,419:CRITICAL:EmoV2_step40: iteration: 2000: Loss: 0.992302, lr: 0.002261
2018-08-11 02:06:53,828:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Accuracy (valence, arousal): 0.000% 0.000%
2018-08-11 02:06:53,829:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Loss: 1.000582
2018-08-11 02:07:05,869:CRITICAL:EmoV2_step40: iteration: 2100: Loss: 1.003650, lr: 0.002326
2018-08-11 02:07:18,517:CRITICAL:EmoV2_step40: iteration: 2200: Loss: 0.988843, lr: 0.002390
2018-08-11 02:07:32,214:CRITICAL:EmoV2_step40: iteration: 2300: Loss: 0.995461, lr: 0.002455
2018-08-11 02:07:45,085:CRITICAL:EmoV2_step40: iteration: 2400: Loss: 1.004073, lr: 0.002520
2018-08-11 02:07:57,222:CRITICAL:EmoV2_step40: iteration: 2500: Loss: 1.001318, lr: 0.002585
2018-08-11 02:08:52,683:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:10:31,221:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:15:16,909:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 1,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 1,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:17:44,657:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 1,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 1,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:18:53,740:CRITICAL:EmoV2_step40: iteration: 0: Loss: nan, lr: 0.000965
2018-08-11 02:19:14,950:CRITICAL:EmoV2_step40: validate. Iteration: 0: Accuracy (valence, arousal): 100.000% 100.000%
2018-08-11 02:19:14,951:CRITICAL:EmoV2_step40: validate. Iteration: 0: Loss: nan
2018-08-11 02:19:27,562:CRITICAL:EmoV2_step40: iteration: 100: Loss: nan, lr: 0.001030
2018-08-11 02:19:40,353:CRITICAL:EmoV2_step40: iteration: 200: Loss: nan, lr: 0.001094
2018-08-11 02:19:52,985:CRITICAL:EmoV2_step40: iteration: 300: Loss: nan, lr: 0.001159
2018-08-11 02:20:05,770:CRITICAL:EmoV2_step40: iteration: 400: Loss: nan, lr: 0.001224
2018-08-11 02:20:18,395:CRITICAL:EmoV2_step40: iteration: 500: Loss: nan, lr: 0.001289
2018-08-11 02:20:31,036:CRITICAL:EmoV2_step40: iteration: 600: Loss: nan, lr: 0.001354
2018-08-11 02:20:43,672:CRITICAL:EmoV2_step40: iteration: 700: Loss: nan, lr: 0.001418
2018-08-11 02:20:56,294:CRITICAL:EmoV2_step40: iteration: 800: Loss: nan, lr: 0.001483
2018-08-11 02:21:08,941:CRITICAL:EmoV2_step40: iteration: 900: Loss: nan, lr: 0.001548
2018-08-11 02:21:21,586:CRITICAL:EmoV2_step40: iteration: 1000: Loss: nan, lr: 0.001613
2018-08-11 02:21:41,547:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Accuracy (valence, arousal): 100.000% 100.000%
2018-08-11 02:21:41,548:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Loss: nan
2018-08-11 02:22:37,195:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:24:43,805:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 34, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:28:37,415:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:30:49,697:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:34:01,398:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 20, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:36:35,512:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 20, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:39:03,493:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:41:40,141:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 18, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:43:19,829:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:53:51,438:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:56:00,529:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 11, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:57:45,431:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 11, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 02:59:23,328:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 10, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:01:39,693:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 9, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:06:30,120:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:13:11,684:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:16:22,750:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:18:20,114:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 2,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:20:33,528:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 224, 'width': 224},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:23:34,142:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 212, 'width': 212},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:29:09,843:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 212, 'width': 212},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:30:57,134:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 3,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:32:06,407:CRITICAL:EmoV2_step40: iteration: 0: Loss: 55.767094, lr: 0.000965
2018-08-11 03:32:14,149:CRITICAL:EmoV2_step40: validate. Iteration: 0: Accuracy (valence, arousal): 0.000% 0.000%
2018-08-11 03:32:14,150:CRITICAL:EmoV2_step40: validate. Iteration: 0: Loss: 6389615.500000
2018-08-11 03:32:25,259:CRITICAL:EmoV2_step40: iteration: 100: Loss: nan, lr: 0.001030
2018-08-11 03:32:36,588:CRITICAL:EmoV2_step40: iteration: 200: Loss: nan, lr: 0.001094
2018-08-11 03:32:49,265:CRITICAL:EmoV2_step40: iteration: 300: Loss: nan, lr: 0.001159
2018-08-11 03:33:00,262:CRITICAL:EmoV2_step40: iteration: 400: Loss: nan, lr: 0.001224
2018-08-11 03:33:11,599:CRITICAL:EmoV2_step40: iteration: 500: Loss: nan, lr: 0.001289
2018-08-11 03:33:25,034:CRITICAL:EmoV2_step40: iteration: 600: Loss: nan, lr: 0.001354
2018-08-11 03:33:36,455:CRITICAL:EmoV2_step40: iteration: 700: Loss: nan, lr: 0.001418
2018-08-11 03:33:47,984:CRITICAL:EmoV2_step40: iteration: 800: Loss: nan, lr: 0.001483
2018-08-11 03:34:00,289:CRITICAL:EmoV2_step40: iteration: 900: Loss: nan, lr: 0.001548
2018-08-11 03:34:11,118:CRITICAL:EmoV2_step40: iteration: 1000: Loss: nan, lr: 0.001613
2018-08-11 03:34:18,201:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Accuracy (valence, arousal): 100.000% 100.000%
2018-08-11 03:34:18,201:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Loss: nan
2018-08-11 03:34:30,425:CRITICAL:EmoV2_step40: iteration: 1100: Loss: nan, lr: 0.001678
2018-08-11 03:34:41,447:CRITICAL:EmoV2_step40: iteration: 1200: Loss: nan, lr: 0.001742
2018-08-11 03:34:52,782:CRITICAL:EmoV2_step40: iteration: 1300: Loss: nan, lr: 0.001807
2018-08-11 03:35:04,573:CRITICAL:EmoV2_step40: iteration: 1400: Loss: nan, lr: 0.001872
2018-08-11 03:35:16,315:CRITICAL:EmoV2_step40: iteration: 1500: Loss: nan, lr: 0.001937
2018-08-11 03:35:27,495:CRITICAL:EmoV2_step40: iteration: 1600: Loss: nan, lr: 0.002002
2018-08-11 03:35:40,227:CRITICAL:EmoV2_step40: iteration: 1700: Loss: nan, lr: 0.002066
2018-08-11 03:35:51,258:CRITICAL:EmoV2_step40: iteration: 1800: Loss: nan, lr: 0.002131
2018-08-11 03:36:01,645:CRITICAL:EmoV2_step40: iteration: 1900: Loss: nan, lr: 0.002196
2018-08-11 03:36:11,978:CRITICAL:EmoV2_step40: iteration: 2000: Loss: nan, lr: 0.002261
2018-08-11 03:36:21,369:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Accuracy (valence, arousal): 100.000% 100.000%
2018-08-11 03:36:21,369:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Loss: nan
2018-08-11 03:36:31,518:CRITICAL:EmoV2_step40: iteration: 2100: Loss: nan, lr: 0.002326
2018-08-11 03:36:42,822:CRITICAL:EmoV2_step40: iteration: 2200: Loss: nan, lr: 0.002390
2018-08-11 03:36:54,529:CRITICAL:EmoV2_step40: iteration: 2300: Loss: nan, lr: 0.002455
2018-08-11 03:37:05,530:CRITICAL:EmoV2_step40: iteration: 2400: Loss: nan, lr: 0.002520
2018-08-11 03:37:16,142:CRITICAL:EmoV2_step40: iteration: 2500: Loss: nan, lr: 0.002585
2018-08-11 03:37:28,098:CRITICAL:EmoV2_step40: iteration: 2600: Loss: nan, lr: 0.002650
2018-08-11 03:37:38,858:CRITICAL:EmoV2_step40: iteration: 2700: Loss: nan, lr: 0.002714
2018-08-11 03:37:49,454:CRITICAL:EmoV2_step40: iteration: 2800: Loss: nan, lr: 0.002779
2018-08-11 03:37:59,190:CRITICAL:EmoV2_step40: iteration: 2900: Loss: nan, lr: 0.002844
2018-08-11 03:38:10,895:CRITICAL:EmoV2_step40: iteration: 3000: Loss: nan, lr: 0.002909
2018-08-11 03:41:08,896:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'MSE': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 5,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 5,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:42:17,996:CRITICAL:EmoV2_step40: iteration: 0: Loss: 41.465218, lr: 0.000965
2018-08-11 03:42:24,259:CRITICAL:EmoV2_step40: validate. Iteration: 0: Accuracy (valence, arousal): 0.000% 0.000%
2018-08-11 03:42:24,260:CRITICAL:EmoV2_step40: validate. Iteration: 0: Loss: 315395.562500
2018-08-11 03:42:39,844:CRITICAL:EmoV2_step40: iteration: 100: Loss: nan, lr: 0.001030
2018-08-11 03:42:58,227:CRITICAL:EmoV2_step40: iteration: 200: Loss: nan, lr: 0.001094
2018-08-11 03:43:15,503:CRITICAL:EmoV2_step40: iteration: 300: Loss: nan, lr: 0.001159
2018-08-11 03:43:32,723:CRITICAL:EmoV2_step40: iteration: 400: Loss: nan, lr: 0.001224
2018-08-11 03:43:48,861:CRITICAL:EmoV2_step40: iteration: 500: Loss: nan, lr: 0.001289
2018-08-11 03:44:05,946:CRITICAL:EmoV2_step40: iteration: 600: Loss: nan, lr: 0.001354
2018-08-11 03:44:21,985:CRITICAL:EmoV2_step40: iteration: 700: Loss: nan, lr: 0.001418
2018-08-11 03:44:38,301:CRITICAL:EmoV2_step40: iteration: 800: Loss: nan, lr: 0.001483
2018-08-11 03:45:21,176:INFO:Configuration is:
{   'batch_proc': {'use_async': True, 'use_pin_memory': True},
    'dataset': {   'train': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_TrainVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_TrainVideos/train_data_with_landmarks.txt'},
                                'server': None},
                   'valid': {   'local': {   'data_root': '/home/mdomrachev/Data/STML/omg_ValidVideos/frames',
                                             'file_list': '/home/mdomrachev/Data/STML/omg_ValidVideos/valid_data_with_landmarks.txt'},
                                'server': None}},
    'ini_net': {   'local': '\\\\unid2face.stc\\PublicB\\kalinovskiy\\ave_log\\EmoV2_step4\\EmoV2_step4_iter_14500.model',
                   'server': '/media/data/kalinovskiy/face_recognition/logs/AlexNet_CombinedMargin_5/AlexNet_CombinedMargin_5_15000.model'},
    'logging': {   'log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                  'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'snapshot_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                       'server': '/media/data/kalinovskiy/face_recognition_merge/logs'},
                   'tb_log_dir': {   'local': '/home/mdomrachev/Data/STML_projects/pytorch/VEmotionNet/aver_log',
                                     'server': '/media/data/kalinovskiy/face_recognition_merge/logs'}},
    'losses': {'CC': {'w': 1.0}},
    'lr_scheduler': {   'gamma': 0.01,
                        'scale_lr': [1.0, 1],
                        'scale_lr_fc': [1.0, 1],
                        'type': 'MultiCyclePolicy',
                        'use_linear_decay': True},
    'net': {   'depth': 18,
               'fine_tune': False,
               'softmax_size': 2,
               'type': 'ResNet'},
    'opt': {   'lr': 0.009,
               'momentum': 0.2,
               'type': 'SGD',
               'weight_decay': 0.0005},
    'parser': {'max_num_clips': 0, 'max_num_samples': 0},
    'preproc': {   'aug': {   'color': 'BGR',
                              'pad': 10,
                              'use_center_crop': True,
                              'use_cutout': False,
                              'use_mirroring': True,
                              'use_random_crop': True,
                              'use_random_gray': False},
                   'crop_size': 200,
                   'data_frame': {'depth': 12, 'height': 128, 'width': 128},
                   'is_color': True,
                   'mean': 127.5,
                   'scale': 0.007843},
    'sampler': {'samples_is_randomize': False, 'step_size_for_samples': 4},
    'seed': 1234,
    'test': {   'cuda_device': 0,
                'dataset': {   'data_root': {   'local': 'D:\\AVER\\AFEW-VA\\crop',
                                                'server': '/media/data/stc-85k-a/faces'},
                               'test_file_list': {   'local': 'D:\\AVER\\AFEW-VA\\crop/test_data_with_landmarks.txt',
                                                     'server': '/media/data/kalinovskiy/train_file_list_85k.txt'}},
                'file_model': '/home/mdomrachev/Data/STML_projects/pytorch/binary_models/step.model'},
    'train': {   'cuda_device': 0,
                 'epoch_size': 50000,
                 'experiment_name': 'EmoV2_step4',
                 'max_iter': 10000000,
                 'snapshot_iter': 10000,
                 'step_print': 100,
                 'step_size': 100,
                 'validate_iter': 1000},
    'train_batcher': {   'batch': 5,
                         'disk_reader_process_num': 1,
                         'queue_size': 5},
    'valid_batcher': {   'batch': 5,
                         'disk_reader_process_num': 1,
                         'queue_size': 5}}
2018-08-11 03:46:29,637:CRITICAL:EmoV2_step40: iteration: 0: Loss: 1.011712, lr: 0.000965
2018-08-11 03:46:36,192:CRITICAL:EmoV2_step40: validate. Iteration: 0: Accuracy (valence, arousal): 0.000% 6.195%
2018-08-11 03:46:36,193:CRITICAL:EmoV2_step40: validate. Iteration: 0: Loss: nan
2018-08-11 03:46:51,607:CRITICAL:EmoV2_step40: iteration: 100: Loss: 0.991840, lr: 0.001030
2018-08-11 03:47:08,429:CRITICAL:EmoV2_step40: iteration: 200: Loss: 0.944732, lr: 0.001094
2018-08-11 03:47:23,696:CRITICAL:EmoV2_step40: iteration: 300: Loss: 1.027915, lr: 0.001159
2018-08-11 03:47:40,431:CRITICAL:EmoV2_step40: iteration: 400: Loss: 0.964736, lr: 0.001224
2018-08-11 03:47:56,090:CRITICAL:EmoV2_step40: iteration: 500: Loss: 0.981717, lr: 0.001289
2018-08-11 03:48:11,412:CRITICAL:EmoV2_step40: iteration: 600: Loss: 0.980050, lr: 0.001354
2018-08-11 03:48:27,411:CRITICAL:EmoV2_step40: iteration: 700: Loss: 1.015874, lr: 0.001418
2018-08-11 03:48:42,262:CRITICAL:EmoV2_step40: iteration: 800: Loss: 0.919730, lr: 0.001483
2018-08-11 03:48:57,380:CRITICAL:EmoV2_step40: iteration: 900: Loss: 1.008261, lr: 0.001548
2018-08-11 03:49:12,299:CRITICAL:EmoV2_step40: iteration: 1000: Loss: 0.952682, lr: 0.001613
2018-08-11 03:49:18,202:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Accuracy (valence, arousal): 3.097% 3.097%
2018-08-11 03:49:18,203:CRITICAL:EmoV2_step40: validate. Iteration: 1000: Loss: nan
2018-08-11 03:49:33,839:CRITICAL:EmoV2_step40: iteration: 1100: Loss: 0.981581, lr: 0.001678
2018-08-11 03:49:49,019:CRITICAL:EmoV2_step40: iteration: 1200: Loss: 1.002283, lr: 0.001742
2018-08-11 03:50:09,764:CRITICAL:EmoV2_step40: iteration: 1300: Loss: 0.995383, lr: 0.001807
2018-08-11 03:50:24,478:CRITICAL:EmoV2_step40: iteration: 1400: Loss: 0.945236, lr: 0.001872
2018-08-11 03:50:38,952:CRITICAL:EmoV2_step40: iteration: 1500: Loss: 0.954953, lr: 0.001937
2018-08-11 03:50:53,920:CRITICAL:EmoV2_step40: iteration: 1600: Loss: 0.998410, lr: 0.002002
2018-08-11 03:51:07,990:CRITICAL:EmoV2_step40: iteration: 1700: Loss: 1.001655, lr: 0.002066
2018-08-11 03:51:23,362:CRITICAL:EmoV2_step40: iteration: 1800: Loss: 0.973272, lr: 0.002131
2018-08-11 03:51:37,503:CRITICAL:EmoV2_step40: iteration: 1900: Loss: 0.954455, lr: 0.002196
2018-08-11 03:51:57,911:CRITICAL:EmoV2_step40: iteration: 2000: Loss: 0.958278, lr: 0.002261
2018-08-11 03:52:03,759:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Accuracy (valence, arousal): 0.885% 0.885%
2018-08-11 03:52:03,760:CRITICAL:EmoV2_step40: validate. Iteration: 2000: Loss: nan
2018-08-11 03:52:17,005:CRITICAL:EmoV2_step40: iteration: 2100: Loss: 0.979082, lr: 0.002326
2018-08-11 03:52:31,987:CRITICAL:EmoV2_step40: iteration: 2200: Loss: 0.962831, lr: 0.002390
2018-08-11 03:52:45,884:CRITICAL:EmoV2_step40: iteration: 2300: Loss: 1.007819, lr: 0.002455
2018-08-11 03:52:59,760:CRITICAL:EmoV2_step40: iteration: 2400: Loss: 0.965397, lr: 0.002520
2018-08-11 03:53:14,178:CRITICAL:EmoV2_step40: iteration: 2500: Loss: 0.982490, lr: 0.002585
2018-08-11 03:53:28,092:CRITICAL:EmoV2_step40: iteration: 2600: Loss: 0.993480, lr: 0.002650
2018-08-11 03:53:42,958:CRITICAL:EmoV2_step40: iteration: 2700: Loss: 0.947295, lr: 0.002714
2018-08-11 03:53:56,969:CRITICAL:EmoV2_step40: iteration: 2800: Loss: 0.985555, lr: 0.002779
2018-08-11 03:54:10,544:CRITICAL:EmoV2_step40: iteration: 2900: Loss: 0.952780, lr: 0.002844
2018-08-11 03:54:24,952:CRITICAL:EmoV2_step40: iteration: 3000: Loss: 0.984755, lr: 0.002909
2018-08-11 03:54:30,596:CRITICAL:EmoV2_step40: validate. Iteration: 3000: Accuracy (valence, arousal): 3.540% 0.442%
2018-08-11 03:54:30,596:CRITICAL:EmoV2_step40: validate. Iteration: 3000: Loss: nan
2018-08-11 03:54:43,932:CRITICAL:EmoV2_step40: iteration: 3100: Loss: 0.962598, lr: 0.002974
2018-08-11 03:54:58,476:CRITICAL:EmoV2_step40: iteration: 3200: Loss: 0.952365, lr: 0.003038
2018-08-11 03:55:12,249:CRITICAL:EmoV2_step40: iteration: 3300: Loss: 0.963555, lr: 0.003103
2018-08-11 03:55:26,521:CRITICAL:EmoV2_step40: iteration: 3400: Loss: 0.987853, lr: 0.003168
2018-08-11 03:55:40,268:CRITICAL:EmoV2_step40: iteration: 3500: Loss: 0.965878, lr: 0.003233
2018-08-11 03:55:53,464:CRITICAL:EmoV2_step40: iteration: 3600: Loss: 0.952187, lr: 0.003298
2018-08-11 03:56:08,323:CRITICAL:EmoV2_step40: iteration: 3700: Loss: 0.960111, lr: 0.003362
2018-08-11 03:56:26,682:CRITICAL:EmoV2_step40: iteration: 3800: Loss: 0.977017, lr: 0.003427
2018-08-11 03:56:41,124:CRITICAL:EmoV2_step40: iteration: 3900: Loss: 1.020764, lr: 0.003492
2018-08-11 03:56:55,117:CRITICAL:EmoV2_step40: iteration: 4000: Loss: 0.989443, lr: 0.003557
2018-08-11 03:57:00,436:CRITICAL:EmoV2_step40: validate. Iteration: 4000: Accuracy (valence, arousal): 0.442% 2.212%
2018-08-11 03:57:00,436:CRITICAL:EmoV2_step40: validate. Iteration: 4000: Loss: nan
2018-08-11 03:57:15,157:CRITICAL:EmoV2_step40: iteration: 4100: Loss: 1.012747, lr: 0.003622
2018-08-11 03:57:29,189:CRITICAL:EmoV2_step40: iteration: 4200: Loss: 0.965226, lr: 0.003686
2018-08-11 03:57:43,119:CRITICAL:EmoV2_step40: iteration: 4300: Loss: 1.014796, lr: 0.003751
2018-08-11 03:57:56,853:CRITICAL:EmoV2_step40: iteration: 4400: Loss: 0.955302, lr: 0.003816
2018-08-11 03:58:10,274:CRITICAL:EmoV2_step40: iteration: 4500: Loss: 0.991705, lr: 0.003881
2018-08-11 03:58:23,701:CRITICAL:EmoV2_step40: iteration: 4600: Loss: 0.985835, lr: 0.003946
2018-08-11 03:58:37,419:CRITICAL:EmoV2_step40: iteration: 4700: Loss: 0.988714, lr: 0.004010
2018-08-11 03:58:50,788:CRITICAL:EmoV2_step40: iteration: 4800: Loss: 0.986201, lr: 0.004075
2018-08-11 03:59:03,986:CRITICAL:EmoV2_step40: iteration: 4900: Loss: 0.983150, lr: 0.004140
2018-08-11 03:59:17,140:CRITICAL:EmoV2_step40: iteration: 5000: Loss: 0.996178, lr: 0.004205
2018-08-11 03:59:22,631:CRITICAL:EmoV2_step40: validate. Iteration: 5000: Accuracy (valence, arousal): 3.540% 1.770%
2018-08-11 03:59:22,631:CRITICAL:EmoV2_step40: validate. Iteration: 5000: Loss: nan
2018-08-11 03:59:36,695:CRITICAL:EmoV2_step40: iteration: 5100: Loss: 1.024996, lr: 0.004270
2018-08-11 03:59:50,226:CRITICAL:EmoV2_step40: iteration: 5200: Loss: 0.978073, lr: 0.004334
2018-08-11 04:00:03,532:CRITICAL:EmoV2_step40: iteration: 5300: Loss: 1.011790, lr: 0.004399
2018-08-11 04:00:16,939:CRITICAL:EmoV2_step40: iteration: 5400: Loss: 0.955815, lr: 0.004464
2018-08-11 04:00:30,101:CRITICAL:EmoV2_step40: iteration: 5500: Loss: 0.924536, lr: 0.004529
2018-08-11 04:00:43,491:CRITICAL:EmoV2_step40: iteration: 5600: Loss: 0.987475, lr: 0.004594
2018-08-11 04:00:56,750:CRITICAL:EmoV2_step40: iteration: 5700: Loss: 0.977960, lr: 0.004658
2018-08-11 04:01:14,656:CRITICAL:EmoV2_step40: iteration: 5800: Loss: 1.021563, lr: 0.004723
2018-08-11 04:01:28,230:CRITICAL:EmoV2_step40: iteration: 5900: Loss: 0.979231, lr: 0.004788
2018-08-11 04:01:41,572:CRITICAL:EmoV2_step40: iteration: 6000: Loss: 1.031050, lr: 0.004853
2018-08-11 04:01:47,020:CRITICAL:EmoV2_step40: validate. Iteration: 6000: Accuracy (valence, arousal): 1.327% 1.327%
2018-08-11 04:01:47,020:CRITICAL:EmoV2_step40: validate. Iteration: 6000: Loss: nan
2018-08-11 04:02:00,214:CRITICAL:EmoV2_step40: iteration: 6100: Loss: 0.961928, lr: 0.004918
2018-08-11 04:02:13,426:CRITICAL:EmoV2_step40: iteration: 6200: Loss: 1.075000, lr: 0.004982
2018-08-11 04:02:31,168:CRITICAL:EmoV2_step40: iteration: 6300: Loss: 0.952273, lr: 0.005047
2018-08-11 04:02:44,336:CRITICAL:EmoV2_step40: iteration: 6400: Loss: 0.933783, lr: 0.005112
2018-08-11 04:02:57,815:CRITICAL:EmoV2_step40: iteration: 6500: Loss: 1.029607, lr: 0.005177
2018-08-11 04:03:11,267:CRITICAL:EmoV2_step40: iteration: 6600: Loss: 1.020180, lr: 0.005242
2018-08-11 04:03:24,418:CRITICAL:EmoV2_step40: iteration: 6700: Loss: 0.991450, lr: 0.005306
2018-08-11 04:03:37,590:CRITICAL:EmoV2_step40: iteration: 6800: Loss: 1.064084, lr: 0.005371
2018-08-11 04:03:50,942:CRITICAL:EmoV2_step40: iteration: 6900: Loss: 0.951755, lr: 0.005436
2018-08-11 04:04:04,091:CRITICAL:EmoV2_step40: iteration: 7000: Loss: 0.988112, lr: 0.005501
2018-08-11 04:04:09,475:CRITICAL:EmoV2_step40: validate. Iteration: 7000: Accuracy (valence, arousal): 2.655% 0.000%
2018-08-11 04:04:09,475:CRITICAL:EmoV2_step40: validate. Iteration: 7000: Loss: nan
2018-08-11 04:04:22,643:CRITICAL:EmoV2_step40: iteration: 7100: Loss: 0.979175, lr: 0.005566
2018-08-11 04:04:35,802:CRITICAL:EmoV2_step40: iteration: 7200: Loss: 0.959696, lr: 0.005630
2018-08-11 04:04:48,975:CRITICAL:EmoV2_step40: iteration: 7300: Loss: 0.966732, lr: 0.005695
2018-08-11 04:05:02,506:CRITICAL:EmoV2_step40: iteration: 7400: Loss: 0.964898, lr: 0.005760
2018-08-11 04:05:15,661:CRITICAL:EmoV2_step40: iteration: 7500: Loss: 0.985455, lr: 0.005825
2018-08-11 04:05:28,836:CRITICAL:EmoV2_step40: iteration: 7600: Loss: 0.926473, lr: 0.005890
2018-08-11 04:05:42,003:CRITICAL:EmoV2_step40: iteration: 7700: Loss: 0.962841, lr: 0.005954
2018-08-11 04:05:55,180:CRITICAL:EmoV2_step40: iteration: 7800: Loss: 0.973458, lr: 0.006019
2018-08-11 04:06:08,474:CRITICAL:EmoV2_step40: iteration: 7900: Loss: 0.985773, lr: 0.006084
2018-08-11 04:06:21,631:CRITICAL:EmoV2_step40: iteration: 8000: Loss: 1.004629, lr: 0.006149
2018-08-11 04:06:27,336:CRITICAL:EmoV2_step40: validate. Iteration: 8000: Accuracy (valence, arousal): 2.212% 0.442%
2018-08-11 04:06:27,337:CRITICAL:EmoV2_step40: validate. Iteration: 8000: Loss: nan
2018-08-11 04:06:45,247:CRITICAL:EmoV2_step40: iteration: 8100: Loss: 0.998123, lr: 0.006214
2018-08-11 04:06:58,755:CRITICAL:EmoV2_step40: iteration: 8200: Loss: 0.965311, lr: 0.006278
2018-08-11 04:07:11,902:CRITICAL:EmoV2_step40: iteration: 8300: Loss: 1.010065, lr: 0.006343
2018-08-11 04:07:25,071:CRITICAL:EmoV2_step40: iteration: 8400: Loss: 0.985973, lr: 0.006408
2018-08-11 04:07:38,263:CRITICAL:EmoV2_step40: iteration: 8500: Loss: 0.996579, lr: 0.006473
2018-08-11 04:07:51,539:CRITICAL:EmoV2_step40: iteration: 8600: Loss: 0.978096, lr: 0.006538
2018-08-11 04:08:04,865:CRITICAL:EmoV2_step40: iteration: 8700: Loss: 1.000540, lr: 0.006602
2018-08-11 04:08:22,631:CRITICAL:EmoV2_step40: iteration: 8800: Loss: 1.041441, lr: 0.006667
2018-08-11 04:08:35,770:CRITICAL:EmoV2_step40: iteration: 8900: Loss: 0.946905, lr: 0.006732
2018-08-11 04:08:48,929:CRITICAL:EmoV2_step40: iteration: 9000: Loss: 1.015745, lr: 0.006797
2018-08-11 04:08:53,968:CRITICAL:EmoV2_step40: validate. Iteration: 9000: Accuracy (valence, arousal): 1.327% 2.212%
2018-08-11 04:08:53,969:CRITICAL:EmoV2_step40: validate. Iteration: 9000: Loss: nan
2018-08-11 04:09:07,330:CRITICAL:EmoV2_step40: iteration: 9100: Loss: 1.005778, lr: 0.006862
2018-08-11 04:09:20,490:CRITICAL:EmoV2_step40: iteration: 9200: Loss: 0.996109, lr: 0.006926
2018-08-11 04:09:33,665:CRITICAL:EmoV2_step40: iteration: 9300: Loss: 0.963548, lr: 0.006991
2018-08-11 04:09:46,943:CRITICAL:EmoV2_step40: iteration: 9400: Loss: 0.998124, lr: 0.007056
2018-08-11 04:10:00,107:CRITICAL:EmoV2_step40: iteration: 9500: Loss: 0.981713, lr: 0.007121
2018-08-11 04:10:13,273:CRITICAL:EmoV2_step40: iteration: 9600: Loss: 0.919191, lr: 0.007186
2018-08-11 04:10:26,453:CRITICAL:EmoV2_step40: iteration: 9700: Loss: 1.045531, lr: 0.007250
2018-08-11 04:10:39,632:CRITICAL:EmoV2_step40: iteration: 9800: Loss: 1.012445, lr: 0.007315
2018-08-11 04:10:52,795:CRITICAL:EmoV2_step40: iteration: 9900: Loss: 0.874770, lr: 0.007380
2018-08-11 04:11:56,586:CRITICAL:EmoV2_step40: iteration: 10000: Loss: 0.964394, lr: 0.007445
2018-08-11 04:12:01,961:CRITICAL:EmoV2_step40: validate. Iteration: 10000: Accuracy (valence, arousal): 2.212% 2.212%
2018-08-11 04:12:01,962:CRITICAL:EmoV2_step40: validate. Iteration: 10000: Loss: nan
2018-08-11 04:12:19,533:CRITICAL:EmoV2_step40: iteration: 10100: Loss: 0.944609, lr: 0.007510
2018-08-11 04:12:32,759:CRITICAL:EmoV2_step40: iteration: 10200: Loss: 0.972327, lr: 0.007574
2018-08-11 04:12:46,065:CRITICAL:EmoV2_step40: iteration: 10300: Loss: 0.960802, lr: 0.007639
2018-08-11 04:12:59,167:CRITICAL:EmoV2_step40: iteration: 10400: Loss: 1.017945, lr: 0.007704
2018-08-11 04:13:12,277:CRITICAL:EmoV2_step40: iteration: 10500: Loss: 0.878481, lr: 0.007769
2018-08-11 04:13:25,396:CRITICAL:EmoV2_step40: iteration: 10600: Loss: 1.009269, lr: 0.007834
2018-08-11 04:13:38,507:CRITICAL:EmoV2_step40: iteration: 10700: Loss: 0.947096, lr: 0.007898
2018-08-11 04:13:51,650:CRITICAL:EmoV2_step40: iteration: 10800: Loss: 0.980428, lr: 0.007963
2018-08-11 04:14:04,818:CRITICAL:EmoV2_step40: iteration: 10900: Loss: 0.976198, lr: 0.008028
2018-08-11 04:14:17,998:CRITICAL:EmoV2_step40: iteration: 11000: Loss: 0.916223, lr: 0.008093
2018-08-11 04:14:23,151:CRITICAL:EmoV2_step40: validate. Iteration: 11000: Accuracy (valence, arousal): 1.327% 6.195%
2018-08-11 04:14:23,152:CRITICAL:EmoV2_step40: validate. Iteration: 11000: Loss: nan
2018-08-11 04:14:36,303:CRITICAL:EmoV2_step40: iteration: 11100: Loss: 0.991042, lr: 0.008158
2018-08-11 04:14:49,465:CRITICAL:EmoV2_step40: iteration: 11200: Loss: 0.958807, lr: 0.008222
2018-08-11 04:15:07,278:CRITICAL:EmoV2_step40: iteration: 11300: Loss: 0.998360, lr: 0.008287
2018-08-11 04:15:20,420:CRITICAL:EmoV2_step40: iteration: 11400: Loss: 0.982129, lr: 0.008352
2018-08-11 04:15:33,588:CRITICAL:EmoV2_step40: iteration: 11500: Loss: 1.048086, lr: 0.008417
2018-08-11 04:15:46,782:CRITICAL:EmoV2_step40: iteration: 11600: Loss: 0.956820, lr: 0.008482
2018-08-11 04:15:59,946:CRITICAL:EmoV2_step40: iteration: 11700: Loss: 1.058486, lr: 0.008546
2018-08-11 04:16:13,112:CRITICAL:EmoV2_step40: iteration: 11800: Loss: 0.947891, lr: 0.008611
2018-08-11 04:16:30,826:CRITICAL:EmoV2_step40: iteration: 11900: Loss: 0.943328, lr: 0.008676
2018-08-11 04:16:43,978:CRITICAL:EmoV2_step40: iteration: 12000: Loss: 0.943927, lr: 0.008741
2018-08-11 04:16:49,196:CRITICAL:EmoV2_step40: validate. Iteration: 12000: Accuracy (valence, arousal): 4.425% 2.212%
2018-08-11 04:16:49,196:CRITICAL:EmoV2_step40: validate. Iteration: 12000: Loss: nan
2018-08-11 04:17:02,353:CRITICAL:EmoV2_step40: iteration: 12100: Loss: 0.849437, lr: 0.008806
2018-08-11 04:17:15,498:CRITICAL:EmoV2_step40: iteration: 12200: Loss: 0.888648, lr: 0.008870
2018-08-11 04:17:28,641:CRITICAL:EmoV2_step40: iteration: 12300: Loss: 1.057228, lr: 0.008935
2018-08-11 04:17:41,806:CRITICAL:EmoV2_step40: iteration: 12400: Loss: 0.892736, lr: 0.009000
2018-08-11 04:17:54,975:CRITICAL:EmoV2_step40: iteration: 12500: Loss: 0.989122, lr: 0.008984
2018-08-11 04:18:08,140:CRITICAL:EmoV2_step40: iteration: 12600: Loss: 0.898536, lr: 0.008968
2018-08-11 04:18:21,303:CRITICAL:EmoV2_step40: iteration: 12700: Loss: 1.069095, lr: 0.008951
2018-08-11 04:18:34,473:CRITICAL:EmoV2_step40: iteration: 12800: Loss: 0.948818, lr: 0.008935
2018-08-11 04:18:47,658:CRITICAL:EmoV2_step40: iteration: 12900: Loss: 0.944438, lr: 0.008919
2018-08-11 04:19:00,841:CRITICAL:EmoV2_step40: iteration: 13000: Loss: 0.951140, lr: 0.008903
2018-08-11 04:19:05,869:CRITICAL:EmoV2_step40: validate. Iteration: 13000: Accuracy (valence, arousal): 1.327% 0.885%
2018-08-11 04:19:05,869:CRITICAL:EmoV2_step40: validate. Iteration: 13000: Loss: nan
2018-08-11 04:19:19,032:CRITICAL:EmoV2_step40: iteration: 13100: Loss: 0.954113, lr: 0.008887
2018-08-11 04:19:32,185:CRITICAL:EmoV2_step40: iteration: 13200: Loss: 1.046566, lr: 0.008870
2018-08-11 04:19:45,338:CRITICAL:EmoV2_step40: iteration: 13300: Loss: 0.897637, lr: 0.008854
2018-08-11 04:19:58,504:CRITICAL:EmoV2_step40: iteration: 13400: Loss: 1.001444, lr: 0.008838
2018-08-11 04:20:11,656:CRITICAL:EmoV2_step40: iteration: 13500: Loss: 0.894504, lr: 0.008822
2018-08-11 04:20:29,224:CRITICAL:EmoV2_step40: iteration: 13600: Loss: 1.028517, lr: 0.008806
2018-08-11 04:20:42,378:CRITICAL:EmoV2_step40: iteration: 13700: Loss: 0.914949, lr: 0.008789
2018-08-11 04:20:55,555:CRITICAL:EmoV2_step40: iteration: 13800: Loss: 1.000904, lr: 0.008773
2018-08-11 04:21:13,282:CRITICAL:EmoV2_step40: iteration: 13900: Loss: 0.908336, lr: 0.008757
2018-08-11 04:21:26,445:CRITICAL:EmoV2_step40: iteration: 14000: Loss: 1.046249, lr: 0.008741
2018-08-11 04:21:31,557:CRITICAL:EmoV2_step40: validate. Iteration: 14000: Accuracy (valence, arousal): 3.097% 1.770%
2018-08-11 04:21:31,558:CRITICAL:EmoV2_step40: validate. Iteration: 14000: Loss: nan
2018-08-11 04:21:44,720:CRITICAL:EmoV2_step40: iteration: 14100: Loss: 0.902469, lr: 0.008725
2018-08-11 04:21:57,883:CRITICAL:EmoV2_step40: iteration: 14200: Loss: 1.021803, lr: 0.008708
2018-08-11 04:22:11,042:CRITICAL:EmoV2_step40: iteration: 14300: Loss: 1.017637, lr: 0.008692
2018-08-11 04:22:24,213:CRITICAL:EmoV2_step40: iteration: 14400: Loss: 1.001173, lr: 0.008676
2018-08-11 04:22:37,374:CRITICAL:EmoV2_step40: iteration: 14500: Loss: 1.016374, lr: 0.008660
2018-08-11 04:22:50,543:CRITICAL:EmoV2_step40: iteration: 14600: Loss: 0.882134, lr: 0.008644
2018-08-11 04:23:03,713:CRITICAL:EmoV2_step40: iteration: 14700: Loss: 0.919129, lr: 0.008627
2018-08-11 04:23:16,904:CRITICAL:EmoV2_step40: iteration: 14800: Loss: 0.959443, lr: 0.008611
2018-08-11 04:23:30,069:CRITICAL:EmoV2_step40: iteration: 14900: Loss: 0.902786, lr: 0.008595
2018-08-11 04:23:43,250:CRITICAL:EmoV2_step40: iteration: 15000: Loss: 0.929100, lr: 0.008579
2018-08-11 04:23:48,365:CRITICAL:EmoV2_step40: validate. Iteration: 15000: Accuracy (valence, arousal): 13.717% 8.407%
2018-08-11 04:23:48,365:CRITICAL:EmoV2_step40: validate. Iteration: 15000: Loss: nan
2018-08-11 04:24:01,531:CRITICAL:EmoV2_step40: iteration: 15100: Loss: 0.943067, lr: 0.008563
2018-08-11 04:24:14,701:CRITICAL:EmoV2_step40: iteration: 15200: Loss: 1.008955, lr: 0.008546
2018-08-11 04:24:32,308:CRITICAL:EmoV2_step40: iteration: 15300: Loss: 1.004112, lr: 0.008530
2018-08-11 04:24:45,461:CRITICAL:EmoV2_step40: iteration: 15400: Loss: 0.902186, lr: 0.008514
2018-08-11 04:24:58,604:CRITICAL:EmoV2_step40: iteration: 15500: Loss: 0.886257, lr: 0.008498
2018-08-11 04:25:11,771:CRITICAL:EmoV2_step40: iteration: 15600: Loss: 0.982172, lr: 0.008482
2018-08-11 04:25:24,935:CRITICAL:EmoV2_step40: iteration: 15700: Loss: 0.951558, lr: 0.008465
2018-08-11 04:25:38,102:CRITICAL:EmoV2_step40: iteration: 15800: Loss: 1.000326, lr: 0.008449
2018-08-11 04:25:51,262:CRITICAL:EmoV2_step40: iteration: 15900: Loss: 0.930203, lr: 0.008433
2018-08-11 04:26:04,424:CRITICAL:EmoV2_step40: iteration: 16000: Loss: 0.831304, lr: 0.008417
2018-08-11 04:26:09,265:CRITICAL:EmoV2_step40: validate. Iteration: 16000: Accuracy (valence, arousal): 1.770% 4.425%
2018-08-11 04:26:09,266:CRITICAL:EmoV2_step40: validate. Iteration: 16000: Loss: nan
2018-08-11 04:26:22,449:CRITICAL:EmoV2_step40: iteration: 16100: Loss: 1.034701, lr: 0.008401
2018-08-11 04:26:35,612:CRITICAL:EmoV2_step40: iteration: 16200: Loss: 1.164363, lr: 0.008384
2018-08-11 04:26:53,382:CRITICAL:EmoV2_step40: iteration: 16300: Loss: 1.018721, lr: 0.008368
2018-08-11 04:27:06,539:CRITICAL:EmoV2_step40: iteration: 16400: Loss: 0.961178, lr: 0.008352
2018-08-11 04:27:19,703:CRITICAL:EmoV2_step40: iteration: 16500: Loss: 0.887809, lr: 0.008336
2018-08-11 04:27:32,857:CRITICAL:EmoV2_step40: iteration: 16600: Loss: 0.795575, lr: 0.008320
2018-08-11 04:27:46,016:CRITICAL:EmoV2_step40: iteration: 16700: Loss: 0.895157, lr: 0.008303
2018-08-11 04:27:59,170:CRITICAL:EmoV2_step40: iteration: 16800: Loss: 0.865992, lr: 0.008287
2018-08-11 04:28:16,784:CRITICAL:EmoV2_step40: iteration: 16900: Loss: 0.780577, lr: 0.008271
2018-08-11 04:28:30,048:CRITICAL:EmoV2_step40: iteration: 17000: Loss: 1.104486, lr: 0.008255
2018-08-11 04:28:35,062:CRITICAL:EmoV2_step40: validate. Iteration: 17000: Accuracy (valence, arousal): 9.292% 1.770%
2018-08-11 04:28:35,063:CRITICAL:EmoV2_step40: validate. Iteration: 17000: Loss: nan
2018-08-11 04:28:48,207:CRITICAL:EmoV2_step40: iteration: 17100: Loss: 1.015565, lr: 0.008239
2018-08-11 04:29:01,368:CRITICAL:EmoV2_step40: iteration: 17200: Loss: 0.642538, lr: 0.008222
2018-08-11 04:29:14,514:CRITICAL:EmoV2_step40: iteration: 17300: Loss: 0.912722, lr: 0.008206
2018-08-11 04:29:27,658:CRITICAL:EmoV2_step40: iteration: 17400: Loss: 0.894945, lr: 0.008190
2018-08-11 04:29:40,797:CRITICAL:EmoV2_step40: iteration: 17500: Loss: 0.822461, lr: 0.008174
2018-08-11 04:29:53,954:CRITICAL:EmoV2_step40: iteration: 17600: Loss: 0.971022, lr: 0.008158
2018-08-11 04:30:07,130:CRITICAL:EmoV2_step40: iteration: 17700: Loss: 1.010285, lr: 0.008141
2018-08-11 04:30:20,307:CRITICAL:EmoV2_step40: iteration: 17800: Loss: 0.943857, lr: 0.008125
2018-08-11 04:30:33,488:CRITICAL:EmoV2_step40: iteration: 17900: Loss: 0.827371, lr: 0.008109
2018-08-11 04:30:46,645:CRITICAL:EmoV2_step40: iteration: 18000: Loss: 0.818741, lr: 0.008093
2018-08-11 04:30:51,386:CRITICAL:EmoV2_step40: validate. Iteration: 18000: Accuracy (valence, arousal): 0.885% 6.195%
2018-08-11 04:30:51,386:CRITICAL:EmoV2_step40: validate. Iteration: 18000: Loss: nan
2018-08-11 04:31:04,558:CRITICAL:EmoV2_step40: iteration: 18100: Loss: 0.830769, lr: 0.008077
2018-08-11 04:31:17,719:CRITICAL:EmoV2_step40: iteration: 18200: Loss: 0.882572, lr: 0.008060
2018-08-11 04:31:30,947:CRITICAL:EmoV2_step40: iteration: 18300: Loss: 0.974515, lr: 0.008044
2018-08-11 04:31:44,118:CRITICAL:EmoV2_step40: iteration: 18400: Loss: 1.163705, lr: 0.008028
2018-08-11 04:31:57,272:CRITICAL:EmoV2_step40: iteration: 18500: Loss: 0.934381, lr: 0.008012
2018-08-11 04:32:10,412:CRITICAL:EmoV2_step40: iteration: 18600: Loss: 0.966549, lr: 0.007996
2018-08-11 04:32:28,029:CRITICAL:EmoV2_step40: iteration: 18700: Loss: 1.178406, lr: 0.007979
2018-08-11 04:32:41,190:CRITICAL:EmoV2_step40: iteration: 18800: Loss: 0.873899, lr: 0.007963
2018-08-11 04:32:58,947:CRITICAL:EmoV2_step40: iteration: 18900: Loss: 0.916723, lr: 0.007947
2018-08-11 04:33:12,097:CRITICAL:EmoV2_step40: iteration: 19000: Loss: 0.953095, lr: 0.007931
2018-08-11 04:33:17,262:CRITICAL:EmoV2_step40: validate. Iteration: 19000: Accuracy (valence, arousal): 7.522% 2.655%
2018-08-11 04:33:17,262:CRITICAL:EmoV2_step40: validate. Iteration: 19000: Loss: nan
2018-08-11 04:33:30,418:CRITICAL:EmoV2_step40: iteration: 19100: Loss: 0.979602, lr: 0.007915
2018-08-11 04:33:43,567:CRITICAL:EmoV2_step40: iteration: 19200: Loss: 0.647283, lr: 0.007898
2018-08-11 04:33:56,729:CRITICAL:EmoV2_step40: iteration: 19300: Loss: 0.979923, lr: 0.007882
2018-08-11 04:34:09,880:CRITICAL:EmoV2_step40: iteration: 19400: Loss: 1.032061, lr: 0.007866
2018-08-11 04:34:23,050:CRITICAL:EmoV2_step40: iteration: 19500: Loss: 0.891668, lr: 0.007850
2018-08-11 04:34:36,217:CRITICAL:EmoV2_step40: iteration: 19600: Loss: 1.189993, lr: 0.007834
2018-08-11 04:34:49,377:CRITICAL:EmoV2_step40: iteration: 19700: Loss: 1.057076, lr: 0.007817
2018-08-11 04:35:02,524:CRITICAL:EmoV2_step40: iteration: 19800: Loss: 1.055541, lr: 0.007801
2018-08-11 04:35:15,666:CRITICAL:EmoV2_step40: iteration: 19900: Loss: 1.203557, lr: 0.007785
2018-08-11 04:36:16,980:CRITICAL:EmoV2_step40: iteration: 20000: Loss: 0.971233, lr: 0.007769
2018-08-11 04:36:24,074:CRITICAL:EmoV2_step40: validate. Iteration: 20000: Accuracy (valence, arousal): 16.372% 14.602%
2018-08-11 04:36:24,074:CRITICAL:EmoV2_step40: validate. Iteration: 20000: Loss: nan
2018-08-11 04:36:37,154:CRITICAL:EmoV2_step40: iteration: 20100: Loss: 1.066128, lr: 0.007753
2018-08-11 04:36:50,218:CRITICAL:EmoV2_step40: iteration: 20200: Loss: 0.937912, lr: 0.007736
2018-08-11 04:37:03,300:CRITICAL:EmoV2_step40: iteration: 20300: Loss: 0.943611, lr: 0.007720
2018-08-11 04:37:20,779:CRITICAL:EmoV2_step40: iteration: 20400: Loss: 0.507332, lr: 0.007704
2018-08-11 04:37:33,858:CRITICAL:EmoV2_step40: iteration: 20500: Loss: 1.086730, lr: 0.007688
2018-08-11 04:37:46,963:CRITICAL:EmoV2_step40: iteration: 20600: Loss: 0.989170, lr: 0.007672
2018-08-11 04:38:00,085:CRITICAL:EmoV2_step40: iteration: 20700: Loss: 0.713217, lr: 0.007655
2018-08-11 04:38:13,182:CRITICAL:EmoV2_step40: iteration: 20800: Loss: 0.869025, lr: 0.007639
2018-08-11 04:38:26,335:CRITICAL:EmoV2_step40: iteration: 20900: Loss: 0.840571, lr: 0.007623
2018-08-11 04:38:39,476:CRITICAL:EmoV2_step40: iteration: 21000: Loss: 0.991742, lr: 0.007607
2018-08-11 04:38:44,358:CRITICAL:EmoV2_step40: validate. Iteration: 21000: Accuracy (valence, arousal): 4.425% 7.080%
2018-08-11 04:38:44,359:CRITICAL:EmoV2_step40: validate. Iteration: 21000: Loss: nan
2018-08-11 04:38:57,513:CRITICAL:EmoV2_step40: iteration: 21100: Loss: 0.748460, lr: 0.007591
2018-08-11 04:39:10,685:CRITICAL:EmoV2_step40: iteration: 21200: Loss: 0.997533, lr: 0.007574
2018-08-11 04:39:23,843:CRITICAL:EmoV2_step40: iteration: 21300: Loss: 0.986025, lr: 0.007558
2018-08-11 04:39:41,641:CRITICAL:EmoV2_step40: iteration: 21400: Loss: 0.862281, lr: 0.007542
2018-08-11 04:39:54,799:CRITICAL:EmoV2_step40: iteration: 21500: Loss: 0.865598, lr: 0.007526
2018-08-11 04:40:07,988:CRITICAL:EmoV2_step40: iteration: 21600: Loss: 0.757573, lr: 0.007510
2018-08-11 04:40:21,143:CRITICAL:EmoV2_step40: iteration: 21700: Loss: 0.777399, lr: 0.007493
2018-08-11 04:40:34,285:CRITICAL:EmoV2_step40: iteration: 21800: Loss: 0.916143, lr: 0.007477
2018-08-11 04:40:47,459:CRITICAL:EmoV2_step40: iteration: 21900: Loss: 1.316868, lr: 0.007461
2018-08-11 04:41:05,019:CRITICAL:EmoV2_step40: iteration: 22000: Loss: 0.793278, lr: 0.007445
2018-08-11 04:41:09,879:CRITICAL:EmoV2_step40: validate. Iteration: 22000: Accuracy (valence, arousal): 11.062% 15.044%
2018-08-11 04:41:09,879:CRITICAL:EmoV2_step40: validate. Iteration: 22000: Loss: nan
2018-08-11 04:41:23,028:CRITICAL:EmoV2_step40: iteration: 22100: Loss: 0.977782, lr: 0.007429
2018-08-11 04:41:36,175:CRITICAL:EmoV2_step40: iteration: 22200: Loss: 0.954262, lr: 0.007412
2018-08-11 04:41:49,327:CRITICAL:EmoV2_step40: iteration: 22300: Loss: 1.002270, lr: 0.007396
2018-08-11 04:42:02,481:CRITICAL:EmoV2_step40: iteration: 22400: Loss: 1.115666, lr: 0.007380
2018-08-11 04:42:15,624:CRITICAL:EmoV2_step40: iteration: 22500: Loss: 0.778234, lr: 0.007364
2018-08-11 04:42:28,778:CRITICAL:EmoV2_step40: iteration: 22600: Loss: 0.889350, lr: 0.007348
2018-08-11 04:42:41,912:CRITICAL:EmoV2_step40: iteration: 22700: Loss: 1.252959, lr: 0.007331
2018-08-11 04:42:55,059:CRITICAL:EmoV2_step40: iteration: 22800: Loss: 0.773089, lr: 0.007315
2018-08-11 04:43:08,201:CRITICAL:EmoV2_step40: iteration: 22900: Loss: 0.800573, lr: 0.007299
2018-08-11 04:43:21,386:CRITICAL:EmoV2_step40: iteration: 23000: Loss: 1.144270, lr: 0.007283
2018-08-11 04:43:26,238:CRITICAL:EmoV2_step40: validate. Iteration: 23000: Accuracy (valence, arousal): 16.814% 14.159%
2018-08-11 04:43:26,238:CRITICAL:EmoV2_step40: validate. Iteration: 23000: Loss: nan
2018-08-11 04:43:39,417:CRITICAL:EmoV2_step40: iteration: 23100: Loss: 0.675396, lr: 0.007267
2018-08-11 04:43:52,577:CRITICAL:EmoV2_step40: iteration: 23200: Loss: 1.101656, lr: 0.007250
2018-08-11 04:44:05,728:CRITICAL:EmoV2_step40: iteration: 23300: Loss: 1.010865, lr: 0.007234
2018-08-11 04:44:18,894:CRITICAL:EmoV2_step40: iteration: 23400: Loss: 1.168725, lr: 0.007218
2018-08-11 04:44:32,063:CRITICAL:EmoV2_step40: iteration: 23500: Loss: 0.899303, lr: 0.007202
2018-08-11 04:44:45,212:CRITICAL:EmoV2_step40: iteration: 23600: Loss: 1.049809, lr: 0.007186
2018-08-11 04:45:02,795:CRITICAL:EmoV2_step40: iteration: 23700: Loss: 0.969784, lr: 0.007169
2018-08-11 04:45:15,951:CRITICAL:EmoV2_step40: iteration: 23800: Loss: 0.977534, lr: 0.007153
2018-08-11 04:45:33,716:CRITICAL:EmoV2_step40: iteration: 23900: Loss: 0.641602, lr: 0.007137
2018-08-11 04:45:46,860:CRITICAL:EmoV2_step40: iteration: 24000: Loss: 1.056763, lr: 0.007121
2018-08-11 04:45:51,993:CRITICAL:EmoV2_step40: validate. Iteration: 24000: Accuracy (valence, arousal): 23.894% 15.044%
2018-08-11 04:45:51,994:CRITICAL:EmoV2_step40: validate. Iteration: 24000: Loss: nan
2018-08-11 04:46:05,157:CRITICAL:EmoV2_step40: iteration: 24100: Loss: 1.143624, lr: 0.007105
2018-08-11 04:46:18,335:CRITICAL:EmoV2_step40: iteration: 24200: Loss: 0.975313, lr: 0.007088
2018-08-11 04:46:31,504:CRITICAL:EmoV2_step40: iteration: 24300: Loss: 1.122109, lr: 0.007072
2018-08-11 04:46:44,643:CRITICAL:EmoV2_step40: iteration: 24400: Loss: 1.191188, lr: 0.007056
2018-08-11 04:46:57,788:CRITICAL:EmoV2_step40: iteration: 24500: Loss: 0.812489, lr: 0.007040
2018-08-11 04:47:10,927:CRITICAL:EmoV2_step40: iteration: 24600: Loss: 0.925631, lr: 0.007024
2018-08-11 04:47:24,098:CRITICAL:EmoV2_step40: iteration: 24700: Loss: 0.732169, lr: 0.007007
2018-08-11 04:47:37,260:CRITICAL:EmoV2_step40: iteration: 24800: Loss: 0.794585, lr: 0.006991
2018-08-11 04:47:50,410:CRITICAL:EmoV2_step40: iteration: 24900: Loss: 1.066377, lr: 0.006975
2018-08-11 04:48:03,566:CRITICAL:EmoV2_step40: iteration: 25000: Loss: 0.958652, lr: 0.006959
2018-08-11 04:48:08,404:CRITICAL:EmoV2_step40: validate. Iteration: 25000: Accuracy (valence, arousal): 7.080% 15.487%
2018-08-11 04:48:08,404:CRITICAL:EmoV2_step40: validate. Iteration: 25000: Loss: nan
2018-08-11 04:48:21,575:CRITICAL:EmoV2_step40: iteration: 25100: Loss: 1.246258, lr: 0.006943
2018-08-11 04:48:34,725:CRITICAL:EmoV2_step40: iteration: 25200: Loss: 0.928467, lr: 0.006926
2018-08-11 04:48:47,868:CRITICAL:EmoV2_step40: iteration: 25300: Loss: 0.798627, lr: 0.006910
2018-08-11 04:49:05,494:CRITICAL:EmoV2_step40: iteration: 25400: Loss: 0.938181, lr: 0.006894
2018-08-11 04:49:18,651:CRITICAL:EmoV2_step40: iteration: 25500: Loss: 0.929303, lr: 0.006878
2018-08-11 04:49:31,798:CRITICAL:EmoV2_step40: iteration: 25600: Loss: 0.941755, lr: 0.006862
2018-08-11 04:49:44,948:CRITICAL:EmoV2_step40: iteration: 25700: Loss: 0.807932, lr: 0.006845
2018-08-11 04:49:58,105:CRITICAL:EmoV2_step40: iteration: 25800: Loss: 0.791524, lr: 0.006829
2018-08-11 04:50:11,280:CRITICAL:EmoV2_step40: iteration: 25900: Loss: 0.981782, lr: 0.006813
2018-08-11 04:50:24,455:CRITICAL:EmoV2_step40: iteration: 26000: Loss: 0.779223, lr: 0.006797
2018-08-11 04:50:29,239:CRITICAL:EmoV2_step40: validate. Iteration: 26000: Accuracy (valence, arousal): 16.814% 21.239%
2018-08-11 04:50:29,239:CRITICAL:EmoV2_step40: validate. Iteration: 26000: Loss: nan
2018-08-11 04:50:42,397:CRITICAL:EmoV2_step40: iteration: 26100: Loss: 0.931035, lr: 0.006781
2018-08-11 04:50:55,595:CRITICAL:EmoV2_step40: iteration: 26200: Loss: 0.648389, lr: 0.006764
2018-08-11 04:51:08,755:CRITICAL:EmoV2_step40: iteration: 26300: Loss: 1.209348, lr: 0.006748
2018-08-11 04:51:26,745:CRITICAL:EmoV2_step40: iteration: 26400: Loss: 1.086264, lr: 0.006732
2018-08-11 04:51:39,893:CRITICAL:EmoV2_step40: iteration: 26500: Loss: 0.508126, lr: 0.006716
2018-08-11 04:51:53,048:CRITICAL:EmoV2_step40: iteration: 26600: Loss: 0.918943, lr: 0.006700
2018-08-11 04:52:06,213:CRITICAL:EmoV2_step40: iteration: 26700: Loss: 0.543595, lr: 0.006683
2018-08-11 04:52:19,366:CRITICAL:EmoV2_step40: iteration: 26800: Loss: 1.056710, lr: 0.006667
2018-08-11 04:52:32,529:CRITICAL:EmoV2_step40: iteration: 26900: Loss: 1.198231, lr: 0.006651
2018-08-11 04:52:50,078:CRITICAL:EmoV2_step40: iteration: 27000: Loss: 0.581698, lr: 0.006635
2018-08-11 04:52:54,941:CRITICAL:EmoV2_step40: validate. Iteration: 27000: Accuracy (valence, arousal): 14.159% 19.912%
2018-08-11 04:52:54,942:CRITICAL:EmoV2_step40: validate. Iteration: 27000: Loss: nan
2018-08-11 04:53:08,081:CRITICAL:EmoV2_step40: iteration: 27100: Loss: 1.205419, lr: 0.006619
2018-08-11 04:53:21,231:CRITICAL:EmoV2_step40: iteration: 27200: Loss: 1.052611, lr: 0.006602
2018-08-11 04:53:34,389:CRITICAL:EmoV2_step40: iteration: 27300: Loss: 1.069852, lr: 0.006586
2018-08-11 04:53:47,540:CRITICAL:EmoV2_step40: iteration: 27400: Loss: 0.645169, lr: 0.006570
2018-08-11 04:54:00,678:CRITICAL:EmoV2_step40: iteration: 27500: Loss: 0.779707, lr: 0.006554
2018-08-11 04:54:13,827:CRITICAL:EmoV2_step40: iteration: 27600: Loss: 0.987142, lr: 0.006538
2018-08-11 04:54:26,979:CRITICAL:EmoV2_step40: iteration: 27700: Loss: 0.937823, lr: 0.006521
2018-08-11 04:54:40,149:CRITICAL:EmoV2_step40: iteration: 27800: Loss: 1.047525, lr: 0.006505
2018-08-11 04:54:53,307:CRITICAL:EmoV2_step40: iteration: 27900: Loss: 0.837140, lr: 0.006489
2018-08-11 04:55:06,481:CRITICAL:EmoV2_step40: iteration: 28000: Loss: 0.747587, lr: 0.006473
2018-08-11 04:55:11,362:CRITICAL:EmoV2_step40: validate. Iteration: 28000: Accuracy (valence, arousal): 29.204% 16.372%
2018-08-11 04:55:11,362:CRITICAL:EmoV2_step40: validate. Iteration: 28000: Loss: nan
2018-08-11 04:55:24,525:CRITICAL:EmoV2_step40: iteration: 28100: Loss: 0.739466, lr: 0.006457
2018-08-11 04:55:37,679:CRITICAL:EmoV2_step40: iteration: 28200: Loss: 0.956364, lr: 0.006440
2018-08-11 04:55:50,843:CRITICAL:EmoV2_step40: iteration: 28300: Loss: 0.786208, lr: 0.006424
2018-08-11 04:56:03,981:CRITICAL:EmoV2_step40: iteration: 28400: Loss: 0.727716, lr: 0.006408
2018-08-11 04:56:17,137:CRITICAL:EmoV2_step40: iteration: 28500: Loss: 0.666260, lr: 0.006392
2018-08-11 04:56:30,302:CRITICAL:EmoV2_step40: iteration: 28600: Loss: 1.107993, lr: 0.006376
2018-08-11 04:56:47,892:CRITICAL:EmoV2_step40: iteration: 28700: Loss: 0.823110, lr: 0.006359
2018-08-11 04:57:01,026:CRITICAL:EmoV2_step40: iteration: 28800: Loss: 1.305519, lr: 0.006343
2018-08-11 04:57:18,777:CRITICAL:EmoV2_step40: iteration: 28900: Loss: 0.564313, lr: 0.006327
2018-08-11 04:57:31,911:CRITICAL:EmoV2_step40: iteration: 29000: Loss: 0.686862, lr: 0.006311
2018-08-11 04:57:36,610:CRITICAL:EmoV2_step40: validate. Iteration: 29000: Accuracy (valence, arousal): 24.336% 18.584%
2018-08-11 04:57:36,611:CRITICAL:EmoV2_step40: validate. Iteration: 29000: Loss: nan
2018-08-11 04:57:49,751:CRITICAL:EmoV2_step40: iteration: 29100: Loss: 0.988265, lr: 0.006295
2018-08-11 04:58:02,890:CRITICAL:EmoV2_step40: iteration: 29200: Loss: 0.705412, lr: 0.006278
2018-08-11 04:58:16,046:CRITICAL:EmoV2_step40: iteration: 29300: Loss: 0.791544, lr: 0.006262
2018-08-11 04:58:29,186:CRITICAL:EmoV2_step40: iteration: 29400: Loss: 0.562995, lr: 0.006246
2018-08-11 04:58:42,339:CRITICAL:EmoV2_step40: iteration: 29500: Loss: 0.880470, lr: 0.006230
2018-08-11 04:58:55,492:CRITICAL:EmoV2_step40: iteration: 29600: Loss: 0.879873, lr: 0.006214
2018-08-11 04:59:08,642:CRITICAL:EmoV2_step40: iteration: 29700: Loss: 0.668789, lr: 0.006197
2018-08-11 04:59:21,804:CRITICAL:EmoV2_step40: iteration: 29800: Loss: 1.082849, lr: 0.006181
2018-08-11 04:59:34,966:CRITICAL:EmoV2_step40: iteration: 29900: Loss: 1.116628, lr: 0.006165
2018-08-11 05:00:35,298:CRITICAL:EmoV2_step40: iteration: 30000: Loss: 0.422242, lr: 0.006149
2018-08-11 05:00:40,198:CRITICAL:EmoV2_step40: validate. Iteration: 30000: Accuracy (valence, arousal): 21.681% 13.717%
2018-08-11 05:00:40,198:CRITICAL:EmoV2_step40: validate. Iteration: 30000: Loss: nan
2018-08-11 05:00:53,262:CRITICAL:EmoV2_step40: iteration: 30100: Loss: 0.934975, lr: 0.006133
2018-08-11 05:01:06,335:CRITICAL:EmoV2_step40: iteration: 30200: Loss: 1.119255, lr: 0.006116
2018-08-11 05:01:20,775:CRITICAL:EmoV2_step40: iteration: 30300: Loss: 1.128380, lr: 0.006100
2018-08-11 05:01:38,255:CRITICAL:EmoV2_step40: iteration: 30400: Loss: 1.221753, lr: 0.006084
2018-08-11 05:01:51,373:CRITICAL:EmoV2_step40: iteration: 30500: Loss: 0.679762, lr: 0.006068
2018-08-11 05:02:04,485:CRITICAL:EmoV2_step40: iteration: 30600: Loss: 1.009875, lr: 0.006052
2018-08-11 05:02:17,587:CRITICAL:EmoV2_step40: iteration: 30700: Loss: 0.580588, lr: 0.006035
2018-08-11 05:02:30,704:CRITICAL:EmoV2_step40: iteration: 30800: Loss: 1.017615, lr: 0.006019
2018-08-11 05:02:43,854:CRITICAL:EmoV2_step40: iteration: 30900: Loss: 1.105824, lr: 0.006003
2018-08-11 05:02:57,006:CRITICAL:EmoV2_step40: iteration: 31000: Loss: 0.498893, lr: 0.005987
2018-08-11 05:03:01,913:CRITICAL:EmoV2_step40: validate. Iteration: 31000: Accuracy (valence, arousal): 33.186% 18.142%
2018-08-11 05:03:01,914:CRITICAL:EmoV2_step40: validate. Iteration: 31000: Loss: nan
2018-08-11 05:03:15,067:CRITICAL:EmoV2_step40: iteration: 31100: Loss: 0.748378, lr: 0.005971
2018-08-11 05:03:28,224:CRITICAL:EmoV2_step40: iteration: 31200: Loss: 0.967443, lr: 0.005954
2018-08-11 05:03:41,383:CRITICAL:EmoV2_step40: iteration: 31300: Loss: 1.043146, lr: 0.005938
2018-08-11 05:03:59,161:CRITICAL:EmoV2_step40: iteration: 31400: Loss: 0.582806, lr: 0.005922
2018-08-11 05:04:12,326:CRITICAL:EmoV2_step40: iteration: 31500: Loss: 0.920476, lr: 0.005906
2018-08-11 05:04:25,475:CRITICAL:EmoV2_step40: iteration: 31600: Loss: 0.635106, lr: 0.005890
2018-08-11 05:04:38,612:CRITICAL:EmoV2_step40: iteration: 31700: Loss: 0.712381, lr: 0.005873
2018-08-11 05:04:51,784:CRITICAL:EmoV2_step40: iteration: 31800: Loss: 0.626569, lr: 0.005857
2018-08-11 05:05:04,929:CRITICAL:EmoV2_step40: iteration: 31900: Loss: 0.522439, lr: 0.005841
2018-08-11 05:05:18,077:CRITICAL:EmoV2_step40: iteration: 32000: Loss: 1.185747, lr: 0.005825
2018-08-11 05:05:22,758:CRITICAL:EmoV2_step40: validate. Iteration: 32000: Accuracy (valence, arousal): 16.372% 15.929%
2018-08-11 05:05:22,758:CRITICAL:EmoV2_step40: validate. Iteration: 32000: Loss: nan
2018-08-11 05:05:40,299:CRITICAL:EmoV2_step40: iteration: 32100: Loss: 0.466875, lr: 0.005809
2018-08-11 05:05:53,461:CRITICAL:EmoV2_step40: iteration: 32200: Loss: 0.965506, lr: 0.005792
2018-08-11 05:06:06,618:CRITICAL:EmoV2_step40: iteration: 32300: Loss: 0.652407, lr: 0.005776
2018-08-11 05:06:19,771:CRITICAL:EmoV2_step40: iteration: 32400: Loss: 0.859452, lr: 0.005760
2018-08-11 05:06:32,933:CRITICAL:EmoV2_step40: iteration: 32500: Loss: 1.132741, lr: 0.005744
2018-08-11 05:06:46,094:CRITICAL:EmoV2_step40: iteration: 32600: Loss: 0.752240, lr: 0.005728
2018-08-11 05:06:59,252:CRITICAL:EmoV2_step40: iteration: 32700: Loss: 0.721349, lr: 0.005711
2018-08-11 05:07:12,404:CRITICAL:EmoV2_step40: iteration: 32800: Loss: 0.799928, lr: 0.005695
2018-08-11 05:07:25,567:CRITICAL:EmoV2_step40: iteration: 32900: Loss: 0.957761, lr: 0.005679
2018-08-11 05:07:38,728:CRITICAL:EmoV2_step40: iteration: 33000: Loss: 0.853848, lr: 0.005663
2018-08-11 05:07:43,474:CRITICAL:EmoV2_step40: validate. Iteration: 33000: Accuracy (valence, arousal): 20.796% 15.044%
2018-08-11 05:07:43,475:CRITICAL:EmoV2_step40: validate. Iteration: 33000: Loss: nan
2018-08-11 05:07:56,637:CRITICAL:EmoV2_step40: iteration: 33100: Loss: 0.832941, lr: 0.005647
2018-08-11 05:08:09,806:CRITICAL:EmoV2_step40: iteration: 33200: Loss: 0.608993, lr: 0.005630
2018-08-11 05:08:22,985:CRITICAL:EmoV2_step40: iteration: 33300: Loss: 0.932901, lr: 0.005614
2018-08-11 05:08:36,148:CRITICAL:EmoV2_step40: iteration: 33400: Loss: 0.874440, lr: 0.005598
2018-08-11 05:08:49,302:CRITICAL:EmoV2_step40: iteration: 33500: Loss: 0.942546, lr: 0.005582
2018-08-11 05:09:02,460:CRITICAL:EmoV2_step40: iteration: 33600: Loss: 0.970192, lr: 0.005566
2018-08-11 05:09:15,619:CRITICAL:EmoV2_step40: iteration: 33700: Loss: 0.907459, lr: 0.005549
2018-08-11 05:09:33,150:CRITICAL:EmoV2_step40: iteration: 33800: Loss: 0.774749, lr: 0.005533
2018-08-11 05:09:50,951:CRITICAL:EmoV2_step40: iteration: 33900: Loss: 0.668044, lr: 0.005517
2018-08-11 05:10:04,085:CRITICAL:EmoV2_step40: iteration: 34000: Loss: 0.834078, lr: 0.005501
2018-08-11 05:10:08,708:CRITICAL:EmoV2_step40: validate. Iteration: 34000: Accuracy (valence, arousal): 23.009% 20.796%
2018-08-11 05:10:08,709:CRITICAL:EmoV2_step40: validate. Iteration: 34000: Loss: nan
2018-08-11 05:10:21,846:CRITICAL:EmoV2_step40: iteration: 34100: Loss: 0.933756, lr: 0.005485
2018-08-11 05:10:35,010:CRITICAL:EmoV2_step40: iteration: 34200: Loss: 0.855551, lr: 0.005468
2018-08-11 05:10:48,175:CRITICAL:EmoV2_step40: iteration: 34300: Loss: 0.742754, lr: 0.005452
2018-08-11 05:11:01,340:CRITICAL:EmoV2_step40: iteration: 34400: Loss: 0.880839, lr: 0.005436
2018-08-11 05:11:14,476:CRITICAL:EmoV2_step40: iteration: 34500: Loss: 0.766979, lr: 0.005420
2018-08-11 05:11:27,631:CRITICAL:EmoV2_step40: iteration: 34600: Loss: 1.108064, lr: 0.005404
2018-08-11 05:11:40,781:CRITICAL:EmoV2_step40: iteration: 34700: Loss: 0.978488, lr: 0.005387
2018-08-11 05:11:53,946:CRITICAL:EmoV2_step40: iteration: 34800: Loss: 0.564054, lr: 0.005371
2018-08-11 05:12:07,105:CRITICAL:EmoV2_step40: iteration: 34900: Loss: 0.927983, lr: 0.005355
2018-08-11 05:12:20,271:CRITICAL:EmoV2_step40: iteration: 35000: Loss: 0.770746, lr: 0.005339
2018-08-11 05:12:25,161:CRITICAL:EmoV2_step40: validate. Iteration: 35000: Accuracy (valence, arousal): 26.549% 22.124%
2018-08-11 05:12:25,162:CRITICAL:EmoV2_step40: validate. Iteration: 35000: Loss: nan
2018-08-11 05:12:38,304:CRITICAL:EmoV2_step40: iteration: 35100: Loss: 0.763632, lr: 0.005323
2018-08-11 05:12:51,480:CRITICAL:EmoV2_step40: iteration: 35200: Loss: 0.798311, lr: 0.005306
2018-08-11 05:13:04,655:CRITICAL:EmoV2_step40: iteration: 35300: Loss: 0.938527, lr: 0.005290
2018-08-11 05:13:22,212:CRITICAL:EmoV2_step40: iteration: 35400: Loss: 0.752482, lr: 0.005274
2018-08-11 05:13:35,348:CRITICAL:EmoV2_step40: iteration: 35500: Loss: 0.770017, lr: 0.005258
2018-08-11 05:13:48,497:CRITICAL:EmoV2_step40: iteration: 35600: Loss: 0.857653, lr: 0.005242
2018-08-11 05:14:01,635:CRITICAL:EmoV2_step40: iteration: 35700: Loss: 1.016063, lr: 0.005225
2018-08-11 05:14:14,778:CRITICAL:EmoV2_step40: iteration: 35800: Loss: 1.076297, lr: 0.005209
2018-08-11 05:14:27,951:CRITICAL:EmoV2_step40: iteration: 35900: Loss: 0.755293, lr: 0.005193
2018-08-11 05:14:41,101:CRITICAL:EmoV2_step40: iteration: 36000: Loss: 0.544415, lr: 0.005177
2018-08-11 05:14:45,811:CRITICAL:EmoV2_step40: validate. Iteration: 36000: Accuracy (valence, arousal): 19.912% 23.009%
2018-08-11 05:14:45,811:CRITICAL:EmoV2_step40: validate. Iteration: 36000: Loss: nan
2018-08-11 05:14:58,966:CRITICAL:EmoV2_step40: iteration: 36100: Loss: 1.090077, lr: 0.005161
2018-08-11 05:15:12,119:CRITICAL:EmoV2_step40: iteration: 36200: Loss: 0.910525, lr: 0.005144
2018-08-11 05:15:25,273:CRITICAL:EmoV2_step40: iteration: 36300: Loss: 0.806314, lr: 0.005128
2018-08-11 05:15:43,047:CRITICAL:EmoV2_step40: iteration: 36400: Loss: 0.679860, lr: 0.005112
2018-08-11 05:15:56,192:CRITICAL:EmoV2_step40: iteration: 36500: Loss: 0.672257, lr: 0.005096
2018-08-11 05:16:09,348:CRITICAL:EmoV2_step40: iteration: 36600: Loss: 1.276883, lr: 0.005080
2018-08-11 05:16:22,494:CRITICAL:EmoV2_step40: iteration: 36700: Loss: 0.695988, lr: 0.005063
2018-08-11 05:16:35,659:CRITICAL:EmoV2_step40: iteration: 36800: Loss: 0.413739, lr: 0.005047
2018-08-11 05:16:48,808:CRITICAL:EmoV2_step40: iteration: 36900: Loss: 0.663960, lr: 0.005031
2018-08-11 05:17:01,943:CRITICAL:EmoV2_step40: iteration: 37000: Loss: 0.623727, lr: 0.005015
2018-08-11 05:17:06,816:CRITICAL:EmoV2_step40: validate. Iteration: 37000: Accuracy (valence, arousal): 29.646% 18.584%
2018-08-11 05:17:06,816:CRITICAL:EmoV2_step40: validate. Iteration: 37000: Loss: nan
2018-08-11 05:17:24,369:CRITICAL:EmoV2_step40: iteration: 37100: Loss: 0.737032, lr: 0.004999
2018-08-11 05:17:37,501:CRITICAL:EmoV2_step40: iteration: 37200: Loss: 0.562348, lr: 0.004982
2018-08-11 05:17:50,644:CRITICAL:EmoV2_step40: iteration: 37300: Loss: 1.142277, lr: 0.004966
2018-08-11 05:18:03,776:CRITICAL:EmoV2_step40: iteration: 37400: Loss: 0.876735, lr: 0.004950
2018-08-11 05:18:16,918:CRITICAL:EmoV2_step40: iteration: 37500: Loss: 0.667117, lr: 0.004934
2018-08-11 05:18:30,056:CRITICAL:EmoV2_step40: iteration: 37600: Loss: 0.513810, lr: 0.004918
2018-08-11 05:18:43,206:CRITICAL:EmoV2_step40: iteration: 37700: Loss: 0.811649, lr: 0.004901
2018-08-11 05:18:56,385:CRITICAL:EmoV2_step40: iteration: 37800: Loss: 0.713137, lr: 0.004885
2018-08-11 05:19:09,537:CRITICAL:EmoV2_step40: iteration: 37900: Loss: 0.710850, lr: 0.004869
2018-08-11 05:19:22,692:CRITICAL:EmoV2_step40: iteration: 38000: Loss: 0.605449, lr: 0.004853
2018-08-11 05:19:27,358:CRITICAL:EmoV2_step40: validate. Iteration: 38000: Accuracy (valence, arousal): 27.434% 16.814%
2018-08-11 05:19:27,359:CRITICAL:EmoV2_step40: validate. Iteration: 38000: Loss: nan
2018-08-11 05:19:40,537:CRITICAL:EmoV2_step40: iteration: 38100: Loss: 0.670529, lr: 0.004837
2018-08-11 05:19:53,706:CRITICAL:EmoV2_step40: iteration: 38200: Loss: 0.766534, lr: 0.004820
2018-08-11 05:20:06,864:CRITICAL:EmoV2_step40: iteration: 38300: Loss: 0.534145, lr: 0.004804
2018-08-11 05:20:20,016:CRITICAL:EmoV2_step40: iteration: 38400: Loss: 0.933488, lr: 0.004788
2018-08-11 05:20:33,175:CRITICAL:EmoV2_step40: iteration: 38500: Loss: 0.646768, lr: 0.004772
2018-08-11 05:20:46,357:CRITICAL:EmoV2_step40: iteration: 38600: Loss: 0.523080, lr: 0.004756
2018-08-11 05:21:03,868:CRITICAL:EmoV2_step40: iteration: 38700: Loss: 0.861780, lr: 0.004739
2018-08-11 05:21:17,031:CRITICAL:EmoV2_step40: iteration: 38800: Loss: 0.751036, lr: 0.004723
2018-08-11 05:21:34,770:CRITICAL:EmoV2_step40: iteration: 38900: Loss: 0.787723, lr: 0.004707
2018-08-11 05:21:47,919:CRITICAL:EmoV2_step40: iteration: 39000: Loss: 0.825251, lr: 0.004691
2018-08-11 05:21:52,727:CRITICAL:EmoV2_step40: validate. Iteration: 39000: Accuracy (valence, arousal): 30.973% 20.354%
2018-08-11 05:21:52,727:CRITICAL:EmoV2_step40: validate. Iteration: 39000: Loss: nan
2018-08-11 05:22:05,879:CRITICAL:EmoV2_step40: iteration: 39100: Loss: 0.649070, lr: 0.004675
2018-08-11 05:22:19,042:CRITICAL:EmoV2_step40: iteration: 39200: Loss: 0.763030, lr: 0.004658
2018-08-11 05:22:32,196:CRITICAL:EmoV2_step40: iteration: 39300: Loss: 0.509667, lr: 0.004642
2018-08-11 05:22:45,346:CRITICAL:EmoV2_step40: iteration: 39400: Loss: 1.027578, lr: 0.004626
2018-08-11 05:22:58,517:CRITICAL:EmoV2_step40: iteration: 39500: Loss: 0.696023, lr: 0.004610
2018-08-11 05:23:11,657:CRITICAL:EmoV2_step40: iteration: 39600: Loss: 0.671510, lr: 0.004594
2018-08-11 05:23:24,807:CRITICAL:EmoV2_step40: iteration: 39700: Loss: 0.431899, lr: 0.004577
2018-08-11 05:23:37,969:CRITICAL:EmoV2_step40: iteration: 39800: Loss: 0.694996, lr: 0.004561
2018-08-11 05:23:51,143:CRITICAL:EmoV2_step40: iteration: 39900: Loss: 0.352137, lr: 0.004545
2018-08-11 05:24:45,612:CRITICAL:EmoV2_step40: iteration: 40000: Loss: 0.553845, lr: 0.004529
2018-08-11 05:24:50,247:CRITICAL:EmoV2_step40: validate. Iteration: 40000: Accuracy (valence, arousal): 30.973% 15.044%
2018-08-11 05:24:50,248:CRITICAL:EmoV2_step40: validate. Iteration: 40000: Loss: nan
2018-08-11 05:25:03,321:CRITICAL:EmoV2_step40: iteration: 40100: Loss: 0.594438, lr: 0.004513
2018-08-11 05:25:16,387:CRITICAL:EmoV2_step40: iteration: 40200: Loss: 0.724799, lr: 0.004496
2018-08-11 05:25:30,291:CRITICAL:EmoV2_step40: iteration: 40300: Loss: 0.427996, lr: 0.004480
2018-08-11 05:25:47,774:CRITICAL:EmoV2_step40: iteration: 40400: Loss: 0.558916, lr: 0.004464
2018-08-11 05:26:00,887:CRITICAL:EmoV2_step40: iteration: 40500: Loss: 0.861425, lr: 0.004448
2018-08-11 05:26:13,984:CRITICAL:EmoV2_step40: iteration: 40600: Loss: 0.750573, lr: 0.004432
2018-08-11 05:26:27,108:CRITICAL:EmoV2_step40: iteration: 40700: Loss: 0.637375, lr: 0.004415
2018-08-11 05:26:40,204:CRITICAL:EmoV2_step40: iteration: 40800: Loss: 0.795699, lr: 0.004399
2018-08-11 05:26:53,352:CRITICAL:EmoV2_step40: iteration: 40900: Loss: 0.936374, lr: 0.004383
2018-08-11 05:27:06,503:CRITICAL:EmoV2_step40: iteration: 41000: Loss: 0.724177, lr: 0.004367
2018-08-11 05:27:11,405:CRITICAL:EmoV2_step40: validate. Iteration: 41000: Accuracy (valence, arousal): 28.319% 18.584%
2018-08-11 05:27:11,405:CRITICAL:EmoV2_step40: validate. Iteration: 41000: Loss: nan
2018-08-11 05:27:24,569:CRITICAL:EmoV2_step40: iteration: 41100: Loss: 0.985785, lr: 0.004351
2018-08-11 05:27:37,728:CRITICAL:EmoV2_step40: iteration: 41200: Loss: 1.170637, lr: 0.004334
2018-08-11 05:27:50,870:CRITICAL:EmoV2_step40: iteration: 41300: Loss: 0.868123, lr: 0.004318
2018-08-11 05:28:08,664:CRITICAL:EmoV2_step40: iteration: 41400: Loss: 0.605623, lr: 0.004302
2018-08-11 05:28:21,805:CRITICAL:EmoV2_step40: iteration: 41500: Loss: 0.545108, lr: 0.004286
2018-08-11 05:28:34,969:CRITICAL:EmoV2_step40: iteration: 41600: Loss: 0.738124, lr: 0.004270
2018-08-11 05:28:48,134:CRITICAL:EmoV2_step40: iteration: 41700: Loss: 0.784615, lr: 0.004253
2018-08-11 05:29:01,299:CRITICAL:EmoV2_step40: iteration: 41800: Loss: 0.638910, lr: 0.004237
2018-08-11 05:29:14,459:CRITICAL:EmoV2_step40: iteration: 41900: Loss: 0.783206, lr: 0.004221
2018-08-11 05:29:27,604:CRITICAL:EmoV2_step40: iteration: 42000: Loss: 1.275301, lr: 0.004205
2018-08-11 05:29:32,222:CRITICAL:EmoV2_step40: validate. Iteration: 42000: Accuracy (valence, arousal): 31.416% 15.929%
2018-08-11 05:29:32,223:CRITICAL:EmoV2_step40: validate. Iteration: 42000: Loss: nan
2018-08-11 05:29:49,807:CRITICAL:EmoV2_step40: iteration: 42100: Loss: 0.993913, lr: 0.004189
2018-08-11 05:30:02,948:CRITICAL:EmoV2_step40: iteration: 42200: Loss: 1.150918, lr: 0.004172
2018-08-11 05:30:16,101:CRITICAL:EmoV2_step40: iteration: 42300: Loss: 0.552950, lr: 0.004156
2018-08-11 05:30:29,250:CRITICAL:EmoV2_step40: iteration: 42400: Loss: 0.776854, lr: 0.004140
2018-08-11 05:30:42,394:CRITICAL:EmoV2_step40: iteration: 42500: Loss: 0.396256, lr: 0.004124
2018-08-11 05:30:55,560:CRITICAL:EmoV2_step40: iteration: 42600: Loss: 0.506491, lr: 0.004108
2018-08-11 05:31:08,701:CRITICAL:EmoV2_step40: iteration: 42700: Loss: 0.613293, lr: 0.004091
2018-08-11 05:31:21,863:CRITICAL:EmoV2_step40: iteration: 42800: Loss: 1.046525, lr: 0.004075
2018-08-11 05:31:35,026:CRITICAL:EmoV2_step40: iteration: 42900: Loss: 0.712527, lr: 0.004059
2018-08-11 05:31:48,188:CRITICAL:EmoV2_step40: iteration: 43000: Loss: 1.258199, lr: 0.004043
2018-08-11 05:31:52,952:CRITICAL:EmoV2_step40: validate. Iteration: 43000: Accuracy (valence, arousal): 25.664% 14.602%
2018-08-11 05:31:52,953:CRITICAL:EmoV2_step40: validate. Iteration: 43000: Loss: nan
2018-08-11 05:32:06,108:CRITICAL:EmoV2_step40: iteration: 43100: Loss: 0.960156, lr: 0.004027
2018-08-11 05:32:19,274:CRITICAL:EmoV2_step40: iteration: 43200: Loss: 0.743850, lr: 0.004010
2018-08-11 05:32:32,435:CRITICAL:EmoV2_step40: iteration: 43300: Loss: 0.656065, lr: 0.003994
2018-08-11 05:32:45,593:CRITICAL:EmoV2_step40: iteration: 43400: Loss: 0.628701, lr: 0.003978
2018-08-11 05:32:58,750:CRITICAL:EmoV2_step40: iteration: 43500: Loss: 0.648837, lr: 0.003962
2018-08-11 05:33:11,926:CRITICAL:EmoV2_step40: iteration: 43600: Loss: 0.454115, lr: 0.003946
2018-08-11 05:33:29,575:CRITICAL:EmoV2_step40: iteration: 43700: Loss: 0.541429, lr: 0.003929
2018-08-11 05:33:42,710:CRITICAL:EmoV2_step40: iteration: 43800: Loss: 0.911334, lr: 0.003913
2018-08-11 05:33:55,855:CRITICAL:EmoV2_step40: iteration: 43900: Loss: 0.467913, lr: 0.003897
2018-08-11 05:34:13,577:CRITICAL:EmoV2_step40: iteration: 44000: Loss: 1.069002, lr: 0.003881
2018-08-11 05:34:18,159:CRITICAL:EmoV2_step40: validate. Iteration: 44000: Accuracy (valence, arousal): 26.549% 17.699%
2018-08-11 05:34:18,160:CRITICAL:EmoV2_step40: validate. Iteration: 44000: Loss: nan
2018-08-11 05:34:31,320:CRITICAL:EmoV2_step40: iteration: 44100: Loss: 0.885615, lr: 0.003865
2018-08-11 05:34:44,474:CRITICAL:EmoV2_step40: iteration: 44200: Loss: 1.307966, lr: 0.003848
2018-08-11 05:34:57,614:CRITICAL:EmoV2_step40: iteration: 44300: Loss: 0.550839, lr: 0.003832
2018-08-11 05:35:10,786:CRITICAL:EmoV2_step40: iteration: 44400: Loss: 0.833472, lr: 0.003816
2018-08-11 05:35:23,947:CRITICAL:EmoV2_step40: iteration: 44500: Loss: 0.455288, lr: 0.003800
2018-08-11 05:35:37,090:CRITICAL:EmoV2_step40: iteration: 44600: Loss: 0.628128, lr: 0.003784
2018-08-11 05:35:50,238:CRITICAL:EmoV2_step40: iteration: 44700: Loss: 0.596253, lr: 0.003767
2018-08-11 05:36:03,392:CRITICAL:EmoV2_step40: iteration: 44800: Loss: 0.963492, lr: 0.003751
2018-08-11 05:36:16,550:CRITICAL:EmoV2_step40: iteration: 44900: Loss: 0.527804, lr: 0.003735
2018-08-11 05:36:29,714:CRITICAL:EmoV2_step40: iteration: 45000: Loss: 0.605927, lr: 0.003719
2018-08-11 05:36:34,480:CRITICAL:EmoV2_step40: validate. Iteration: 45000: Accuracy (valence, arousal): 28.319% 19.027%
2018-08-11 05:36:34,480:CRITICAL:EmoV2_step40: validate. Iteration: 45000: Loss: nan
2018-08-11 05:36:47,669:CRITICAL:EmoV2_step40: iteration: 45100: Loss: 0.717826, lr: 0.003703
2018-08-11 05:37:00,843:CRITICAL:EmoV2_step40: iteration: 45200: Loss: 0.915612, lr: 0.003686
2018-08-11 05:37:13,987:CRITICAL:EmoV2_step40: iteration: 45300: Loss: 0.636648, lr: 0.003670
2018-08-11 05:37:31,509:CRITICAL:EmoV2_step40: iteration: 45400: Loss: 0.368320, lr: 0.003654
2018-08-11 05:37:44,658:CRITICAL:EmoV2_step40: iteration: 45500: Loss: 0.550559, lr: 0.003638
2018-08-11 05:37:57,803:CRITICAL:EmoV2_step40: iteration: 45600: Loss: 0.685433, lr: 0.003622
2018-08-11 05:38:10,945:CRITICAL:EmoV2_step40: iteration: 45700: Loss: 0.575642, lr: 0.003605
2018-08-11 05:38:24,110:CRITICAL:EmoV2_step40: iteration: 45800: Loss: 0.964668, lr: 0.003589
2018-08-11 05:38:37,249:CRITICAL:EmoV2_step40: iteration: 45900: Loss: 0.549403, lr: 0.003573
2018-08-11 05:38:50,416:CRITICAL:EmoV2_step40: iteration: 46000: Loss: 0.646690, lr: 0.003557
2018-08-11 05:38:55,182:CRITICAL:EmoV2_step40: validate. Iteration: 46000: Accuracy (valence, arousal): 30.531% 22.566%
2018-08-11 05:38:55,182:CRITICAL:EmoV2_step40: validate. Iteration: 46000: Loss: nan
2018-08-11 05:39:08,340:CRITICAL:EmoV2_step40: iteration: 46100: Loss: 0.490308, lr: 0.003541
2018-08-11 05:39:21,504:CRITICAL:EmoV2_step40: iteration: 46200: Loss: 0.798786, lr: 0.003524
2018-08-11 05:39:34,666:CRITICAL:EmoV2_step40: iteration: 46300: Loss: 0.406082, lr: 0.003508
2018-08-11 05:39:52,414:CRITICAL:EmoV2_step40: iteration: 46400: Loss: 0.485454, lr: 0.003492
2018-08-11 05:40:05,562:CRITICAL:EmoV2_step40: iteration: 46500: Loss: 0.419744, lr: 0.003476
2018-08-11 05:40:18,699:CRITICAL:EmoV2_step40: iteration: 46600: Loss: 0.593358, lr: 0.003460
2018-08-11 05:40:31,841:CRITICAL:EmoV2_step40: iteration: 46700: Loss: 0.683647, lr: 0.003443
2018-08-11 05:40:45,001:CRITICAL:EmoV2_step40: iteration: 46800: Loss: 0.716859, lr: 0.003427
2018-08-11 05:40:58,157:CRITICAL:EmoV2_step40: iteration: 46900: Loss: 0.647014, lr: 0.003411
2018-08-11 05:41:11,318:CRITICAL:EmoV2_step40: iteration: 47000: Loss: 0.787136, lr: 0.003395
2018-08-11 05:41:16,423:CRITICAL:EmoV2_step40: validate. Iteration: 47000: Accuracy (valence, arousal): 26.549% 21.681%
2018-08-11 05:41:16,424:CRITICAL:EmoV2_step40: validate. Iteration: 47000: Loss: nan
2018-08-11 05:41:33,954:CRITICAL:EmoV2_step40: iteration: 47100: Loss: 0.899938, lr: 0.003379
2018-08-11 05:41:47,107:CRITICAL:EmoV2_step40: iteration: 47200: Loss: 0.625646, lr: 0.003362
2018-08-11 05:42:00,254:CRITICAL:EmoV2_step40: iteration: 47300: Loss: 1.047109, lr: 0.003346
2018-08-11 05:42:13,402:CRITICAL:EmoV2_step40: iteration: 47400: Loss: 0.538393, lr: 0.003330
2018-08-11 05:42:26,558:CRITICAL:EmoV2_step40: iteration: 47500: Loss: 0.679361, lr: 0.003314
2018-08-11 05:42:39,718:CRITICAL:EmoV2_step40: iteration: 47600: Loss: 0.460051, lr: 0.003298
2018-08-11 05:42:52,870:CRITICAL:EmoV2_step40: iteration: 47700: Loss: 0.521512, lr: 0.003281
2018-08-11 05:43:06,025:CRITICAL:EmoV2_step40: iteration: 47800: Loss: 0.393730, lr: 0.003265
2018-08-11 05:43:19,174:CRITICAL:EmoV2_step40: iteration: 47900: Loss: 0.460827, lr: 0.003249
2018-08-11 05:43:32,360:CRITICAL:EmoV2_step40: iteration: 48000: Loss: 0.687374, lr: 0.003233
2018-08-11 05:43:37,283:CRITICAL:EmoV2_step40: validate. Iteration: 48000: Accuracy (valence, arousal): 29.204% 23.894%
2018-08-11 05:43:37,284:CRITICAL:EmoV2_step40: validate. Iteration: 48000: Loss: nan
2018-08-11 05:43:50,429:CRITICAL:EmoV2_step40: iteration: 48100: Loss: 0.385069, lr: 0.003217
2018-08-11 05:44:03,572:CRITICAL:EmoV2_step40: iteration: 48200: Loss: 0.747588, lr: 0.003200
2018-08-11 05:44:16,712:CRITICAL:EmoV2_step40: iteration: 48300: Loss: 0.800208, lr: 0.003184
2018-08-11 05:44:29,867:CRITICAL:EmoV2_step40: iteration: 48400: Loss: 0.998511, lr: 0.003168
2018-08-11 05:44:43,017:CRITICAL:EmoV2_step40: iteration: 48500: Loss: 0.753783, lr: 0.003152
2018-08-11 05:44:56,172:CRITICAL:EmoV2_step40: iteration: 48600: Loss: 0.468150, lr: 0.003136
2018-08-11 05:45:09,322:CRITICAL:EmoV2_step40: iteration: 48700: Loss: 0.474769, lr: 0.003119
2018-08-11 05:45:26,836:CRITICAL:EmoV2_step40: iteration: 48800: Loss: 0.523250, lr: 0.003103
2018-08-11 05:45:39,992:CRITICAL:EmoV2_step40: iteration: 48900: Loss: 0.788525, lr: 0.003087
2018-08-11 05:45:57,738:CRITICAL:EmoV2_step40: iteration: 49000: Loss: 0.540837, lr: 0.003071
2018-08-11 05:46:02,676:CRITICAL:EmoV2_step40: validate. Iteration: 49000: Accuracy (valence, arousal): 34.513% 19.912%
2018-08-11 05:46:02,677:CRITICAL:EmoV2_step40: validate. Iteration: 49000: Loss: nan
2018-08-11 05:46:15,830:CRITICAL:EmoV2_step40: iteration: 49100: Loss: 0.771777, lr: 0.003055
2018-08-11 05:46:28,980:CRITICAL:EmoV2_step40: iteration: 49200: Loss: 0.983983, lr: 0.003038
2018-08-11 05:46:42,129:CRITICAL:EmoV2_step40: iteration: 49300: Loss: 0.461475, lr: 0.003022
2018-08-11 05:46:55,279:CRITICAL:EmoV2_step40: iteration: 49400: Loss: 0.797054, lr: 0.003006
2018-08-11 05:47:08,411:CRITICAL:EmoV2_step40: iteration: 49500: Loss: 0.657169, lr: 0.002990
2018-08-11 05:47:21,563:CRITICAL:EmoV2_step40: iteration: 49600: Loss: 0.589704, lr: 0.002974
2018-08-11 05:47:34,717:CRITICAL:EmoV2_step40: iteration: 49700: Loss: 0.568101, lr: 0.002957
2018-08-11 05:47:47,860:CRITICAL:EmoV2_step40: iteration: 49800: Loss: 0.887092, lr: 0.002941
2018-08-11 05:48:01,016:CRITICAL:EmoV2_step40: iteration: 49900: Loss: 0.811932, lr: 0.002925
2018-08-11 05:48:55,543:CRITICAL:EmoV2_step40: iteration: 50000: Loss: 0.493422, lr: 0.002909
2018-08-11 05:49:00,099:CRITICAL:EmoV2_step40: validate. Iteration: 50000: Accuracy (valence, arousal): 34.071% 20.354%
2018-08-11 05:49:00,099:CRITICAL:EmoV2_step40: validate. Iteration: 50000: Loss: nan
2018-08-11 05:49:13,165:CRITICAL:EmoV2_step40: iteration: 50100: Loss: 0.969000, lr: 0.002893
2018-08-11 05:49:26,239:CRITICAL:EmoV2_step40: iteration: 50200: Loss: 0.699518, lr: 0.002876
2018-08-11 05:49:39,337:CRITICAL:EmoV2_step40: iteration: 50300: Loss: 0.431312, lr: 0.002860
2018-08-11 05:49:56,853:CRITICAL:EmoV2_step40: iteration: 50400: Loss: 0.674188, lr: 0.002844
2018-08-11 05:50:09,958:CRITICAL:EmoV2_step40: iteration: 50500: Loss: 0.962157, lr: 0.002828
2018-08-11 05:50:23,063:CRITICAL:EmoV2_step40: iteration: 50600: Loss: 0.797457, lr: 0.002812
2018-08-11 05:50:36,163:CRITICAL:EmoV2_step40: iteration: 50700: Loss: 0.804740, lr: 0.002795
2018-08-11 05:50:49,261:CRITICAL:EmoV2_step40: iteration: 50800: Loss: 0.454143, lr: 0.002779
2018-08-11 05:51:02,372:CRITICAL:EmoV2_step40: iteration: 50900: Loss: 0.501706, lr: 0.002763
2018-08-11 05:51:15,515:CRITICAL:EmoV2_step40: iteration: 51000: Loss: 0.809346, lr: 0.002747
2018-08-11 05:51:20,098:CRITICAL:EmoV2_step40: validate. Iteration: 51000: Accuracy (valence, arousal): 26.549% 19.912%
2018-08-11 05:51:20,098:CRITICAL:EmoV2_step40: validate. Iteration: 51000: Loss: nan
2018-08-11 05:51:33,248:CRITICAL:EmoV2_step40: iteration: 51100: Loss: 0.634715, lr: 0.002731
2018-08-11 05:51:46,404:CRITICAL:EmoV2_step40: iteration: 51200: Loss: 0.491960, lr: 0.002714
2018-08-11 05:51:59,544:CRITICAL:EmoV2_step40: iteration: 51300: Loss: 0.663479, lr: 0.002698
2018-08-11 05:52:12,699:CRITICAL:EmoV2_step40: iteration: 51400: Loss: 0.422976, lr: 0.002682
2018-08-11 05:52:30,474:CRITICAL:EmoV2_step40: iteration: 51500: Loss: 0.748646, lr: 0.002666
2018-08-11 05:52:43,618:CRITICAL:EmoV2_step40: iteration: 51600: Loss: 0.325694, lr: 0.002650
2018-08-11 05:52:56,780:CRITICAL:EmoV2_step40: iteration: 51700: Loss: 0.722553, lr: 0.002633
2018-08-11 05:53:09,926:CRITICAL:EmoV2_step40: iteration: 51800: Loss: 0.519022, lr: 0.002617
2018-08-11 05:53:23,067:CRITICAL:EmoV2_step40: iteration: 51900: Loss: 0.820553, lr: 0.002601
2018-08-11 05:53:36,212:CRITICAL:EmoV2_step40: iteration: 52000: Loss: 0.602502, lr: 0.002585
2018-08-11 05:53:41,134:CRITICAL:EmoV2_step40: validate. Iteration: 52000: Accuracy (valence, arousal): 27.434% 21.239%
2018-08-11 05:53:41,134:CRITICAL:EmoV2_step40: validate. Iteration: 52000: Loss: nan
2018-08-11 05:53:58,764:CRITICAL:EmoV2_step40: iteration: 52100: Loss: 1.127746, lr: 0.002569
2018-08-11 05:54:11,928:CRITICAL:EmoV2_step40: iteration: 52200: Loss: 0.465046, lr: 0.002552
2018-08-11 05:54:25,073:CRITICAL:EmoV2_step40: iteration: 52300: Loss: 0.484208, lr: 0.002536
2018-08-11 05:54:38,228:CRITICAL:EmoV2_step40: iteration: 52400: Loss: 0.705743, lr: 0.002520
2018-08-11 05:54:51,399:CRITICAL:EmoV2_step40: iteration: 52500: Loss: 0.529793, lr: 0.002504
2018-08-11 05:55:04,554:CRITICAL:EmoV2_step40: iteration: 52600: Loss: 0.459016, lr: 0.002488
2018-08-11 05:55:17,713:CRITICAL:EmoV2_step40: iteration: 52700: Loss: 0.950682, lr: 0.002471
2018-08-11 05:55:30,861:CRITICAL:EmoV2_step40: iteration: 52800: Loss: 0.768579, lr: 0.002455
2018-08-11 05:55:44,010:CRITICAL:EmoV2_step40: iteration: 52900: Loss: 0.381775, lr: 0.002439
2018-08-11 05:55:57,161:CRITICAL:EmoV2_step40: iteration: 53000: Loss: 0.933727, lr: 0.002423
2018-08-11 05:56:01,730:CRITICAL:EmoV2_step40: validate. Iteration: 53000: Accuracy (valence, arousal): 24.336% 20.796%
2018-08-11 05:56:01,730:CRITICAL:EmoV2_step40: validate. Iteration: 53000: Loss: nan
2018-08-11 05:56:14,892:CRITICAL:EmoV2_step40: iteration: 53100: Loss: 0.897875, lr: 0.002407
2018-08-11 05:56:28,056:CRITICAL:EmoV2_step40: iteration: 53200: Loss: 0.409432, lr: 0.002390
2018-08-11 05:56:41,221:CRITICAL:EmoV2_step40: iteration: 53300: Loss: 0.654693, lr: 0.002374
2018-08-11 05:56:54,375:CRITICAL:EmoV2_step40: iteration: 53400: Loss: 0.512124, lr: 0.002358
2018-08-11 05:57:07,541:CRITICAL:EmoV2_step40: iteration: 53500: Loss: 1.024580, lr: 0.002342
2018-08-11 05:57:20,691:CRITICAL:EmoV2_step40: iteration: 53600: Loss: 0.577575, lr: 0.002326
2018-08-11 05:57:38,244:CRITICAL:EmoV2_step40: iteration: 53700: Loss: 0.548165, lr: 0.002309
2018-08-11 05:57:51,404:CRITICAL:EmoV2_step40: iteration: 53800: Loss: 0.753006, lr: 0.002293
2018-08-11 05:58:04,556:CRITICAL:EmoV2_step40: iteration: 53900: Loss: 0.546586, lr: 0.002277
2018-08-11 05:58:22,297:CRITICAL:EmoV2_step40: iteration: 54000: Loss: 0.648528, lr: 0.002261
2018-08-11 05:58:26,919:CRITICAL:EmoV2_step40: validate. Iteration: 54000: Accuracy (valence, arousal): 34.956% 24.336%
2018-08-11 05:58:26,919:CRITICAL:EmoV2_step40: validate. Iteration: 54000: Loss: nan
2018-08-11 05:58:40,064:CRITICAL:EmoV2_step40: iteration: 54100: Loss: 0.812526, lr: 0.002245
2018-08-11 05:58:53,204:CRITICAL:EmoV2_step40: iteration: 54200: Loss: 0.759248, lr: 0.002228
2018-08-11 05:59:06,356:CRITICAL:EmoV2_step40: iteration: 54300: Loss: 0.589615, lr: 0.002212
2018-08-11 05:59:19,494:CRITICAL:EmoV2_step40: iteration: 54400: Loss: 0.562586, lr: 0.002196
2018-08-11 05:59:32,634:CRITICAL:EmoV2_step40: iteration: 54500: Loss: 0.407726, lr: 0.002180
2018-08-11 05:59:45,779:CRITICAL:EmoV2_step40: iteration: 54600: Loss: 0.857930, lr: 0.002164
2018-08-11 05:59:58,924:CRITICAL:EmoV2_step40: iteration: 54700: Loss: 1.003704, lr: 0.002147
2018-08-11 06:00:12,066:CRITICAL:EmoV2_step40: iteration: 54800: Loss: 0.656836, lr: 0.002131
2018-08-11 06:00:25,228:CRITICAL:EmoV2_step40: iteration: 54900: Loss: 0.370209, lr: 0.002115
2018-08-11 06:00:38,373:CRITICAL:EmoV2_step40: iteration: 55000: Loss: 0.530359, lr: 0.002099
2018-08-11 06:00:43,003:CRITICAL:EmoV2_step40: validate. Iteration: 55000: Accuracy (valence, arousal): 31.416% 22.124%
2018-08-11 06:00:43,003:CRITICAL:EmoV2_step40: validate. Iteration: 55000: Loss: nan
2018-08-11 06:00:56,158:CRITICAL:EmoV2_step40: iteration: 55100: Loss: 0.603009, lr: 0.002083
2018-08-11 06:01:09,311:CRITICAL:EmoV2_step40: iteration: 55200: Loss: 0.512316, lr: 0.002066
2018-08-11 06:01:22,460:CRITICAL:EmoV2_step40: iteration: 55300: Loss: 0.499936, lr: 0.002050
2018-08-11 06:01:40,094:CRITICAL:EmoV2_step40: iteration: 55400: Loss: 0.871463, lr: 0.002034
2018-08-11 06:01:53,229:CRITICAL:EmoV2_step40: iteration: 55500: Loss: 0.683297, lr: 0.002018
2018-08-11 06:02:06,379:CRITICAL:EmoV2_step40: iteration: 55600: Loss: 0.511905, lr: 0.002002
2018-08-11 06:02:19,547:CRITICAL:EmoV2_step40: iteration: 55700: Loss: 0.682026, lr: 0.001985
2018-08-11 06:02:32,696:CRITICAL:EmoV2_step40: iteration: 55800: Loss: 0.415621, lr: 0.001969
2018-08-11 06:02:45,855:CRITICAL:EmoV2_step40: iteration: 55900: Loss: 0.891180, lr: 0.001953
2018-08-11 06:02:59,016:CRITICAL:EmoV2_step40: iteration: 56000: Loss: 0.433171, lr: 0.001937
2018-08-11 06:03:03,653:CRITICAL:EmoV2_step40: validate. Iteration: 56000: Accuracy (valence, arousal): 31.416% 23.894%
2018-08-11 06:03:03,653:CRITICAL:EmoV2_step40: validate. Iteration: 56000: Loss: nan
2018-08-11 06:03:16,827:CRITICAL:EmoV2_step40: iteration: 56100: Loss: 0.492865, lr: 0.001921
2018-08-11 06:03:29,977:CRITICAL:EmoV2_step40: iteration: 56200: Loss: 0.477452, lr: 0.001904
2018-08-11 06:03:43,136:CRITICAL:EmoV2_step40: iteration: 56300: Loss: 0.499165, lr: 0.001888
2018-08-11 06:03:56,293:CRITICAL:EmoV2_step40: iteration: 56400: Loss: 0.590020, lr: 0.001872
2018-08-11 06:04:14,091:CRITICAL:EmoV2_step40: iteration: 56500: Loss: 0.519602, lr: 0.001856
2018-08-11 06:04:27,243:CRITICAL:EmoV2_step40: iteration: 56600: Loss: 0.639690, lr: 0.001840
2018-08-11 06:04:40,389:CRITICAL:EmoV2_step40: iteration: 56700: Loss: 0.678823, lr: 0.001823
2018-08-11 06:04:53,527:CRITICAL:EmoV2_step40: iteration: 56800: Loss: 0.644430, lr: 0.001807
2018-08-11 06:05:06,680:CRITICAL:EmoV2_step40: iteration: 56900: Loss: 0.668320, lr: 0.001791
2018-08-11 06:05:24,283:CRITICAL:EmoV2_step40: iteration: 57000: Loss: 0.482238, lr: 0.001775
2018-08-11 06:05:28,860:CRITICAL:EmoV2_step40: validate. Iteration: 57000: Accuracy (valence, arousal): 33.628% 21.681%
2018-08-11 06:05:28,860:CRITICAL:EmoV2_step40: validate. Iteration: 57000: Loss: nan
2018-08-11 06:05:42,009:CRITICAL:EmoV2_step40: iteration: 57100: Loss: 0.682598, lr: 0.001759
2018-08-11 06:05:55,155:CRITICAL:EmoV2_step40: iteration: 57200: Loss: 0.420564, lr: 0.001742
2018-08-11 06:06:08,292:CRITICAL:EmoV2_step40: iteration: 57300: Loss: 0.464427, lr: 0.001726
2018-08-11 06:06:21,436:CRITICAL:EmoV2_step40: iteration: 57400: Loss: 0.379801, lr: 0.001710
2018-08-11 06:06:34,590:CRITICAL:EmoV2_step40: iteration: 57500: Loss: 0.760912, lr: 0.001694
2018-08-11 06:06:47,750:CRITICAL:EmoV2_step40: iteration: 57600: Loss: 0.646612, lr: 0.001678
2018-08-11 06:07:00,888:CRITICAL:EmoV2_step40: iteration: 57700: Loss: 0.355283, lr: 0.001661
2018-08-11 06:07:14,036:CRITICAL:EmoV2_step40: iteration: 57800: Loss: 0.644208, lr: 0.001645
2018-08-11 06:07:27,194:CRITICAL:EmoV2_step40: iteration: 57900: Loss: 0.513084, lr: 0.001629
2018-08-11 06:07:40,338:CRITICAL:EmoV2_step40: iteration: 58000: Loss: 0.552072, lr: 0.001613
2018-08-11 06:07:45,022:CRITICAL:EmoV2_step40: validate. Iteration: 58000: Accuracy (valence, arousal): 30.973% 21.239%
2018-08-11 06:07:45,022:CRITICAL:EmoV2_step40: validate. Iteration: 58000: Loss: nan
2018-08-11 06:07:58,183:CRITICAL:EmoV2_step40: iteration: 58100: Loss: 0.623681, lr: 0.001597
2018-08-11 06:08:11,337:CRITICAL:EmoV2_step40: iteration: 58200: Loss: 0.669663, lr: 0.001580
2018-08-11 06:08:24,479:CRITICAL:EmoV2_step40: iteration: 58300: Loss: 0.755789, lr: 0.001564
2018-08-11 06:08:37,616:CRITICAL:EmoV2_step40: iteration: 58400: Loss: 0.600295, lr: 0.001548
2018-08-11 06:08:50,757:CRITICAL:EmoV2_step40: iteration: 58500: Loss: 0.476452, lr: 0.001532
2018-08-11 06:09:08,281:CRITICAL:EmoV2_step40: iteration: 58600: Loss: 0.634339, lr: 0.001516
2018-08-11 06:09:21,441:CRITICAL:EmoV2_step40: iteration: 58700: Loss: 0.989489, lr: 0.001499
2018-08-11 06:09:34,589:CRITICAL:EmoV2_step40: iteration: 58800: Loss: 0.922522, lr: 0.001483
2018-08-11 06:09:47,733:CRITICAL:EmoV2_step40: iteration: 58900: Loss: 0.814798, lr: 0.001467
2018-08-11 06:10:05,499:CRITICAL:EmoV2_step40: iteration: 59000: Loss: 0.943908, lr: 0.001451
2018-08-11 06:10:10,119:CRITICAL:EmoV2_step40: validate. Iteration: 59000: Accuracy (valence, arousal): 37.168% 18.142%
2018-08-11 06:10:10,120:CRITICAL:EmoV2_step40: validate. Iteration: 59000: Loss: nan
2018-08-11 06:10:23,266:CRITICAL:EmoV2_step40: iteration: 59100: Loss: 0.917997, lr: 0.001435
2018-08-11 06:10:36,403:CRITICAL:EmoV2_step40: iteration: 59200: Loss: 0.530098, lr: 0.001418
2018-08-11 06:10:49,557:CRITICAL:EmoV2_step40: iteration: 59300: Loss: 0.425747, lr: 0.001402
2018-08-11 06:11:02,710:CRITICAL:EmoV2_step40: iteration: 59400: Loss: 0.488733, lr: 0.001386
2018-08-11 06:11:15,850:CRITICAL:EmoV2_step40: iteration: 59500: Loss: 0.515600, lr: 0.001370
2018-08-11 06:11:29,007:CRITICAL:EmoV2_step40: iteration: 59600: Loss: 0.512960, lr: 0.001354
2018-08-11 06:11:42,146:CRITICAL:EmoV2_step40: iteration: 59700: Loss: 0.953996, lr: 0.001337
2018-08-11 06:11:55,301:CRITICAL:EmoV2_step40: iteration: 59800: Loss: 0.559279, lr: 0.001321
2018-08-11 06:12:08,454:CRITICAL:EmoV2_step40: iteration: 59900: Loss: 0.921086, lr: 0.001305
2018-08-11 06:13:02,842:CRITICAL:EmoV2_step40: iteration: 60000: Loss: 0.359134, lr: 0.001289
2018-08-11 06:13:07,472:CRITICAL:EmoV2_step40: validate. Iteration: 60000: Accuracy (valence, arousal): 30.973% 19.027%
2018-08-11 06:13:07,473:CRITICAL:EmoV2_step40: validate. Iteration: 60000: Loss: nan
2018-08-11 06:13:20,539:CRITICAL:EmoV2_step40: iteration: 60100: Loss: 0.465596, lr: 0.001273
2018-08-11 06:13:33,595:CRITICAL:EmoV2_step40: iteration: 60200: Loss: 0.619519, lr: 0.001256
2018-08-11 06:13:51,434:CRITICAL:EmoV2_step40: iteration: 60300: Loss: 0.606310, lr: 0.001240
2018-08-11 06:14:04,527:CRITICAL:EmoV2_step40: iteration: 60400: Loss: 1.125915, lr: 0.001224
2018-08-11 06:14:17,626:CRITICAL:EmoV2_step40: iteration: 60500: Loss: 0.358200, lr: 0.001208
2018-08-11 06:14:30,709:CRITICAL:EmoV2_step40: iteration: 60600: Loss: 0.469271, lr: 0.001192
2018-08-11 06:14:43,799:CRITICAL:EmoV2_step40: iteration: 60700: Loss: 0.606074, lr: 0.001175
2018-08-11 06:14:56,900:CRITICAL:EmoV2_step40: iteration: 60800: Loss: 0.380143, lr: 0.001159
2018-08-11 06:15:10,035:CRITICAL:EmoV2_step40: iteration: 60900: Loss: 0.445538, lr: 0.001143
2018-08-11 06:15:23,186:CRITICAL:EmoV2_step40: iteration: 61000: Loss: 0.524535, lr: 0.001127
2018-08-11 06:15:27,728:CRITICAL:EmoV2_step40: validate. Iteration: 61000: Accuracy (valence, arousal): 33.628% 22.124%
2018-08-11 06:15:27,728:CRITICAL:EmoV2_step40: validate. Iteration: 61000: Loss: nan
2018-08-11 06:15:40,880:CRITICAL:EmoV2_step40: iteration: 61100: Loss: 0.770544, lr: 0.001111
2018-08-11 06:15:54,023:CRITICAL:EmoV2_step40: iteration: 61200: Loss: 0.619655, lr: 0.001094
2018-08-11 06:16:07,183:CRITICAL:EmoV2_step40: iteration: 61300: Loss: 0.435561, lr: 0.001078
2018-08-11 06:16:20,348:CRITICAL:EmoV2_step40: iteration: 61400: Loss: 0.441958, lr: 0.001062
2018-08-11 06:16:38,137:CRITICAL:EmoV2_step40: iteration: 61500: Loss: 0.333137, lr: 0.001046
2018-08-11 06:16:51,293:CRITICAL:EmoV2_step40: iteration: 61600: Loss: 0.957766, lr: 0.001030
2018-08-11 06:17:04,466:CRITICAL:EmoV2_step40: iteration: 61700: Loss: 0.672701, lr: 0.001013
2018-08-11 06:17:17,616:CRITICAL:EmoV2_step40: iteration: 61800: Loss: 0.541963, lr: 0.000997
2018-08-11 06:17:30,783:CRITICAL:EmoV2_step40: iteration: 61900: Loss: 0.743235, lr: 0.000981
2018-08-11 06:17:48,392:CRITICAL:EmoV2_step40: iteration: 62000: Loss: 0.741452, lr: 0.000965
2018-08-11 06:17:53,127:CRITICAL:EmoV2_step40: validate. Iteration: 62000: Accuracy (valence, arousal): 34.956% 17.699%
2018-08-11 06:17:53,127:CRITICAL:EmoV2_step40: validate. Iteration: 62000: Loss: nan
2018-08-11 06:18:06,281:CRITICAL:EmoV2_step40: iteration: 62100: Loss: 0.351877, lr: 0.000949
2018-08-11 06:18:19,452:CRITICAL:EmoV2_step40: iteration: 62200: Loss: 0.630539, lr: 0.000932
2018-08-11 06:18:32,608:CRITICAL:EmoV2_step40: iteration: 62300: Loss: 0.705095, lr: 0.000916
2018-08-11 06:18:45,754:CRITICAL:EmoV2_step40: iteration: 62400: Loss: 1.092122, lr: 0.000900
2018-08-11 06:18:58,916:CRITICAL:EmoV2_step40: iteration: 62500: Loss: 0.679277, lr: 0.000900
2018-08-11 06:19:12,066:CRITICAL:EmoV2_step40: iteration: 62600: Loss: 0.499822, lr: 0.000921
2018-08-11 06:19:25,212:CRITICAL:EmoV2_step40: iteration: 62700: Loss: 0.611652, lr: 0.000942
2018-08-11 06:19:38,351:CRITICAL:EmoV2_step40: iteration: 62800: Loss: 0.775140, lr: 0.000963
2018-08-11 06:19:51,493:CRITICAL:EmoV2_step40: iteration: 62900: Loss: 0.781050, lr: 0.000984
2018-08-11 06:20:04,655:CRITICAL:EmoV2_step40: iteration: 63000: Loss: 0.387414, lr: 0.001005
2018-08-11 06:20:09,312:CRITICAL:EmoV2_step40: validate. Iteration: 63000: Accuracy (valence, arousal): 34.956% 18.584%
2018-08-11 06:20:09,312:CRITICAL:EmoV2_step40: validate. Iteration: 63000: Loss: nan
2018-08-11 06:20:22,459:CRITICAL:EmoV2_step40: iteration: 63100: Loss: 0.549607, lr: 0.001026
2018-08-11 06:20:35,614:CRITICAL:EmoV2_step40: iteration: 63200: Loss: 0.581708, lr: 0.001047
2018-08-11 06:20:48,760:CRITICAL:EmoV2_step40: iteration: 63300: Loss: 0.599436, lr: 0.001068
2018-08-11 06:21:01,916:CRITICAL:EmoV2_step40: iteration: 63400: Loss: 0.460526, lr: 0.001089
2018-08-11 06:21:15,056:CRITICAL:EmoV2_step40: iteration: 63500: Loss: 0.401376, lr: 0.001110
2018-08-11 06:21:32,600:CRITICAL:EmoV2_step40: iteration: 63600: Loss: 0.270001, lr: 0.001131
2018-08-11 06:21:45,746:CRITICAL:EmoV2_step40: iteration: 63700: Loss: 0.969721, lr: 0.001152
2018-08-11 06:21:58,898:CRITICAL:EmoV2_step40: iteration: 63800: Loss: 0.508319, lr: 0.001173
2018-08-11 06:22:12,042:CRITICAL:EmoV2_step40: iteration: 63900: Loss: 0.322364, lr: 0.001194
2018-08-11 06:22:29,845:CRITICAL:EmoV2_step40: iteration: 64000: Loss: 0.290564, lr: 0.001215
2018-08-11 06:22:34,507:CRITICAL:EmoV2_step40: validate. Iteration: 64000: Accuracy (valence, arousal): 37.611% 18.584%
2018-08-11 06:22:34,507:CRITICAL:EmoV2_step40: validate. Iteration: 64000: Loss: nan
2018-08-11 06:22:47,660:CRITICAL:EmoV2_step40: iteration: 64100: Loss: 0.450987, lr: 0.001236
2018-08-11 06:23:00,807:CRITICAL:EmoV2_step40: iteration: 64200: Loss: 0.784621, lr: 0.001257
2018-08-11 06:23:13,972:CRITICAL:EmoV2_step40: iteration: 64300: Loss: 0.490849, lr: 0.001278
2018-08-11 06:23:27,123:CRITICAL:EmoV2_step40: iteration: 64400: Loss: 0.491205, lr: 0.001299
2018-08-11 06:23:40,291:CRITICAL:EmoV2_step40: iteration: 64500: Loss: 0.415761, lr: 0.001320
2018-08-11 06:23:53,434:CRITICAL:EmoV2_step40: iteration: 64600: Loss: 0.528619, lr: 0.001341
2018-08-11 06:24:06,576:CRITICAL:EmoV2_step40: iteration: 64700: Loss: 1.077643, lr: 0.001362
2018-08-11 06:24:19,724:CRITICAL:EmoV2_step40: iteration: 64800: Loss: 0.896792, lr: 0.001383
2018-08-11 06:24:32,859:CRITICAL:EmoV2_step40: iteration: 64900: Loss: 0.576315, lr: 0.001404
2018-08-11 06:24:46,010:CRITICAL:EmoV2_step40: iteration: 65000: Loss: 0.649997, lr: 0.001425
2018-08-11 06:24:50,761:CRITICAL:EmoV2_step40: validate. Iteration: 65000: Accuracy (valence, arousal): 36.726% 19.469%
2018-08-11 06:24:50,762:CRITICAL:EmoV2_step40: validate. Iteration: 65000: Loss: nan
2018-08-11 06:25:03,933:CRITICAL:EmoV2_step40: iteration: 65100: Loss: 0.800840, lr: 0.001446
2018-08-11 06:25:21,496:CRITICAL:EmoV2_step40: iteration: 65200: Loss: 0.548076, lr: 0.001467
2018-08-11 06:25:34,643:CRITICAL:EmoV2_step40: iteration: 65300: Loss: 0.431579, lr: 0.001488
2018-08-11 06:25:47,804:CRITICAL:EmoV2_step40: iteration: 65400: Loss: 0.616914, lr: 0.001509
2018-08-11 06:26:00,955:CRITICAL:EmoV2_step40: iteration: 65500: Loss: 1.177795, lr: 0.001530
2018-08-11 06:26:14,095:CRITICAL:EmoV2_step40: iteration: 65600: Loss: 0.537116, lr: 0.001551
2018-08-11 06:26:27,246:CRITICAL:EmoV2_step40: iteration: 65700: Loss: 0.462870, lr: 0.001572
2018-08-11 06:26:40,393:CRITICAL:EmoV2_step40: iteration: 65800: Loss: 0.451011, lr: 0.001593
2018-08-11 06:26:53,539:CRITICAL:EmoV2_step40: iteration: 65900: Loss: 0.460375, lr: 0.001614
2018-08-11 06:27:06,689:CRITICAL:EmoV2_step40: iteration: 66000: Loss: 0.342404, lr: 0.001635
2018-08-11 06:27:11,403:CRITICAL:EmoV2_step40: validate. Iteration: 66000: Accuracy (valence, arousal): 33.186% 23.009%
2018-08-11 06:27:11,404:CRITICAL:EmoV2_step40: validate. Iteration: 66000: Loss: nan
2018-08-11 06:27:24,552:CRITICAL:EmoV2_step40: iteration: 66100: Loss: 0.532610, lr: 0.001656
2018-08-11 06:27:37,697:CRITICAL:EmoV2_step40: iteration: 66200: Loss: 0.850054, lr: 0.001677
2018-08-11 06:27:50,833:CRITICAL:EmoV2_step40: iteration: 66300: Loss: 0.522255, lr: 0.001698
2018-08-11 06:28:03,978:CRITICAL:EmoV2_step40: iteration: 66400: Loss: 0.783162, lr: 0.001719
2018-08-11 06:28:21,755:CRITICAL:EmoV2_step40: iteration: 66500: Loss: 0.571436, lr: 0.001740
2018-08-11 06:28:34,897:CRITICAL:EmoV2_step40: iteration: 66600: Loss: 0.394749, lr: 0.001761
2018-08-11 06:28:48,032:CRITICAL:EmoV2_step40: iteration: 66700: Loss: 0.817127, lr: 0.001782
2018-08-11 06:29:01,173:CRITICAL:EmoV2_step40: iteration: 66800: Loss: 0.604229, lr: 0.001803
2018-08-11 06:29:18,773:CRITICAL:EmoV2_step40: iteration: 66900: Loss: 0.719764, lr: 0.001824
2018-08-11 06:29:31,907:CRITICAL:EmoV2_step40: iteration: 67000: Loss: 0.933030, lr: 0.001845
2018-08-11 06:29:36,495:CRITICAL:EmoV2_step40: validate. Iteration: 67000: Accuracy (valence, arousal): 34.071% 19.027%
2018-08-11 06:29:36,495:CRITICAL:EmoV2_step40: validate. Iteration: 67000: Loss: nan
2018-08-11 06:29:49,651:CRITICAL:EmoV2_step40: iteration: 67100: Loss: 0.490984, lr: 0.001866
2018-08-11 06:30:02,793:CRITICAL:EmoV2_step40: iteration: 67200: Loss: 0.331644, lr: 0.001887
2018-08-11 06:30:15,931:CRITICAL:EmoV2_step40: iteration: 67300: Loss: 0.854984, lr: 0.001908
2018-08-11 06:30:29,070:CRITICAL:EmoV2_step40: iteration: 67400: Loss: 0.257955, lr: 0.001929
2018-08-11 06:30:42,208:CRITICAL:EmoV2_step40: iteration: 67500: Loss: 0.340010, lr: 0.001950
2018-08-11 06:30:55,363:CRITICAL:EmoV2_step40: iteration: 67600: Loss: 0.428219, lr: 0.001971
2018-08-11 06:31:08,529:CRITICAL:EmoV2_step40: iteration: 67700: Loss: 0.602797, lr: 0.001992
2018-08-11 06:31:21,685:CRITICAL:EmoV2_step40: iteration: 67800: Loss: 0.493947, lr: 0.002013
2018-08-11 06:31:34,846:CRITICAL:EmoV2_step40: iteration: 67900: Loss: 1.339507, lr: 0.002034
2018-08-11 06:31:48,004:CRITICAL:EmoV2_step40: iteration: 68000: Loss: 1.013619, lr: 0.002055
2018-08-11 06:31:52,728:CRITICAL:EmoV2_step40: validate. Iteration: 68000: Accuracy (valence, arousal): 33.628% 20.796%
2018-08-11 06:31:52,729:CRITICAL:EmoV2_step40: validate. Iteration: 68000: Loss: nan
2018-08-11 06:32:05,879:CRITICAL:EmoV2_step40: iteration: 68100: Loss: 0.604765, lr: 0.002076
2018-08-11 06:32:19,022:CRITICAL:EmoV2_step40: iteration: 68200: Loss: 0.446686, lr: 0.002097
2018-08-11 06:32:32,160:CRITICAL:EmoV2_step40: iteration: 68300: Loss: 0.550592, lr: 0.002118
2018-08-11 06:32:45,314:CRITICAL:EmoV2_step40: iteration: 68400: Loss: 0.586395, lr: 0.002139
2018-08-11 06:33:02,860:CRITICAL:EmoV2_step40: iteration: 68500: Loss: 0.745843, lr: 0.002160
2018-08-11 06:33:15,999:CRITICAL:EmoV2_step40: iteration: 68600: Loss: 1.130120, lr: 0.002181
2018-08-11 06:33:29,153:CRITICAL:EmoV2_step40: iteration: 68700: Loss: 0.370946, lr: 0.002202
2018-08-11 06:33:42,302:CRITICAL:EmoV2_step40: iteration: 68800: Loss: 0.464626, lr: 0.002223
2018-08-11 06:33:55,468:CRITICAL:EmoV2_step40: iteration: 68900: Loss: 0.553600, lr: 0.002244
2018-08-11 06:34:13,224:CRITICAL:EmoV2_step40: iteration: 69000: Loss: 0.491323, lr: 0.002265
2018-08-11 06:34:17,889:CRITICAL:EmoV2_step40: validate. Iteration: 69000: Accuracy (valence, arousal): 30.531% 18.584%
2018-08-11 06:34:17,891:CRITICAL:EmoV2_step40: validate. Iteration: 69000: Loss: nan
2018-08-11 06:34:31,037:CRITICAL:EmoV2_step40: iteration: 69100: Loss: 0.761371, lr: 0.002286
2018-08-11 06:34:44,190:CRITICAL:EmoV2_step40: iteration: 69200: Loss: 0.567008, lr: 0.002307
2018-08-11 06:34:57,336:CRITICAL:EmoV2_step40: iteration: 69300: Loss: 0.563119, lr: 0.002328
2018-08-11 06:35:10,493:CRITICAL:EmoV2_step40: iteration: 69400: Loss: 0.472693, lr: 0.002349
2018-08-11 06:35:23,649:CRITICAL:EmoV2_step40: iteration: 69500: Loss: 0.655673, lr: 0.002370
2018-08-11 06:35:36,804:CRITICAL:EmoV2_step40: iteration: 69600: Loss: 0.822887, lr: 0.002391
2018-08-11 06:35:49,963:CRITICAL:EmoV2_step40: iteration: 69700: Loss: 0.812839, lr: 0.002412
2018-08-11 06:36:03,100:CRITICAL:EmoV2_step40: iteration: 69800: Loss: 0.523487, lr: 0.002433
2018-08-11 06:36:16,264:CRITICAL:EmoV2_step40: iteration: 69900: Loss: 0.332054, lr: 0.002454
2018-08-11 06:37:10,845:CRITICAL:EmoV2_step40: iteration: 70000: Loss: 0.764811, lr: 0.002475
2018-08-11 06:37:15,547:CRITICAL:EmoV2_step40: validate. Iteration: 70000: Accuracy (valence, arousal): 31.416% 22.566%
2018-08-11 06:37:15,547:CRITICAL:EmoV2_step40: validate. Iteration: 70000: Loss: nan
2018-08-11 06:37:28,609:CRITICAL:EmoV2_step40: iteration: 70100: Loss: 0.650656, lr: 0.002496
2018-08-11 06:37:46,036:CRITICAL:EmoV2_step40: iteration: 70200: Loss: 0.511905, lr: 0.002517
2018-08-11 06:37:59,093:CRITICAL:EmoV2_step40: iteration: 70300: Loss: 0.699086, lr: 0.002538
2018-08-11 06:38:12,172:CRITICAL:EmoV2_step40: iteration: 70400: Loss: 0.664052, lr: 0.002559
2018-08-11 06:38:25,272:CRITICAL:EmoV2_step40: iteration: 70500: Loss: 0.485600, lr: 0.002580
2018-08-11 06:38:38,355:CRITICAL:EmoV2_step40: iteration: 70600: Loss: 0.690861, lr: 0.002601
2018-08-11 06:38:51,443:CRITICAL:EmoV2_step40: iteration: 70700: Loss: 0.700880, lr: 0.002622
2018-08-11 06:39:04,534:CRITICAL:EmoV2_step40: iteration: 70800: Loss: 0.516907, lr: 0.002643
2018-08-11 06:39:17,684:CRITICAL:EmoV2_step40: iteration: 70900: Loss: 0.803501, lr: 0.002664
2018-08-11 06:39:30,833:CRITICAL:EmoV2_step40: iteration: 71000: Loss: 0.479643, lr: 0.002685
2018-08-11 06:39:35,489:CRITICAL:EmoV2_step40: validate. Iteration: 71000: Accuracy (valence, arousal): 32.301% 20.796%
2018-08-11 06:39:35,490:CRITICAL:EmoV2_step40: validate. Iteration: 71000: Loss: nan
2018-08-11 06:39:48,629:CRITICAL:EmoV2_step40: iteration: 71100: Loss: 0.426258, lr: 0.002706
2018-08-11 06:40:01,784:CRITICAL:EmoV2_step40: iteration: 71200: Loss: 0.708690, lr: 0.002727
2018-08-11 06:40:14,933:CRITICAL:EmoV2_step40: iteration: 71300: Loss: 0.547062, lr: 0.002748
2018-08-11 06:40:28,098:CRITICAL:EmoV2_step40: iteration: 71400: Loss: 0.671848, lr: 0.002769
2018-08-11 06:40:45,886:CRITICAL:EmoV2_step40: iteration: 71500: Loss: 0.453374, lr: 0.002790
2018-08-11 06:40:59,043:CRITICAL:EmoV2_step40: iteration: 71600: Loss: 0.710459, lr: 0.002811
2018-08-11 06:41:12,192:CRITICAL:EmoV2_step40: iteration: 71700: Loss: 0.354985, lr: 0.002832
2018-08-11 06:41:29,705:CRITICAL:EmoV2_step40: iteration: 71800: Loss: 0.778645, lr: 0.002853
2018-08-11 06:41:42,850:CRITICAL:EmoV2_step40: iteration: 71900: Loss: 0.450053, lr: 0.002874
2018-08-11 06:41:55,993:CRITICAL:EmoV2_step40: iteration: 72000: Loss: 0.807447, lr: 0.002895
2018-08-11 06:42:00,459:CRITICAL:EmoV2_step40: validate. Iteration: 72000: Accuracy (valence, arousal): 34.071% 23.009%
2018-08-11 06:42:00,459:CRITICAL:EmoV2_step40: validate. Iteration: 72000: Loss: nan
2018-08-11 06:42:13,616:CRITICAL:EmoV2_step40: iteration: 72100: Loss: 0.756792, lr: 0.002916
2018-08-11 06:42:26,764:CRITICAL:EmoV2_step40: iteration: 72200: Loss: 0.439625, lr: 0.002937
2018-08-11 06:42:39,906:CRITICAL:EmoV2_step40: iteration: 72300: Loss: 0.374191, lr: 0.002958
2018-08-11 06:42:53,053:CRITICAL:EmoV2_step40: iteration: 72400: Loss: 0.440915, lr: 0.002979
2018-08-11 06:43:06,192:CRITICAL:EmoV2_step40: iteration: 72500: Loss: 0.686290, lr: 0.002974
2018-08-11 06:43:19,357:CRITICAL:EmoV2_step40: iteration: 72600: Loss: 0.396983, lr: 0.002968
2018-08-11 06:43:32,516:CRITICAL:EmoV2_step40: iteration: 72700: Loss: 0.827199, lr: 0.002963
2018-08-11 06:43:45,672:CRITICAL:EmoV2_step40: iteration: 72800: Loss: 0.933082, lr: 0.002958
2018-08-11 06:43:58,838:CRITICAL:EmoV2_step40: iteration: 72900: Loss: 0.390900, lr: 0.002953
2018-08-11 06:44:11,991:CRITICAL:EmoV2_step40: iteration: 73000: Loss: 0.437668, lr: 0.002947
2018-08-11 06:44:16,481:CRITICAL:EmoV2_step40: validate. Iteration: 73000: Accuracy (valence, arousal): 33.628% 22.566%
2018-08-11 06:44:16,481:CRITICAL:EmoV2_step40: validate. Iteration: 73000: Loss: nan
2018-08-11 06:44:29,640:CRITICAL:EmoV2_step40: iteration: 73100: Loss: 0.444386, lr: 0.002942
2018-08-11 06:44:42,798:CRITICAL:EmoV2_step40: iteration: 73200: Loss: 0.643169, lr: 0.002937
2018-08-11 06:44:55,952:CRITICAL:EmoV2_step40: iteration: 73300: Loss: 0.340365, lr: 0.002932
2018-08-11 06:45:09,126:CRITICAL:EmoV2_step40: iteration: 73400: Loss: 0.945210, lr: 0.002926
2018-08-11 06:45:26,741:CRITICAL:EmoV2_step40: iteration: 73500: Loss: 0.331563, lr: 0.002921
2018-08-11 06:45:39,885:CRITICAL:EmoV2_step40: iteration: 73600: Loss: 0.892753, lr: 0.002916
2018-08-11 06:45:53,031:CRITICAL:EmoV2_step40: iteration: 73700: Loss: 0.738637, lr: 0.002911
2018-08-11 06:46:06,178:CRITICAL:EmoV2_step40: iteration: 73800: Loss: 0.504561, lr: 0.002905
2018-08-11 06:46:19,341:CRITICAL:EmoV2_step40: iteration: 73900: Loss: 0.472921, lr: 0.002900
2018-08-11 06:46:32,491:CRITICAL:EmoV2_step40: iteration: 74000: Loss: 0.418964, lr: 0.002895
2018-08-11 06:46:41,514:CRITICAL:EmoV2_step40: validate. Iteration: 74000: Accuracy (valence, arousal): 31.416% 22.566%
2018-08-11 06:46:41,514:CRITICAL:EmoV2_step40: validate. Iteration: 74000: Loss: nan
2018-08-11 06:46:54,660:CRITICAL:EmoV2_step40: iteration: 74100: Loss: 0.513124, lr: 0.002890
2018-08-11 06:47:07,796:CRITICAL:EmoV2_step40: iteration: 74200: Loss: 1.411047, lr: 0.002884
2018-08-11 06:47:20,944:CRITICAL:EmoV2_step40: iteration: 74300: Loss: 0.585905, lr: 0.002879
2018-08-11 06:47:34,092:CRITICAL:EmoV2_step40: iteration: 74400: Loss: 0.624868, lr: 0.002874
2018-08-11 06:47:47,239:CRITICAL:EmoV2_step40: iteration: 74500: Loss: 0.883029, lr: 0.002869
2018-08-11 06:48:00,400:CRITICAL:EmoV2_step40: iteration: 74600: Loss: 0.713181, lr: 0.002863
2018-08-11 06:48:13,544:CRITICAL:EmoV2_step40: iteration: 74700: Loss: 0.384301, lr: 0.002858
2018-08-11 06:48:26,686:CRITICAL:EmoV2_step40: iteration: 74800: Loss: 0.903008, lr: 0.002853
2018-08-11 06:48:39,834:CRITICAL:EmoV2_step40: iteration: 74900: Loss: 0.961281, lr: 0.002848
2018-08-11 06:48:52,985:CRITICAL:EmoV2_step40: iteration: 75000: Loss: 0.309323, lr: 0.002842
2018-08-11 06:48:57,812:CRITICAL:EmoV2_step40: validate. Iteration: 75000: Accuracy (valence, arousal): 33.628% 19.912%
2018-08-11 06:48:57,813:CRITICAL:EmoV2_step40: validate. Iteration: 75000: Loss: nan
2018-08-11 06:49:15,336:CRITICAL:EmoV2_step40: iteration: 75100: Loss: 0.452005, lr: 0.002837
2018-08-11 06:49:28,468:CRITICAL:EmoV2_step40: iteration: 75200: Loss: 0.463087, lr: 0.002832
2018-08-11 06:49:41,627:CRITICAL:EmoV2_step40: iteration: 75300: Loss: 0.281528, lr: 0.002827
2018-08-11 06:49:54,788:CRITICAL:EmoV2_step40: iteration: 75400: Loss: 0.706210, lr: 0.002821
2018-08-11 06:50:07,929:CRITICAL:EmoV2_step40: iteration: 75500: Loss: 0.949652, lr: 0.002816
2018-08-11 06:50:21,070:CRITICAL:EmoV2_step40: iteration: 75600: Loss: 0.613207, lr: 0.002811
2018-08-11 06:50:34,225:CRITICAL:EmoV2_step40: iteration: 75700: Loss: 0.415347, lr: 0.002806
2018-08-11 06:50:47,371:CRITICAL:EmoV2_step40: iteration: 75800: Loss: 0.373601, lr: 0.002800
2018-08-11 06:51:00,525:CRITICAL:EmoV2_step40: iteration: 75900: Loss: 0.454323, lr: 0.002795
2018-08-11 06:51:13,697:CRITICAL:EmoV2_step40: iteration: 76000: Loss: 0.742753, lr: 0.002790
2018-08-11 06:51:18,575:CRITICAL:EmoV2_step40: validate. Iteration: 76000: Accuracy (valence, arousal): 33.628% 22.124%
2018-08-11 06:51:18,575:CRITICAL:EmoV2_step40: validate. Iteration: 76000: Loss: nan
2018-08-11 06:51:31,734:CRITICAL:EmoV2_step40: iteration: 76100: Loss: 0.385460, lr: 0.002785
2018-08-11 06:51:44,876:CRITICAL:EmoV2_step40: iteration: 76200: Loss: 0.551018, lr: 0.002779
2018-08-11 06:51:58,018:CRITICAL:EmoV2_step40: iteration: 76300: Loss: 0.507625, lr: 0.002774
2018-08-11 06:52:11,154:CRITICAL:EmoV2_step40: iteration: 76400: Loss: 0.368631, lr: 0.002769
2018-08-11 06:52:28,928:CRITICAL:EmoV2_step40: iteration: 76500: Loss: 0.517934, lr: 0.002764
2018-08-11 06:52:42,084:CRITICAL:EmoV2_step40: iteration: 76600: Loss: 0.984956, lr: 0.002758
2018-08-11 06:52:55,239:CRITICAL:EmoV2_step40: iteration: 76700: Loss: 0.860657, lr: 0.002753
2018-08-11 06:53:12,775:CRITICAL:EmoV2_step40: iteration: 76800: Loss: 0.395698, lr: 0.002748
2018-08-11 06:53:25,936:CRITICAL:EmoV2_step40: iteration: 76900: Loss: 0.837470, lr: 0.002743
2018-08-11 06:53:39,086:CRITICAL:EmoV2_step40: iteration: 77000: Loss: 0.741502, lr: 0.002737
2018-08-11 06:53:43,560:CRITICAL:EmoV2_step40: validate. Iteration: 77000: Accuracy (valence, arousal): 30.088% 23.009%
2018-08-11 06:53:43,560:CRITICAL:EmoV2_step40: validate. Iteration: 77000: Loss: nan
2018-08-11 06:53:56,703:CRITICAL:EmoV2_step40: iteration: 77100: Loss: 0.669490, lr: 0.002732
2018-08-11 06:54:09,853:CRITICAL:EmoV2_step40: iteration: 77200: Loss: 0.413878, lr: 0.002727
2018-08-11 06:54:22,992:CRITICAL:EmoV2_step40: iteration: 77300: Loss: 0.678586, lr: 0.002722
2018-08-11 06:54:36,145:CRITICAL:EmoV2_step40: iteration: 77400: Loss: 0.583867, lr: 0.002716
2018-08-11 06:54:49,289:CRITICAL:EmoV2_step40: iteration: 77500: Loss: 0.851385, lr: 0.002711
2018-08-11 06:55:02,436:CRITICAL:EmoV2_step40: iteration: 77600: Loss: 0.594398, lr: 0.002706
2018-08-11 06:55:15,597:CRITICAL:EmoV2_step40: iteration: 77700: Loss: 0.714988, lr: 0.002701
2018-08-11 06:55:28,739:CRITICAL:EmoV2_step40: iteration: 77800: Loss: 0.527054, lr: 0.002695
2018-08-11 06:55:41,884:CRITICAL:EmoV2_step40: iteration: 77900: Loss: 1.458203, lr: 0.002690
2018-08-11 06:55:55,034:CRITICAL:EmoV2_step40: iteration: 78000: Loss: 0.567958, lr: 0.002685
2018-08-11 06:56:00,042:CRITICAL:EmoV2_step40: validate. Iteration: 78000: Accuracy (valence, arousal): 37.611% 21.681%
2018-08-11 06:56:00,042:CRITICAL:EmoV2_step40: validate. Iteration: 78000: Loss: nan
2018-08-11 06:56:13,200:CRITICAL:EmoV2_step40: iteration: 78100: Loss: 0.740807, lr: 0.002680
2018-08-11 06:56:26,361:CRITICAL:EmoV2_step40: iteration: 78200: Loss: 0.594714, lr: 0.002674
2018-08-11 06:56:39,526:CRITICAL:EmoV2_step40: iteration: 78300: Loss: 0.663779, lr: 0.002669
2018-08-11 06:56:57,084:CRITICAL:EmoV2_step40: iteration: 78400: Loss: 0.468368, lr: 0.002664
2018-08-11 06:57:10,234:CRITICAL:EmoV2_step40: iteration: 78500: Loss: 0.680297, lr: 0.002659
2018-08-11 06:57:23,382:CRITICAL:EmoV2_step40: iteration: 78600: Loss: 0.917360, lr: 0.002653
2018-08-11 06:57:36,521:CRITICAL:EmoV2_step40: iteration: 78700: Loss: 0.403265, lr: 0.002648
2018-08-11 06:57:49,679:CRITICAL:EmoV2_step40: iteration: 78800: Loss: 0.644171, lr: 0.002643
2018-08-11 06:58:02,842:CRITICAL:EmoV2_step40: iteration: 78900: Loss: 0.643590, lr: 0.002638
2018-08-11 06:58:15,985:CRITICAL:EmoV2_step40: iteration: 79000: Loss: 0.353095, lr: 0.002632
2018-08-11 06:58:25,033:CRITICAL:EmoV2_step40: validate. Iteration: 79000: Accuracy (valence, arousal): 32.301% 20.354%
2018-08-11 06:58:25,033:CRITICAL:EmoV2_step40: validate. Iteration: 79000: Loss: nan
2018-08-11 06:58:38,189:CRITICAL:EmoV2_step40: iteration: 79100: Loss: 0.962145, lr: 0.002627
2018-08-11 06:58:51,342:CRITICAL:EmoV2_step40: iteration: 79200: Loss: 0.445392, lr: 0.002622
2018-08-11 06:59:04,491:CRITICAL:EmoV2_step40: iteration: 79300: Loss: 0.476215, lr: 0.002617
2018-08-11 06:59:17,647:CRITICAL:EmoV2_step40: iteration: 79400: Loss: 0.423044, lr: 0.002611
2018-08-11 06:59:30,783:CRITICAL:EmoV2_step40: iteration: 79500: Loss: 0.356569, lr: 0.002606
2018-08-11 06:59:43,931:CRITICAL:EmoV2_step40: iteration: 79600: Loss: 0.622597, lr: 0.002601
2018-08-11 06:59:57,070:CRITICAL:EmoV2_step40: iteration: 79700: Loss: 0.530634, lr: 0.002596
2018-08-11 07:00:10,218:CRITICAL:EmoV2_step40: iteration: 79800: Loss: 1.267362, lr: 0.002590
2018-08-11 07:00:23,377:CRITICAL:EmoV2_step40: iteration: 79900: Loss: 0.545116, lr: 0.002585
2018-08-11 07:01:17,794:CRITICAL:EmoV2_step40: iteration: 80000: Loss: 0.680018, lr: 0.002580
2018-08-11 07:01:22,394:CRITICAL:EmoV2_step40: validate. Iteration: 80000: Accuracy (valence, arousal): 27.876% 20.796%
2018-08-11 07:01:22,394:CRITICAL:EmoV2_step40: validate. Iteration: 80000: Loss: nan
2018-08-11 07:01:39,979:CRITICAL:EmoV2_step40: iteration: 80100: Loss: 0.479226, lr: 0.002575
2018-08-11 07:01:53,047:CRITICAL:EmoV2_step40: iteration: 80200: Loss: 0.525821, lr: 0.002569
2018-08-11 07:02:06,104:CRITICAL:EmoV2_step40: iteration: 80300: Loss: 1.059878, lr: 0.002564
2018-08-11 07:02:19,181:CRITICAL:EmoV2_step40: iteration: 80400: Loss: 0.774775, lr: 0.002559
2018-08-11 07:02:32,289:CRITICAL:EmoV2_step40: iteration: 80500: Loss: 0.449949, lr: 0.002554
2018-08-11 07:02:45,398:CRITICAL:EmoV2_step40: iteration: 80600: Loss: 0.326530, lr: 0.002548
2018-08-11 07:02:58,495:CRITICAL:EmoV2_step40: iteration: 80700: Loss: 0.655833, lr: 0.002543
2018-08-11 07:03:11,600:CRITICAL:EmoV2_step40: iteration: 80800: Loss: 1.064213, lr: 0.002538
2018-08-11 07:03:24,732:CRITICAL:EmoV2_step40: iteration: 80900: Loss: 0.859176, lr: 0.002533
2018-08-11 07:03:37,887:CRITICAL:EmoV2_step40: iteration: 81000: Loss: 0.572948, lr: 0.002527
2018-08-11 07:03:42,852:CRITICAL:EmoV2_step40: validate. Iteration: 81000: Accuracy (valence, arousal): 28.761% 23.009%
2018-08-11 07:03:42,853:CRITICAL:EmoV2_step40: validate. Iteration: 81000: Loss: nan
2018-08-11 07:03:55,995:CRITICAL:EmoV2_step40: iteration: 81100: Loss: 0.457462, lr: 0.002522
2018-08-11 07:04:09,131:CRITICAL:EmoV2_step40: iteration: 81200: Loss: 0.300907, lr: 0.002517
2018-08-11 07:04:22,272:CRITICAL:EmoV2_step40: iteration: 81300: Loss: 0.692099, lr: 0.002512
2018-08-11 07:04:35,425:CRITICAL:EmoV2_step40: iteration: 81400: Loss: 0.617566, lr: 0.002506
2018-08-11 07:04:48,576:CRITICAL:EmoV2_step40: iteration: 81500: Loss: 0.611538, lr: 0.002501
2018-08-11 07:05:06,354:CRITICAL:EmoV2_step40: iteration: 81600: Loss: 0.755655, lr: 0.002496
2018-08-11 07:05:23,955:CRITICAL:EmoV2_step40: iteration: 81700: Loss: 0.479197, lr: 0.002491
2018-08-11 07:05:37,097:CRITICAL:EmoV2_step40: iteration: 81800: Loss: 0.743614, lr: 0.002485
2018-08-11 07:05:50,232:CRITICAL:EmoV2_step40: iteration: 81900: Loss: 0.512166, lr: 0.002480
2018-08-11 07:06:03,364:CRITICAL:EmoV2_step40: iteration: 82000: Loss: 0.455481, lr: 0.002475
2018-08-11 07:06:07,942:CRITICAL:EmoV2_step40: validate. Iteration: 82000: Accuracy (valence, arousal): 29.646% 24.336%
2018-08-11 07:06:07,942:CRITICAL:EmoV2_step40: validate. Iteration: 82000: Loss: nan
2018-08-11 07:06:21,099:CRITICAL:EmoV2_step40: iteration: 82100: Loss: 0.825558, lr: 0.002470
2018-08-11 07:06:34,250:CRITICAL:EmoV2_step40: iteration: 82200: Loss: 0.566258, lr: 0.002464
2018-08-11 07:06:47,408:CRITICAL:EmoV2_step40: iteration: 82300: Loss: 1.030413, lr: 0.002459
2018-08-11 07:07:00,568:CRITICAL:EmoV2_step40: iteration: 82400: Loss: 0.454282, lr: 0.002454
2018-08-11 07:07:13,715:CRITICAL:EmoV2_step40: iteration: 82500: Loss: 0.600977, lr: 0.002449
2018-08-11 07:07:26,866:CRITICAL:EmoV2_step40: iteration: 82600: Loss: 0.832808, lr: 0.002443
2018-08-11 07:07:40,021:CRITICAL:EmoV2_step40: iteration: 82700: Loss: 0.406219, lr: 0.002438
2018-08-11 07:07:53,202:CRITICAL:EmoV2_step40: iteration: 82800: Loss: 0.400741, lr: 0.002433
2018-08-11 07:08:06,355:CRITICAL:EmoV2_step40: iteration: 82900: Loss: 0.614818, lr: 0.002428
2018-08-11 07:08:19,495:CRITICAL:EmoV2_step40: iteration: 83000: Loss: 0.619220, lr: 0.002422
2018-08-11 07:08:24,142:CRITICAL:EmoV2_step40: validate. Iteration: 83000: Accuracy (valence, arousal): 30.531% 20.354%
2018-08-11 07:08:24,143:CRITICAL:EmoV2_step40: validate. Iteration: 83000: Loss: nan
2018-08-11 07:08:37,300:CRITICAL:EmoV2_step40: iteration: 83100: Loss: 0.451272, lr: 0.002417
2018-08-11 07:08:50,460:CRITICAL:EmoV2_step40: iteration: 83200: Loss: 0.864644, lr: 0.002412
2018-08-11 07:09:03,598:CRITICAL:EmoV2_step40: iteration: 83300: Loss: 0.531514, lr: 0.002407
2018-08-11 07:09:21,118:CRITICAL:EmoV2_step40: iteration: 83400: Loss: 0.789319, lr: 0.002401
2018-08-11 07:09:34,261:CRITICAL:EmoV2_step40: iteration: 83500: Loss: 0.955372, lr: 0.002396
2018-08-11 07:09:47,404:CRITICAL:EmoV2_step40: iteration: 83600: Loss: 0.358904, lr: 0.002391
2018-08-11 07:10:00,569:CRITICAL:EmoV2_step40: iteration: 83700: Loss: 0.797667, lr: 0.002386
2018-08-11 07:10:13,706:CRITICAL:EmoV2_step40: iteration: 83800: Loss: 0.347978, lr: 0.002380
2018-08-11 07:10:26,843:CRITICAL:EmoV2_step40: iteration: 83900: Loss: 0.367164, lr: 0.002375
2018-08-11 07:10:40,007:CRITICAL:EmoV2_step40: iteration: 84000: Loss: 0.336048, lr: 0.002370
2018-08-11 07:10:49,067:CRITICAL:EmoV2_step40: validate. Iteration: 84000: Accuracy (valence, arousal): 27.876% 19.469%
2018-08-11 07:10:49,068:CRITICAL:EmoV2_step40: validate. Iteration: 84000: Loss: nan
2018-08-11 07:11:02,213:CRITICAL:EmoV2_step40: iteration: 84100: Loss: 0.346705, lr: 0.002365
2018-08-11 07:11:15,373:CRITICAL:EmoV2_step40: iteration: 84200: Loss: 0.407836, lr: 0.002359
2018-08-11 07:11:28,520:CRITICAL:EmoV2_step40: iteration: 84300: Loss: 0.935616, lr: 0.002354
2018-08-11 07:11:41,667:CRITICAL:EmoV2_step40: iteration: 84400: Loss: 0.599766, lr: 0.002349
2018-08-11 07:11:54,811:CRITICAL:EmoV2_step40: iteration: 84500: Loss: 0.739327, lr: 0.002344
2018-08-11 07:12:07,946:CRITICAL:EmoV2_step40: iteration: 84600: Loss: 0.399804, lr: 0.002338
2018-08-11 07:12:21,099:CRITICAL:EmoV2_step40: iteration: 84700: Loss: 0.331555, lr: 0.002333
2018-08-11 07:12:34,267:CRITICAL:EmoV2_step40: iteration: 84800: Loss: 0.563064, lr: 0.002328
2018-08-11 07:12:47,430:CRITICAL:EmoV2_step40: iteration: 84900: Loss: 0.787458, lr: 0.002323
2018-08-11 07:13:05,013:CRITICAL:EmoV2_step40: iteration: 85000: Loss: 0.425315, lr: 0.002317
2018-08-11 07:13:09,541:CRITICAL:EmoV2_step40: validate. Iteration: 85000: Accuracy (valence, arousal): 29.204% 26.106%
2018-08-11 07:13:09,541:CRITICAL:EmoV2_step40: validate. Iteration: 85000: Loss: nan
2018-08-11 07:13:22,685:CRITICAL:EmoV2_step40: iteration: 85100: Loss: 0.471171, lr: 0.002312
2018-08-11 07:13:35,833:CRITICAL:EmoV2_step40: iteration: 85200: Loss: 0.905228, lr: 0.002307
2018-08-11 07:13:48,991:CRITICAL:EmoV2_step40: iteration: 85300: Loss: 0.478466, lr: 0.002302
2018-08-11 07:14:02,150:CRITICAL:EmoV2_step40: iteration: 85400: Loss: 0.351190, lr: 0.002296
2018-08-11 07:14:15,303:CRITICAL:EmoV2_step40: iteration: 85500: Loss: 0.537936, lr: 0.002291
2018-08-11 07:14:28,445:CRITICAL:EmoV2_step40: iteration: 85600: Loss: 0.476063, lr: 0.002286
2018-08-11 07:14:41,586:CRITICAL:EmoV2_step40: iteration: 85700: Loss: 0.422106, lr: 0.002281
2018-08-11 07:14:54,743:CRITICAL:EmoV2_step40: iteration: 85800: Loss: 1.013268, lr: 0.002275
2018-08-11 07:15:07,890:CRITICAL:EmoV2_step40: iteration: 85900: Loss: 0.440938, lr: 0.002270
2018-08-11 07:15:21,038:CRITICAL:EmoV2_step40: iteration: 86000: Loss: 0.669862, lr: 0.002265
2018-08-11 07:15:25,798:CRITICAL:EmoV2_step40: validate. Iteration: 86000: Accuracy (valence, arousal): 29.204% 23.451%
2018-08-11 07:15:25,799:CRITICAL:EmoV2_step40: validate. Iteration: 86000: Loss: nan
2018-08-11 07:15:38,956:CRITICAL:EmoV2_step40: iteration: 86100: Loss: 0.586909, lr: 0.002260
2018-08-11 07:15:52,109:CRITICAL:EmoV2_step40: iteration: 86200: Loss: 0.398501, lr: 0.002254
2018-08-11 07:16:05,255:CRITICAL:EmoV2_step40: iteration: 86300: Loss: 1.155596, lr: 0.002249
2018-08-11 07:16:18,397:CRITICAL:EmoV2_step40: iteration: 86400: Loss: 0.526049, lr: 0.002244
2018-08-11 07:16:31,542:CRITICAL:EmoV2_step40: iteration: 86500: Loss: 0.589821, lr: 0.002239
2018-08-11 07:16:49,277:CRITICAL:EmoV2_step40: iteration: 86600: Loss: 0.409692, lr: 0.002233
2018-08-11 07:17:06,766:CRITICAL:EmoV2_step40: iteration: 86700: Loss: 0.506806, lr: 0.002228
2018-08-11 07:17:19,906:CRITICAL:EmoV2_step40: iteration: 86800: Loss: 0.636063, lr: 0.002223
2018-08-11 07:17:33,051:CRITICAL:EmoV2_step40: iteration: 86900: Loss: 0.369345, lr: 0.002218
2018-08-11 07:17:46,193:CRITICAL:EmoV2_step40: iteration: 87000: Loss: 0.351494, lr: 0.002212
2018-08-11 07:17:50,934:CRITICAL:EmoV2_step40: validate. Iteration: 87000: Accuracy (valence, arousal): 31.858% 21.681%
2018-08-11 07:17:50,935:CRITICAL:EmoV2_step40: validate. Iteration: 87000: Loss: nan
2018-08-11 07:18:04,077:CRITICAL:EmoV2_step40: iteration: 87100: Loss: 0.410084, lr: 0.002207
2018-08-11 07:18:17,212:CRITICAL:EmoV2_step40: iteration: 87200: Loss: 0.692519, lr: 0.002202
2018-08-11 07:18:30,362:CRITICAL:EmoV2_step40: iteration: 87300: Loss: 0.505191, lr: 0.002197
2018-08-11 07:18:43,497:CRITICAL:EmoV2_step40: iteration: 87400: Loss: 0.626356, lr: 0.002191
2018-08-11 07:18:56,641:CRITICAL:EmoV2_step40: iteration: 87500: Loss: 0.654390, lr: 0.002186
2018-08-11 07:19:09,786:CRITICAL:EmoV2_step40: iteration: 87600: Loss: 1.340375, lr: 0.002181
2018-08-11 07:19:22,931:CRITICAL:EmoV2_step40: iteration: 87700: Loss: 0.409062, lr: 0.002176
2018-08-11 07:19:36,072:CRITICAL:EmoV2_step40: iteration: 87800: Loss: 0.714973, lr: 0.002170
2018-08-11 07:19:49,235:CRITICAL:EmoV2_step40: iteration: 87900: Loss: 0.738874, lr: 0.002165
2018-08-11 07:20:02,401:CRITICAL:EmoV2_step40: iteration: 88000: Loss: 0.525052, lr: 0.002160
2018-08-11 07:20:07,117:CRITICAL:EmoV2_step40: validate. Iteration: 88000: Accuracy (valence, arousal): 27.876% 23.894%
2018-08-11 07:20:07,118:CRITICAL:EmoV2_step40: validate. Iteration: 88000: Loss: nan
2018-08-11 07:20:20,275:CRITICAL:EmoV2_step40: iteration: 88100: Loss: 0.854354, lr: 0.002155
2018-08-11 07:20:33,419:CRITICAL:EmoV2_step40: iteration: 88200: Loss: 0.977110, lr: 0.002149
2018-08-11 07:20:50,980:CRITICAL:EmoV2_step40: iteration: 88300: Loss: 0.519389, lr: 0.002144
2018-08-11 07:21:04,116:CRITICAL:EmoV2_step40: iteration: 88400: Loss: 0.332710, lr: 0.002139
2018-08-11 07:21:17,265:CRITICAL:EmoV2_step40: iteration: 88500: Loss: 0.466128, lr: 0.002134
2018-08-11 07:21:30,414:CRITICAL:EmoV2_step40: iteration: 88600: Loss: 0.811331, lr: 0.002128
2018-08-11 07:21:43,555:CRITICAL:EmoV2_step40: iteration: 88700: Loss: 0.478983, lr: 0.002123
2018-08-11 07:21:56,698:CRITICAL:EmoV2_step40: iteration: 88800: Loss: 0.392051, lr: 0.002118
2018-08-11 07:22:09,851:CRITICAL:EmoV2_step40: iteration: 88900: Loss: 0.493962, lr: 0.002113
2018-08-11 07:22:23,000:CRITICAL:EmoV2_step40: iteration: 89000: Loss: 0.635227, lr: 0.002107
2018-08-11 07:22:27,739:CRITICAL:EmoV2_step40: validate. Iteration: 89000: Accuracy (valence, arousal): 28.761% 23.894%
2018-08-11 07:22:27,740:CRITICAL:EmoV2_step40: validate. Iteration: 89000: Loss: nan
2018-08-11 07:22:45,497:CRITICAL:EmoV2_step40: iteration: 89100: Loss: 0.705244, lr: 0.002102
2018-08-11 07:22:58,653:CRITICAL:EmoV2_step40: iteration: 89200: Loss: 0.412013, lr: 0.002097
2018-08-11 07:23:11,824:CRITICAL:EmoV2_step40: iteration: 89300: Loss: 0.458826, lr: 0.002092
2018-08-11 07:23:24,967:CRITICAL:EmoV2_step40: iteration: 89400: Loss: 0.698279, lr: 0.002086
2018-08-11 07:23:38,110:CRITICAL:EmoV2_step40: iteration: 89500: Loss: 0.454111, lr: 0.002081
2018-08-11 07:23:51,253:CRITICAL:EmoV2_step40: iteration: 89600: Loss: 1.068954, lr: 0.002076
2018-08-11 07:24:04,395:CRITICAL:EmoV2_step40: iteration: 89700: Loss: 0.838249, lr: 0.002071
2018-08-11 07:24:17,545:CRITICAL:EmoV2_step40: iteration: 89800: Loss: 0.504791, lr: 0.002065
2018-08-11 07:24:30,691:CRITICAL:EmoV2_step40: iteration: 89900: Loss: 0.847919, lr: 0.002060
2018-08-11 07:25:29,787:CRITICAL:EmoV2_step40: iteration: 90000: Loss: 0.962872, lr: 0.002055
2018-08-11 07:25:35,211:CRITICAL:EmoV2_step40: validate. Iteration: 90000: Accuracy (valence, arousal): 27.434% 21.681%
2018-08-11 07:25:35,212:CRITICAL:EmoV2_step40: validate. Iteration: 90000: Loss: nan
2018-08-11 07:25:48,270:CRITICAL:EmoV2_step40: iteration: 90100: Loss: 0.849604, lr: 0.002050
2018-08-11 07:26:01,335:CRITICAL:EmoV2_step40: iteration: 90200: Loss: 0.525688, lr: 0.002044
2018-08-11 07:26:14,409:CRITICAL:EmoV2_step40: iteration: 90300: Loss: 0.638585, lr: 0.002039
2018-08-11 07:26:27,492:CRITICAL:EmoV2_step40: iteration: 90400: Loss: 0.698173, lr: 0.002034
2018-08-11 07:26:40,583:CRITICAL:EmoV2_step40: iteration: 90500: Loss: 0.456513, lr: 0.002029
2018-08-11 07:26:53,684:CRITICAL:EmoV2_step40: iteration: 90600: Loss: 1.210376, lr: 0.002023
2018-08-11 07:27:06,789:CRITICAL:EmoV2_step40: iteration: 90700: Loss: 0.934316, lr: 0.002018
2018-08-11 07:27:19,888:CRITICAL:EmoV2_step40: iteration: 90800: Loss: 0.473402, lr: 0.002013
2018-08-11 07:27:33,018:CRITICAL:EmoV2_step40: iteration: 90900: Loss: 0.660684, lr: 0.002008
2018-08-11 07:27:46,173:CRITICAL:EmoV2_step40: iteration: 91000: Loss: 0.372086, lr: 0.002002
2018-08-11 07:27:51,195:CRITICAL:EmoV2_step40: validate. Iteration: 91000: Accuracy (valence, arousal): 32.743% 20.354%
2018-08-11 07:27:51,195:CRITICAL:EmoV2_step40: validate. Iteration: 91000: Loss: nan
2018-08-11 07:28:04,378:CRITICAL:EmoV2_step40: iteration: 91100: Loss: 0.297924, lr: 0.001997
2018-08-11 07:28:17,546:CRITICAL:EmoV2_step40: iteration: 91200: Loss: 0.333381, lr: 0.001992
2018-08-11 07:28:30,714:CRITICAL:EmoV2_step40: iteration: 91300: Loss: 0.601989, lr: 0.001987
2018-08-11 07:28:43,875:CRITICAL:EmoV2_step40: iteration: 91400: Loss: 0.467470, lr: 0.001981
2018-08-11 07:28:57,023:CRITICAL:EmoV2_step40: iteration: 91500: Loss: 0.422552, lr: 0.001976
2018-08-11 07:29:19,136:CRITICAL:EmoV2_step40: iteration: 91600: Loss: 0.689325, lr: 0.001971
2018-08-11 07:29:32,285:CRITICAL:EmoV2_step40: iteration: 91700: Loss: 0.627755, lr: 0.001966
2018-08-11 07:29:45,437:CRITICAL:EmoV2_step40: iteration: 91800: Loss: 0.897377, lr: 0.001960
2018-08-11 07:29:58,588:CRITICAL:EmoV2_step40: iteration: 91900: Loss: 0.680282, lr: 0.001955
2018-08-11 07:30:11,739:CRITICAL:EmoV2_step40: iteration: 92000: Loss: 0.705522, lr: 0.001950
2018-08-11 07:30:16,330:CRITICAL:EmoV2_step40: validate. Iteration: 92000: Accuracy (valence, arousal): 28.319% 19.469%
2018-08-11 07:30:16,331:CRITICAL:EmoV2_step40: validate. Iteration: 92000: Loss: nan
2018-08-11 07:30:29,500:CRITICAL:EmoV2_step40: iteration: 92100: Loss: 0.755732, lr: 0.001945
2018-08-11 07:30:42,654:CRITICAL:EmoV2_step40: iteration: 92200: Loss: 0.909129, lr: 0.001939
2018-08-11 07:30:55,816:CRITICAL:EmoV2_step40: iteration: 92300: Loss: 0.380991, lr: 0.001934
2018-08-11 07:31:08,983:CRITICAL:EmoV2_step40: iteration: 92400: Loss: 0.693432, lr: 0.001929
2018-08-11 07:31:22,123:CRITICAL:EmoV2_step40: iteration: 92500: Loss: 0.828016, lr: 0.001924
2018-08-11 07:31:35,268:CRITICAL:EmoV2_step40: iteration: 92600: Loss: 0.662820, lr: 0.001918
2018-08-11 07:31:48,449:CRITICAL:EmoV2_step40: iteration: 92700: Loss: 0.418905, lr: 0.001913
2018-08-11 07:32:01,613:CRITICAL:EmoV2_step40: iteration: 92800: Loss: 0.403489, lr: 0.001908
2018-08-11 07:32:14,765:CRITICAL:EmoV2_step40: iteration: 92900: Loss: 0.810251, lr: 0.001903
2018-08-11 07:32:27,938:CRITICAL:EmoV2_step40: iteration: 93000: Loss: 0.341536, lr: 0.001897
2018-08-11 07:32:32,488:CRITICAL:EmoV2_step40: validate. Iteration: 93000: Accuracy (valence, arousal): 33.628% 19.469%
2018-08-11 07:32:32,488:CRITICAL:EmoV2_step40: validate. Iteration: 93000: Loss: nan
2018-08-11 07:32:45,644:CRITICAL:EmoV2_step40: iteration: 93100: Loss: 0.448940, lr: 0.001892
2018-08-11 07:32:58,792:CRITICAL:EmoV2_step40: iteration: 93200: Loss: 0.520083, lr: 0.001887
2018-08-11 07:33:16,304:CRITICAL:EmoV2_step40: iteration: 93300: Loss: 0.714172, lr: 0.001882
2018-08-11 07:33:29,454:CRITICAL:EmoV2_step40: iteration: 93400: Loss: 0.487344, lr: 0.001876
2018-08-11 07:33:42,622:CRITICAL:EmoV2_step40: iteration: 93500: Loss: 0.343543, lr: 0.001871
2018-08-11 07:33:55,765:CRITICAL:EmoV2_step40: iteration: 93600: Loss: 0.721061, lr: 0.001866
2018-08-11 07:34:08,918:CRITICAL:EmoV2_step40: iteration: 93700: Loss: 0.582886, lr: 0.001861
2018-08-11 07:34:22,081:CRITICAL:EmoV2_step40: iteration: 93800: Loss: 0.645361, lr: 0.001855
2018-08-11 07:34:35,235:CRITICAL:EmoV2_step40: iteration: 93900: Loss: 0.814268, lr: 0.001850
2018-08-11 07:34:48,390:CRITICAL:EmoV2_step40: iteration: 94000: Loss: 0.674123, lr: 0.001845
2018-08-11 07:34:53,051:CRITICAL:EmoV2_step40: validate. Iteration: 94000: Accuracy (valence, arousal): 31.416% 22.124%
2018-08-11 07:34:53,051:CRITICAL:EmoV2_step40: validate. Iteration: 94000: Loss: nan
2018-08-11 07:35:10,803:CRITICAL:EmoV2_step40: iteration: 94100: Loss: 0.841820, lr: 0.001840
2018-08-11 07:35:23,942:CRITICAL:EmoV2_step40: iteration: 94200: Loss: 0.633705, lr: 0.001834
2018-08-11 07:35:37,076:CRITICAL:EmoV2_step40: iteration: 94300: Loss: 0.460229, lr: 0.001829
2018-08-11 07:35:50,211:CRITICAL:EmoV2_step40: iteration: 94400: Loss: 0.487482, lr: 0.001824
2018-08-11 07:36:03,362:CRITICAL:EmoV2_step40: iteration: 94500: Loss: 0.425810, lr: 0.001819
2018-08-11 07:36:16,515:CRITICAL:EmoV2_step40: iteration: 94600: Loss: 0.403027, lr: 0.001813
2018-08-11 07:36:29,669:CRITICAL:EmoV2_step40: iteration: 94700: Loss: 0.486270, lr: 0.001808
2018-08-11 07:36:42,824:CRITICAL:EmoV2_step40: iteration: 94800: Loss: 0.722737, lr: 0.001803
2018-08-11 07:37:00,336:CRITICAL:EmoV2_step40: iteration: 94900: Loss: 0.387183, lr: 0.001798
2018-08-11 07:37:13,476:CRITICAL:EmoV2_step40: iteration: 95000: Loss: 0.482109, lr: 0.001792
2018-08-11 07:37:18,242:CRITICAL:EmoV2_step40: validate. Iteration: 95000: Accuracy (valence, arousal): 26.991% 18.142%
2018-08-11 07:37:18,243:CRITICAL:EmoV2_step40: validate. Iteration: 95000: Loss: nan
2018-08-11 07:37:31,394:CRITICAL:EmoV2_step40: iteration: 95100: Loss: 0.951731, lr: 0.001787
2018-08-11 07:37:44,529:CRITICAL:EmoV2_step40: iteration: 95200: Loss: 0.657779, lr: 0.001782
2018-08-11 07:37:57,684:CRITICAL:EmoV2_step40: iteration: 95300: Loss: 0.567725, lr: 0.001777
2018-08-11 07:38:10,835:CRITICAL:EmoV2_step40: iteration: 95400: Loss: 0.807406, lr: 0.001771
2018-08-11 07:38:23,990:CRITICAL:EmoV2_step40: iteration: 95500: Loss: 0.550351, lr: 0.001766
2018-08-11 07:38:37,146:CRITICAL:EmoV2_step40: iteration: 95600: Loss: 0.539476, lr: 0.001761
2018-08-11 07:38:50,298:CRITICAL:EmoV2_step40: iteration: 95700: Loss: 0.951145, lr: 0.001756
2018-08-11 07:39:03,442:CRITICAL:EmoV2_step40: iteration: 95800: Loss: 0.574569, lr: 0.001750
2018-08-11 07:39:16,584:CRITICAL:EmoV2_step40: iteration: 95900: Loss: 0.793482, lr: 0.001745
2018-08-11 07:39:29,745:CRITICAL:EmoV2_step40: iteration: 96000: Loss: 0.722729, lr: 0.001740
2018-08-11 07:39:34,276:CRITICAL:EmoV2_step40: validate. Iteration: 96000: Accuracy (valence, arousal): 29.646% 19.469%
2018-08-11 07:39:34,277:CRITICAL:EmoV2_step40: validate. Iteration: 96000: Loss: nan
2018-08-11 07:39:47,442:CRITICAL:EmoV2_step40: iteration: 96100: Loss: 0.468560, lr: 0.001735
2018-08-11 07:40:00,609:CRITICAL:EmoV2_step40: iteration: 96200: Loss: 0.535143, lr: 0.001729
2018-08-11 07:40:13,765:CRITICAL:EmoV2_step40: iteration: 96300: Loss: 0.657825, lr: 0.001724
2018-08-11 07:40:26,941:CRITICAL:EmoV2_step40: iteration: 96400: Loss: 0.463376, lr: 0.001719
2018-08-11 07:40:40,092:CRITICAL:EmoV2_step40: iteration: 96500: Loss: 0.304198, lr: 0.001714
2018-08-11 07:41:02,212:CRITICAL:EmoV2_step40: iteration: 96600: Loss: 0.846580, lr: 0.001708
2018-08-11 07:41:15,366:CRITICAL:EmoV2_step40: iteration: 96700: Loss: 0.673610, lr: 0.001703
2018-08-11 07:41:28,517:CRITICAL:EmoV2_step40: iteration: 96800: Loss: 0.886517, lr: 0.001698
2018-08-11 07:41:41,668:CRITICAL:EmoV2_step40: iteration: 96900: Loss: 0.556986, lr: 0.001693
2018-08-11 07:41:54,818:CRITICAL:EmoV2_step40: iteration: 97000: Loss: 0.579977, lr: 0.001687
2018-08-11 07:41:59,450:CRITICAL:EmoV2_step40: validate. Iteration: 97000: Accuracy (valence, arousal): 31.858% 22.124%
2018-08-11 07:41:59,451:CRITICAL:EmoV2_step40: validate. Iteration: 97000: Loss: nan
2018-08-11 07:42:12,586:CRITICAL:EmoV2_step40: iteration: 97100: Loss: 0.747334, lr: 0.001682
2018-08-11 07:42:25,728:CRITICAL:EmoV2_step40: iteration: 97200: Loss: 0.376606, lr: 0.001677
2018-08-11 07:42:38,867:CRITICAL:EmoV2_step40: iteration: 97300: Loss: 0.750335, lr: 0.001672
2018-08-11 07:42:52,028:CRITICAL:EmoV2_step40: iteration: 97400: Loss: 0.940611, lr: 0.001666
2018-08-11 07:43:05,169:CRITICAL:EmoV2_step40: iteration: 97500: Loss: 1.014404, lr: 0.001661
2018-08-11 07:43:18,318:CRITICAL:EmoV2_step40: iteration: 97600: Loss: 0.660417, lr: 0.001656
2018-08-11 07:43:31,466:CRITICAL:EmoV2_step40: iteration: 97700: Loss: 0.327924, lr: 0.001651
2018-08-11 07:43:44,607:CRITICAL:EmoV2_step40: iteration: 97800: Loss: 0.406258, lr: 0.001645
2018-08-11 07:43:57,756:CRITICAL:EmoV2_step40: iteration: 97900: Loss: 0.599201, lr: 0.001640
2018-08-11 07:44:10,909:CRITICAL:EmoV2_step40: iteration: 98000: Loss: 0.624502, lr: 0.001635
2018-08-11 07:44:15,412:CRITICAL:EmoV2_step40: validate. Iteration: 98000: Accuracy (valence, arousal): 30.973% 21.681%
2018-08-11 07:44:15,412:CRITICAL:EmoV2_step40: validate. Iteration: 98000: Loss: nan
2018-08-11 07:44:28,554:CRITICAL:EmoV2_step40: iteration: 98100: Loss: 0.645437, lr: 0.001630
2018-08-11 07:44:46,137:CRITICAL:EmoV2_step40: iteration: 98200: Loss: 0.448664, lr: 0.001624
2018-08-11 07:44:59,298:CRITICAL:EmoV2_step40: iteration: 98300: Loss: 0.413376, lr: 0.001619
2018-08-11 07:45:12,458:CRITICAL:EmoV2_step40: iteration: 98400: Loss: 0.512387, lr: 0.001614
2018-08-11 07:45:25,602:CRITICAL:EmoV2_step40: iteration: 98500: Loss: 0.895770, lr: 0.001609
2018-08-11 07:45:38,744:CRITICAL:EmoV2_step40: iteration: 98600: Loss: 0.387690, lr: 0.001603
2018-08-11 07:45:51,891:CRITICAL:EmoV2_step40: iteration: 98700: Loss: 0.605159, lr: 0.001598
2018-08-11 07:46:05,048:CRITICAL:EmoV2_step40: iteration: 98800: Loss: 0.801939, lr: 0.001593
2018-08-11 07:46:18,210:CRITICAL:EmoV2_step40: iteration: 98900: Loss: 0.833528, lr: 0.001588
2018-08-11 07:46:31,365:CRITICAL:EmoV2_step40: iteration: 99000: Loss: 0.345942, lr: 0.001582
2018-08-11 07:46:36,103:CRITICAL:EmoV2_step40: validate. Iteration: 99000: Accuracy (valence, arousal): 30.531% 19.027%
2018-08-11 07:46:36,104:CRITICAL:EmoV2_step40: validate. Iteration: 99000: Loss: nan
2018-08-11 07:46:53,920:CRITICAL:EmoV2_step40: iteration: 99100: Loss: 0.771955, lr: 0.001577
2018-08-11 07:47:07,059:CRITICAL:EmoV2_step40: iteration: 99200: Loss: 0.684402, lr: 0.001572
2018-08-11 07:47:20,215:CRITICAL:EmoV2_step40: iteration: 99300: Loss: 0.794997, lr: 0.001567
2018-08-11 07:47:33,357:CRITICAL:EmoV2_step40: iteration: 99400: Loss: 0.982411, lr: 0.001561
2018-08-11 07:47:46,503:CRITICAL:EmoV2_step40: iteration: 99500: Loss: 0.642480, lr: 0.001556
2018-08-11 07:47:59,649:CRITICAL:EmoV2_step40: iteration: 99600: Loss: 0.557678, lr: 0.001551
2018-08-11 07:48:12,799:CRITICAL:EmoV2_step40: iteration: 99700: Loss: 0.785368, lr: 0.001546
2018-08-11 07:48:25,941:CRITICAL:EmoV2_step40: iteration: 99800: Loss: 0.417460, lr: 0.001540
2018-08-11 07:48:43,491:CRITICAL:EmoV2_step40: iteration: 99900: Loss: 0.467446, lr: 0.001535
2018-08-11 07:49:37,929:CRITICAL:EmoV2_step40: iteration: 100000: Loss: 0.579599, lr: 0.001530
2018-08-11 07:49:42,718:CRITICAL:EmoV2_step40: validate. Iteration: 100000: Accuracy (valence, arousal): 31.416% 17.257%
2018-08-11 07:49:42,718:CRITICAL:EmoV2_step40: validate. Iteration: 100000: Loss: nan
2018-08-11 07:49:55,781:CRITICAL:EmoV2_step40: iteration: 100100: Loss: 0.391273, lr: 0.001525
2018-08-11 07:50:08,834:CRITICAL:EmoV2_step40: iteration: 100200: Loss: 0.673556, lr: 0.001519
2018-08-11 07:50:21,913:CRITICAL:EmoV2_step40: iteration: 100300: Loss: 0.699672, lr: 0.001514
2018-08-11 07:50:35,016:CRITICAL:EmoV2_step40: iteration: 100400: Loss: 0.409631, lr: 0.001509
2018-08-11 07:50:48,124:CRITICAL:EmoV2_step40: iteration: 100500: Loss: 0.943880, lr: 0.001504
2018-08-11 07:51:01,206:CRITICAL:EmoV2_step40: iteration: 100600: Loss: 0.534598, lr: 0.001498
2018-08-11 07:51:14,305:CRITICAL:EmoV2_step40: iteration: 100700: Loss: 0.650441, lr: 0.001493
2018-08-11 07:51:27,421:CRITICAL:EmoV2_step40: iteration: 100800: Loss: 0.749643, lr: 0.001488
2018-08-11 07:51:40,573:CRITICAL:EmoV2_step40: iteration: 100900: Loss: 0.522566, lr: 0.001483
2018-08-11 07:51:53,716:CRITICAL:EmoV2_step40: iteration: 101000: Loss: 0.593831, lr: 0.001477
2018-08-11 07:51:58,476:CRITICAL:EmoV2_step40: validate. Iteration: 101000: Accuracy (valence, arousal): 31.858% 21.239%
2018-08-11 07:51:58,477:CRITICAL:EmoV2_step40: validate. Iteration: 101000: Loss: nan
2018-08-11 07:52:11,640:CRITICAL:EmoV2_step40: iteration: 101100: Loss: 0.487403, lr: 0.001472
2018-08-11 07:52:24,794:CRITICAL:EmoV2_step40: iteration: 101200: Loss: 0.567398, lr: 0.001467
2018-08-11 07:52:37,956:CRITICAL:EmoV2_step40: iteration: 101300: Loss: 0.514996, lr: 0.001462
2018-08-11 07:52:51,123:CRITICAL:EmoV2_step40: iteration: 101400: Loss: 0.374662, lr: 0.001456
2018-08-11 07:53:08,624:CRITICAL:EmoV2_step40: iteration: 101500: Loss: 0.318701, lr: 0.001451
2018-08-11 07:53:26,365:CRITICAL:EmoV2_step40: iteration: 101600: Loss: 0.789661, lr: 0.001446
2018-08-11 07:53:39,521:CRITICAL:EmoV2_step40: iteration: 101700: Loss: 0.319297, lr: 0.001441
2018-08-11 07:53:52,663:CRITICAL:EmoV2_step40: iteration: 101800: Loss: 0.717956, lr: 0.001435
2018-08-11 07:54:05,808:CRITICAL:EmoV2_step40: iteration: 101900: Loss: 0.772081, lr: 0.001430
2018-08-11 07:54:18,955:CRITICAL:EmoV2_step40: iteration: 102000: Loss: 0.966298, lr: 0.001425
2018-08-11 07:54:23,391:CRITICAL:EmoV2_step40: validate. Iteration: 102000: Accuracy (valence, arousal): 30.088% 19.027%
2018-08-11 07:54:23,391:CRITICAL:EmoV2_step40: validate. Iteration: 102000: Loss: nan
2018-08-11 07:54:36,556:CRITICAL:EmoV2_step40: iteration: 102100: Loss: 0.704878, lr: 0.001420
2018-08-11 07:54:49,705:CRITICAL:EmoV2_step40: iteration: 102200: Loss: 0.368744, lr: 0.001414
2018-08-11 07:55:02,866:CRITICAL:EmoV2_step40: iteration: 102300: Loss: 0.339242, lr: 0.001409
2018-08-11 07:55:16,021:CRITICAL:EmoV2_step40: iteration: 102400: Loss: 1.059668, lr: 0.001404
2018-08-11 07:55:29,181:CRITICAL:EmoV2_step40: iteration: 102500: Loss: 0.579065, lr: 0.001399
2018-08-11 07:55:42,351:CRITICAL:EmoV2_step40: iteration: 102600: Loss: 0.278804, lr: 0.001393
2018-08-11 07:55:55,503:CRITICAL:EmoV2_step40: iteration: 102700: Loss: 0.523064, lr: 0.001388
2018-08-11 07:56:08,649:CRITICAL:EmoV2_step40: iteration: 102800: Loss: 0.560283, lr: 0.001383
2018-08-11 07:56:21,808:CRITICAL:EmoV2_step40: iteration: 102900: Loss: 0.456779, lr: 0.001378
2018-08-11 07:56:34,954:CRITICAL:EmoV2_step40: iteration: 103000: Loss: 0.544427, lr: 0.001372
2018-08-11 07:56:39,558:CRITICAL:EmoV2_step40: validate. Iteration: 103000: Accuracy (valence, arousal): 30.088% 18.584%
2018-08-11 07:56:39,558:CRITICAL:EmoV2_step40: validate. Iteration: 103000: Loss: nan
2018-08-11 07:56:52,726:CRITICAL:EmoV2_step40: iteration: 103100: Loss: 0.520391, lr: 0.001367
2018-08-11 07:57:10,240:CRITICAL:EmoV2_step40: iteration: 103200: Loss: 0.411975, lr: 0.001362
2018-08-11 07:57:23,380:CRITICAL:EmoV2_step40: iteration: 103300: Loss: 0.704960, lr: 0.001357
2018-08-11 07:57:36,523:CRITICAL:EmoV2_step40: iteration: 103400: Loss: 0.282199, lr: 0.001352
2018-08-11 07:57:49,702:CRITICAL:EmoV2_step40: iteration: 103500: Loss: 0.520120, lr: 0.001346
2018-08-11 07:58:02,859:CRITICAL:EmoV2_step40: iteration: 103600: Loss: 0.456953, lr: 0.001341
2018-08-11 07:58:15,997:CRITICAL:EmoV2_step40: iteration: 103700: Loss: 0.305257, lr: 0.001336
2018-08-11 07:58:29,162:CRITICAL:EmoV2_step40: iteration: 103800: Loss: 0.421380, lr: 0.001331
2018-08-11 07:58:42,323:CRITICAL:EmoV2_step40: iteration: 103900: Loss: 0.531390, lr: 0.001325
2018-08-11 07:58:55,478:CRITICAL:EmoV2_step40: iteration: 104000: Loss: 0.836849, lr: 0.001320
2018-08-11 07:59:00,190:CRITICAL:EmoV2_step40: validate. Iteration: 104000: Accuracy (valence, arousal): 33.628% 18.142%
2018-08-11 07:59:00,191:CRITICAL:EmoV2_step40: validate. Iteration: 104000: Loss: nan
2018-08-11 07:59:17,963:CRITICAL:EmoV2_step40: iteration: 104100: Loss: 0.564879, lr: 0.001315
2018-08-11 07:59:31,108:CRITICAL:EmoV2_step40: iteration: 104200: Loss: 0.451175, lr: 0.001310
2018-08-11 07:59:44,245:CRITICAL:EmoV2_step40: iteration: 104300: Loss: 0.401339, lr: 0.001304
2018-08-11 07:59:57,400:CRITICAL:EmoV2_step40: iteration: 104400: Loss: 0.440867, lr: 0.001299
2018-08-11 08:00:10,548:CRITICAL:EmoV2_step40: iteration: 104500: Loss: 0.416444, lr: 0.001294
2018-08-11 08:00:23,698:CRITICAL:EmoV2_step40: iteration: 104600: Loss: 0.341009, lr: 0.001289
2018-08-11 08:00:36,872:CRITICAL:EmoV2_step40: iteration: 104700: Loss: 0.604846, lr: 0.001283
2018-08-11 08:00:54,407:CRITICAL:EmoV2_step40: iteration: 104800: Loss: 0.560338, lr: 0.001278
2018-08-11 08:01:07,547:CRITICAL:EmoV2_step40: iteration: 104900: Loss: 0.949556, lr: 0.001273
2018-08-11 08:01:20,695:CRITICAL:EmoV2_step40: iteration: 105000: Loss: 0.426085, lr: 0.001268
2018-08-11 08:01:25,730:CRITICAL:EmoV2_step40: validate. Iteration: 105000: Accuracy (valence, arousal): 28.761% 19.027%
2018-08-11 08:01:25,730:CRITICAL:EmoV2_step40: validate. Iteration: 105000: Loss: nan
2018-08-11 08:01:38,877:CRITICAL:EmoV2_step40: iteration: 105100: Loss: 0.387226, lr: 0.001262
2018-08-11 08:01:52,034:CRITICAL:EmoV2_step40: iteration: 105200: Loss: 0.331010, lr: 0.001257
2018-08-11 08:02:05,189:CRITICAL:EmoV2_step40: iteration: 105300: Loss: 0.449906, lr: 0.001252
2018-08-11 08:02:18,333:CRITICAL:EmoV2_step40: iteration: 105400: Loss: 0.513338, lr: 0.001247
2018-08-11 08:02:31,470:CRITICAL:EmoV2_step40: iteration: 105500: Loss: 0.321126, lr: 0.001241
2018-08-11 08:02:44,621:CRITICAL:EmoV2_step40: iteration: 105600: Loss: 0.461902, lr: 0.001236
2018-08-11 08:02:57,769:CRITICAL:EmoV2_step40: iteration: 105700: Loss: 0.566232, lr: 0.001231
2018-08-11 08:03:10,913:CRITICAL:EmoV2_step40: iteration: 105800: Loss: 0.507436, lr: 0.001226
2018-08-11 08:03:24,083:CRITICAL:EmoV2_step40: iteration: 105900: Loss: 0.375543, lr: 0.001220
2018-08-11 08:03:37,241:CRITICAL:EmoV2_step40: iteration: 106000: Loss: 0.833333, lr: 0.001215
2018-08-11 08:03:41,885:CRITICAL:EmoV2_step40: validate. Iteration: 106000: Accuracy (valence, arousal): 30.088% 19.469%
2018-08-11 08:03:41,886:CRITICAL:EmoV2_step40: validate. Iteration: 106000: Loss: nan
2018-08-11 08:03:55,034:CRITICAL:EmoV2_step40: iteration: 106100: Loss: 0.494190, lr: 0.001210
2018-08-11 08:04:08,179:CRITICAL:EmoV2_step40: iteration: 106200: Loss: 0.355587, lr: 0.001205
2018-08-11 08:04:21,342:CRITICAL:EmoV2_step40: iteration: 106300: Loss: 0.333939, lr: 0.001199
2018-08-11 08:04:34,513:CRITICAL:EmoV2_step40: iteration: 106400: Loss: 0.513624, lr: 0.001194
2018-08-11 08:04:52,098:CRITICAL:EmoV2_step40: iteration: 106500: Loss: 0.706558, lr: 0.001189
2018-08-11 08:05:09,838:CRITICAL:EmoV2_step40: iteration: 106600: Loss: 0.886492, lr: 0.001184
2018-08-11 08:05:22,995:CRITICAL:EmoV2_step40: iteration: 106700: Loss: 0.429866, lr: 0.001178
2018-08-11 08:05:36,139:CRITICAL:EmoV2_step40: iteration: 106800: Loss: 0.487909, lr: 0.001173
2018-08-11 08:05:49,278:CRITICAL:EmoV2_step40: iteration: 106900: Loss: 0.620018, lr: 0.001168
2018-08-11 08:06:02,421:CRITICAL:EmoV2_step40: iteration: 107000: Loss: 0.312556, lr: 0.001163
2018-08-11 08:06:07,048:CRITICAL:EmoV2_step40: validate. Iteration: 107000: Accuracy (valence, arousal): 30.531% 18.142%
2018-08-11 08:06:07,048:CRITICAL:EmoV2_step40: validate. Iteration: 107000: Loss: nan
2018-08-11 08:06:20,194:CRITICAL:EmoV2_step40: iteration: 107100: Loss: 0.521739, lr: 0.001157
2018-08-11 08:06:33,338:CRITICAL:EmoV2_step40: iteration: 107200: Loss: 0.594669, lr: 0.001152
2018-08-11 08:06:46,500:CRITICAL:EmoV2_step40: iteration: 107300: Loss: 0.350689, lr: 0.001147
2018-08-11 08:06:59,642:CRITICAL:EmoV2_step40: iteration: 107400: Loss: 1.069290, lr: 0.001142
2018-08-11 08:07:12,807:CRITICAL:EmoV2_step40: iteration: 107500: Loss: 0.451135, lr: 0.001136
2018-08-11 08:07:25,974:CRITICAL:EmoV2_step40: iteration: 107600: Loss: 0.334617, lr: 0.001131
2018-08-11 08:07:39,137:CRITICAL:EmoV2_step40: iteration: 107700: Loss: 0.435344, lr: 0.001126
2018-08-11 08:07:52,287:CRITICAL:EmoV2_step40: iteration: 107800: Loss: 0.613442, lr: 0.001121
2018-08-11 08:08:05,446:CRITICAL:EmoV2_step40: iteration: 107900: Loss: 0.348187, lr: 0.001115
2018-08-11 08:08:18,608:CRITICAL:EmoV2_step40: iteration: 108000: Loss: 0.279961, lr: 0.001110
2018-08-11 08:08:23,078:CRITICAL:EmoV2_step40: validate. Iteration: 108000: Accuracy (valence, arousal): 30.973% 19.469%
2018-08-11 08:08:23,078:CRITICAL:EmoV2_step40: validate. Iteration: 108000: Loss: nan
2018-08-11 08:08:40,615:CRITICAL:EmoV2_step40: iteration: 108100: Loss: 0.631611, lr: 0.001105
2018-08-11 08:08:53,761:CRITICAL:EmoV2_step40: iteration: 108200: Loss: 0.321922, lr: 0.001100
2018-08-11 08:09:06,931:CRITICAL:EmoV2_step40: iteration: 108300: Loss: 0.753641, lr: 0.001094
2018-08-11 08:09:20,081:CRITICAL:EmoV2_step40: iteration: 108400: Loss: 0.738600, lr: 0.001089
2018-08-11 08:09:33,232:CRITICAL:EmoV2_step40: iteration: 108500: Loss: 0.764740, lr: 0.001084
2018-08-11 08:09:46,372:CRITICAL:EmoV2_step40: iteration: 108600: Loss: 0.328166, lr: 0.001079
2018-08-11 08:09:59,540:CRITICAL:EmoV2_step40: iteration: 108700: Loss: 0.571623, lr: 0.001073
2018-08-11 08:10:12,701:CRITICAL:EmoV2_step40: iteration: 108800: Loss: 0.714924, lr: 0.001068
2018-08-11 08:10:25,859:CRITICAL:EmoV2_step40: iteration: 108900: Loss: 0.392595, lr: 0.001063
2018-08-11 08:10:39,023:CRITICAL:EmoV2_step40: iteration: 109000: Loss: 0.807538, lr: 0.001058
2018-08-11 08:10:43,529:CRITICAL:EmoV2_step40: validate. Iteration: 109000: Accuracy (valence, arousal): 33.186% 23.009%
2018-08-11 08:10:43,529:CRITICAL:EmoV2_step40: validate. Iteration: 109000: Loss: nan
2018-08-11 08:11:01,305:CRITICAL:EmoV2_step40: iteration: 109100: Loss: 0.451062, lr: 0.001052
2018-08-11 08:11:14,459:CRITICAL:EmoV2_step40: iteration: 109200: Loss: 0.275855, lr: 0.001047
2018-08-11 08:11:27,613:CRITICAL:EmoV2_step40: iteration: 109300: Loss: 0.305993, lr: 0.001042
2018-08-11 08:11:40,762:CRITICAL:EmoV2_step40: iteration: 109400: Loss: 0.365605, lr: 0.001037
2018-08-11 08:11:53,905:CRITICAL:EmoV2_step40: iteration: 109500: Loss: 0.455157, lr: 0.001031
2018-08-11 08:12:07,060:CRITICAL:EmoV2_step40: iteration: 109600: Loss: 0.591769, lr: 0.001026
2018-08-11 08:12:20,209:CRITICAL:EmoV2_step40: iteration: 109700: Loss: 0.401166, lr: 0.001021
2018-08-11 08:12:37,766:CRITICAL:EmoV2_step40: iteration: 109800: Loss: 0.307739, lr: 0.001016
2018-08-11 08:12:50,908:CRITICAL:EmoV2_step40: iteration: 109900: Loss: 0.598368, lr: 0.001010
2018-08-11 08:13:45,304:CRITICAL:EmoV2_step40: iteration: 110000: Loss: 0.660021, lr: 0.001005
2018-08-11 08:13:50,191:CRITICAL:EmoV2_step40: validate. Iteration: 110000: Accuracy (valence, arousal): 30.531% 18.584%
2018-08-11 08:13:50,192:CRITICAL:EmoV2_step40: validate. Iteration: 110000: Loss: nan
2018-08-11 08:14:03,255:CRITICAL:EmoV2_step40: iteration: 110100: Loss: 0.474085, lr: 0.001000
2018-08-11 08:14:16,322:CRITICAL:EmoV2_step40: iteration: 110200: Loss: 0.845457, lr: 0.000995
2018-08-11 08:14:29,403:CRITICAL:EmoV2_step40: iteration: 110300: Loss: 0.275803, lr: 0.000989
2018-08-11 08:14:42,501:CRITICAL:EmoV2_step40: iteration: 110400: Loss: 0.703292, lr: 0.000984
2018-08-11 08:14:55,607:CRITICAL:EmoV2_step40: iteration: 110500: Loss: 0.453184, lr: 0.000979
2018-08-11 08:15:08,701:CRITICAL:EmoV2_step40: iteration: 110600: Loss: 0.493654, lr: 0.000974
2018-08-11 08:15:21,794:CRITICAL:EmoV2_step40: iteration: 110700: Loss: 0.790189, lr: 0.000968
2018-08-11 08:15:34,910:CRITICAL:EmoV2_step40: iteration: 110800: Loss: 0.403419, lr: 0.000963
2018-08-11 08:15:48,066:CRITICAL:EmoV2_step40: iteration: 110900: Loss: 0.291941, lr: 0.000958
2018-08-11 08:16:01,232:CRITICAL:EmoV2_step40: iteration: 111000: Loss: 0.885873, lr: 0.000953
2018-08-11 08:16:05,927:CRITICAL:EmoV2_step40: validate. Iteration: 111000: Accuracy (valence, arousal): 30.531% 18.584%
2018-08-11 08:16:05,928:CRITICAL:EmoV2_step40: validate. Iteration: 111000: Loss: nan
2018-08-11 08:16:19,087:CRITICAL:EmoV2_step40: iteration: 111100: Loss: 0.448422, lr: 0.000947
2018-08-11 08:16:32,234:CRITICAL:EmoV2_step40: iteration: 111200: Loss: 0.394293, lr: 0.000942
2018-08-11 08:16:45,391:CRITICAL:EmoV2_step40: iteration: 111300: Loss: 0.578185, lr: 0.000937
2018-08-11 08:17:03,016:CRITICAL:EmoV2_step40: iteration: 111400: Loss: 0.445988, lr: 0.000932
2018-08-11 08:17:16,178:CRITICAL:EmoV2_step40: iteration: 111500: Loss: 0.556891, lr: 0.000926
2018-08-11 08:17:29,323:CRITICAL:EmoV2_step40: iteration: 111600: Loss: 0.472230, lr: 0.000921
2018-08-11 08:17:47,060:CRITICAL:EmoV2_step40: iteration: 111700: Loss: 0.492380, lr: 0.000916
2018-08-11 08:18:00,217:CRITICAL:EmoV2_step40: iteration: 111800: Loss: 0.874693, lr: 0.000911
2018-08-11 08:18:13,360:CRITICAL:EmoV2_step40: iteration: 111900: Loss: 0.575978, lr: 0.000905
2018-08-11 08:18:26,508:CRITICAL:EmoV2_step40: iteration: 112000: Loss: 0.282332, lr: 0.000900
2018-08-11 08:18:31,082:CRITICAL:EmoV2_step40: validate. Iteration: 112000: Accuracy (valence, arousal): 33.186% 19.027%
2018-08-11 08:18:31,083:CRITICAL:EmoV2_step40: validate. Iteration: 112000: Loss: nan
2018-08-11 08:18:44,234:CRITICAL:EmoV2_step40: iteration: 112100: Loss: 1.077244, lr: 0.000895
2018-08-11 08:18:57,398:CRITICAL:EmoV2_step40: iteration: 112200: Loss: 0.458032, lr: 0.000890
2018-08-11 08:19:10,547:CRITICAL:EmoV2_step40: iteration: 112300: Loss: 0.572266, lr: 0.000884
2018-08-11 08:19:23,700:CRITICAL:EmoV2_step40: iteration: 112400: Loss: 0.728029, lr: 0.000879
2018-08-11 08:19:36,857:CRITICAL:EmoV2_step40: iteration: 112500: Loss: 0.939571, lr: 0.000874
2018-08-11 08:19:50,004:CRITICAL:EmoV2_step40: iteration: 112600: Loss: 0.439850, lr: 0.000869
2018-08-11 08:20:03,148:CRITICAL:EmoV2_step40: iteration: 112700: Loss: 0.514495, lr: 0.000863
2018-08-11 08:20:16,297:CRITICAL:EmoV2_step40: iteration: 112800: Loss: 0.488741, lr: 0.000858
2018-08-11 08:20:29,461:CRITICAL:EmoV2_step40: iteration: 112900: Loss: 0.556127, lr: 0.000853
2018-08-11 08:20:42,609:CRITICAL:EmoV2_step40: iteration: 113000: Loss: 0.479537, lr: 0.000848
2018-08-11 08:20:47,210:CRITICAL:EmoV2_step40: validate. Iteration: 113000: Accuracy (valence, arousal): 32.743% 20.354%
2018-08-11 08:20:47,210:CRITICAL:EmoV2_step40: validate. Iteration: 113000: Loss: nan
2018-08-11 08:21:04,705:CRITICAL:EmoV2_step40: iteration: 113100: Loss: 0.447998, lr: 0.000842
2018-08-11 08:21:17,839:CRITICAL:EmoV2_step40: iteration: 113200: Loss: 0.288509, lr: 0.000837
2018-08-11 08:21:30,977:CRITICAL:EmoV2_step40: iteration: 113300: Loss: 0.324991, lr: 0.000832
2018-08-11 08:21:44,110:CRITICAL:EmoV2_step40: iteration: 113400: Loss: 0.594194, lr: 0.000827
2018-08-11 08:21:57,252:CRITICAL:EmoV2_step40: iteration: 113500: Loss: 0.458541, lr: 0.000821
2018-08-11 08:22:10,397:CRITICAL:EmoV2_step40: iteration: 113600: Loss: 0.300569, lr: 0.000816
2018-08-11 08:22:23,553:CRITICAL:EmoV2_step40: iteration: 113700: Loss: 0.483507, lr: 0.000811
2018-08-11 08:22:36,708:CRITICAL:EmoV2_step40: iteration: 113800: Loss: 0.488243, lr: 0.000806
2018-08-11 08:22:49,864:CRITICAL:EmoV2_step40: iteration: 113900: Loss: 0.428243, lr: 0.000800
2018-08-11 08:23:03,023:CRITICAL:EmoV2_step40: iteration: 114000: Loss: 0.532879, lr: 0.000795
2018-08-11 08:23:07,537:CRITICAL:EmoV2_step40: validate. Iteration: 114000: Accuracy (valence, arousal): 31.416% 17.257%
2018-08-11 08:23:07,537:CRITICAL:EmoV2_step40: validate. Iteration: 114000: Loss: nan
2018-08-11 08:23:25,328:CRITICAL:EmoV2_step40: iteration: 114100: Loss: 0.537919, lr: 0.000790
2018-08-11 08:23:38,471:CRITICAL:EmoV2_step40: iteration: 114200: Loss: 0.821902, lr: 0.000785
2018-08-11 08:23:51,613:CRITICAL:EmoV2_step40: iteration: 114300: Loss: 0.822723, lr: 0.000779
2018-08-11 08:24:04,770:CRITICAL:EmoV2_step40: iteration: 114400: Loss: 0.893143, lr: 0.000774
2018-08-11 08:24:17,913:CRITICAL:EmoV2_step40: iteration: 114500: Loss: 0.633938, lr: 0.000769
2018-08-11 08:24:31,082:CRITICAL:EmoV2_step40: iteration: 114600: Loss: 0.636395, lr: 0.000764
2018-08-11 08:24:48,665:CRITICAL:EmoV2_step40: iteration: 114700: Loss: 0.383814, lr: 0.000758
2018-08-11 08:25:01,815:CRITICAL:EmoV2_step40: iteration: 114800: Loss: 0.318949, lr: 0.000753
2018-08-11 08:25:14,959:CRITICAL:EmoV2_step40: iteration: 114900: Loss: 0.478575, lr: 0.000748
2018-08-11 08:25:28,111:CRITICAL:EmoV2_step40: iteration: 115000: Loss: 0.401626, lr: 0.000743
2018-08-11 08:25:32,721:CRITICAL:EmoV2_step40: validate. Iteration: 115000: Accuracy (valence, arousal): 30.973% 16.814%
2018-08-11 08:25:32,722:CRITICAL:EmoV2_step40: validate. Iteration: 115000: Loss: nan
2018-08-11 08:25:45,880:CRITICAL:EmoV2_step40: iteration: 115100: Loss: 0.293317, lr: 0.000737
2018-08-11 08:25:59,025:CRITICAL:EmoV2_step40: iteration: 115200: Loss: 0.261573, lr: 0.000732
2018-08-11 08:26:12,184:CRITICAL:EmoV2_step40: iteration: 115300: Loss: 0.617511, lr: 0.000727
2018-08-11 08:26:25,334:CRITICAL:EmoV2_step40: iteration: 115400: Loss: 0.317257, lr: 0.000722
2018-08-11 08:26:38,493:CRITICAL:EmoV2_step40: iteration: 115500: Loss: 0.472013, lr: 0.000716
2018-08-11 08:26:51,634:CRITICAL:EmoV2_step40: iteration: 115600: Loss: 0.497463, lr: 0.000711
2018-08-11 08:27:04,774:CRITICAL:EmoV2_step40: iteration: 115700: Loss: 0.674281, lr: 0.000706
2018-08-11 08:27:17,922:CRITICAL:EmoV2_step40: iteration: 115800: Loss: 0.421034, lr: 0.000701
2018-08-11 08:27:31,061:CRITICAL:EmoV2_step40: iteration: 115900: Loss: 0.845543, lr: 0.000695
2018-08-11 08:27:44,205:CRITICAL:EmoV2_step40: iteration: 116000: Loss: 0.423335, lr: 0.000690
2018-08-11 08:27:48,673:CRITICAL:EmoV2_step40: validate. Iteration: 116000: Accuracy (valence, arousal): 34.071% 17.257%
2018-08-11 08:27:48,674:CRITICAL:EmoV2_step40: validate. Iteration: 116000: Loss: nan
2018-08-11 08:28:01,820:CRITICAL:EmoV2_step40: iteration: 116100: Loss: 0.576860, lr: 0.000685
2018-08-11 08:28:14,963:CRITICAL:EmoV2_step40: iteration: 116200: Loss: 0.645335, lr: 0.000680
2018-08-11 08:28:28,109:CRITICAL:EmoV2_step40: iteration: 116300: Loss: 0.345094, lr: 0.000674
2018-08-11 08:28:45,694:CRITICAL:EmoV2_step40: iteration: 116400: Loss: 0.727573, lr: 0.000669
2018-08-11 08:28:58,844:CRITICAL:EmoV2_step40: iteration: 116500: Loss: 0.574459, lr: 0.000664
2018-08-11 08:29:11,986:CRITICAL:EmoV2_step40: iteration: 116600: Loss: 1.116644, lr: 0.000659
2018-08-11 08:29:29,766:CRITICAL:EmoV2_step40: iteration: 116700: Loss: 0.661713, lr: 0.000653
2018-08-11 08:29:42,918:CRITICAL:EmoV2_step40: iteration: 116800: Loss: 0.495927, lr: 0.000648
2018-08-11 08:29:56,078:CRITICAL:EmoV2_step40: iteration: 116900: Loss: 0.520336, lr: 0.000643
2018-08-11 08:30:09,232:CRITICAL:EmoV2_step40: iteration: 117000: Loss: 0.463105, lr: 0.000638
2018-08-11 08:30:13,824:CRITICAL:EmoV2_step40: validate. Iteration: 117000: Accuracy (valence, arousal): 32.743% 15.044%
2018-08-11 08:30:13,824:CRITICAL:EmoV2_step40: validate. Iteration: 117000: Loss: nan
2018-08-11 08:30:26,972:CRITICAL:EmoV2_step40: iteration: 117100: Loss: 0.403354, lr: 0.000632
2018-08-11 08:30:40,112:CRITICAL:EmoV2_step40: iteration: 117200: Loss: 0.846113, lr: 0.000627
2018-08-11 08:30:53,268:CRITICAL:EmoV2_step40: iteration: 117300: Loss: 0.624312, lr: 0.000622
2018-08-11 08:31:06,418:CRITICAL:EmoV2_step40: iteration: 117400: Loss: 0.581252, lr: 0.000617
2018-08-11 08:31:19,563:CRITICAL:EmoV2_step40: iteration: 117500: Loss: 0.344596, lr: 0.000617
2018-08-11 08:31:32,740:CRITICAL:EmoV2_step40: iteration: 117600: Loss: 0.630189, lr: 0.000621
2018-08-11 08:31:45,893:CRITICAL:EmoV2_step40: iteration: 117700: Loss: 0.566550, lr: 0.000626
2018-08-11 08:31:59,047:CRITICAL:EmoV2_step40: iteration: 117800: Loss: 0.311558, lr: 0.000631
2018-08-11 08:32:12,205:CRITICAL:EmoV2_step40: iteration: 117900: Loss: 0.482655, lr: 0.000636
2018-08-11 08:32:29,749:CRITICAL:EmoV2_step40: iteration: 118000: Loss: 0.873928, lr: 0.000640
2018-08-11 08:32:34,366:CRITICAL:EmoV2_step40: validate. Iteration: 118000: Accuracy (valence, arousal): 32.743% 17.699%
2018-08-11 08:32:34,366:CRITICAL:EmoV2_step40: validate. Iteration: 118000: Loss: nan
2018-08-11 08:32:47,515:CRITICAL:EmoV2_step40: iteration: 118100: Loss: 0.663186, lr: 0.000645
2018-08-11 08:33:00,656:CRITICAL:EmoV2_step40: iteration: 118200: Loss: 0.391521, lr: 0.000650
2018-08-11 08:33:13,808:CRITICAL:EmoV2_step40: iteration: 118300: Loss: 0.463698, lr: 0.000655
2018-08-11 08:33:26,964:CRITICAL:EmoV2_step40: iteration: 118400: Loss: 0.685738, lr: 0.000660
2018-08-11 08:33:40,122:CRITICAL:EmoV2_step40: iteration: 118500: Loss: 0.852912, lr: 0.000664
2018-08-11 08:33:53,280:CRITICAL:EmoV2_step40: iteration: 118600: Loss: 0.593613, lr: 0.000669
2018-08-11 08:34:06,440:CRITICAL:EmoV2_step40: iteration: 118700: Loss: 0.738915, lr: 0.000674
2018-08-11 08:34:19,584:CRITICAL:EmoV2_step40: iteration: 118800: Loss: 0.396705, lr: 0.000679
2018-08-11 08:34:32,741:CRITICAL:EmoV2_step40: iteration: 118900: Loss: 0.401343, lr: 0.000684
2018-08-11 08:34:45,886:CRITICAL:EmoV2_step40: iteration: 119000: Loss: 0.499478, lr: 0.000688
2018-08-11 08:34:50,337:CRITICAL:EmoV2_step40: validate. Iteration: 119000: Accuracy (valence, arousal): 29.646% 19.027%
2018-08-11 08:34:50,337:CRITICAL:EmoV2_step40: validate. Iteration: 119000: Loss: nan
2018-08-11 08:35:03,478:CRITICAL:EmoV2_step40: iteration: 119100: Loss: 0.475636, lr: 0.000693
2018-08-11 08:35:21,276:CRITICAL:EmoV2_step40: iteration: 119200: Loss: 0.374826, lr: 0.000698
2018-08-11 08:35:34,412:CRITICAL:EmoV2_step40: iteration: 119300: Loss: 0.572685, lr: 0.000703
2018-08-11 08:35:47,552:CRITICAL:EmoV2_step40: iteration: 119400: Loss: 0.335233, lr: 0.000708
2018-08-11 08:36:00,701:CRITICAL:EmoV2_step40: iteration: 119500: Loss: 0.437491, lr: 0.000712
2018-08-11 08:36:13,854:CRITICAL:EmoV2_step40: iteration: 119600: Loss: 0.282960, lr: 0.000717
2018-08-11 08:36:31,371:CRITICAL:EmoV2_step40: iteration: 119700: Loss: 0.340130, lr: 0.000722
2018-08-11 08:36:44,518:CRITICAL:EmoV2_step40: iteration: 119800: Loss: 0.712969, lr: 0.000727
2018-08-11 08:36:57,671:CRITICAL:EmoV2_step40: iteration: 119900: Loss: 0.366243, lr: 0.000732
2018-08-11 08:37:52,097:CRITICAL:EmoV2_step40: iteration: 120000: Loss: 0.626857, lr: 0.000736
2018-08-11 08:37:56,738:CRITICAL:EmoV2_step40: validate. Iteration: 120000: Accuracy (valence, arousal): 30.973% 15.044%
2018-08-11 08:37:56,739:CRITICAL:EmoV2_step40: validate. Iteration: 120000: Loss: nan
2018-08-11 08:38:09,803:CRITICAL:EmoV2_step40: iteration: 120100: Loss: 0.464814, lr: 0.000741
2018-08-11 08:38:22,875:CRITICAL:EmoV2_step40: iteration: 120200: Loss: 0.563644, lr: 0.000746
2018-08-11 08:38:35,937:CRITICAL:EmoV2_step40: iteration: 120300: Loss: 0.663349, lr: 0.000751
2018-08-11 08:38:49,017:CRITICAL:EmoV2_step40: iteration: 120400: Loss: 0.652145, lr: 0.000756
2018-08-11 08:39:02,105:CRITICAL:EmoV2_step40: iteration: 120500: Loss: 0.749781, lr: 0.000760
2018-08-11 08:39:15,197:CRITICAL:EmoV2_step40: iteration: 120600: Loss: 0.312793, lr: 0.000765
2018-08-11 08:39:28,302:CRITICAL:EmoV2_step40: iteration: 120700: Loss: 0.617634, lr: 0.000770
2018-08-11 08:39:41,403:CRITICAL:EmoV2_step40: iteration: 120800: Loss: 1.018956, lr: 0.000775
2018-08-11 08:39:54,549:CRITICAL:EmoV2_step40: iteration: 120900: Loss: 0.346377, lr: 0.000779
2018-08-11 08:40:07,685:CRITICAL:EmoV2_step40: iteration: 121000: Loss: 0.736486, lr: 0.000784
2018-08-11 08:40:12,323:CRITICAL:EmoV2_step40: validate. Iteration: 121000: Accuracy (valence, arousal): 31.858% 16.814%
2018-08-11 08:40:12,323:CRITICAL:EmoV2_step40: validate. Iteration: 121000: Loss: nan
2018-08-11 08:40:25,469:CRITICAL:EmoV2_step40: iteration: 121100: Loss: 0.291235, lr: 0.000789
2018-08-11 08:40:38,620:CRITICAL:EmoV2_step40: iteration: 121200: Loss: 0.409629, lr: 0.000794
2018-08-11 08:40:56,274:CRITICAL:EmoV2_step40: iteration: 121300: Loss: 0.345777, lr: 0.000799
2018-08-11 08:41:09,420:CRITICAL:EmoV2_step40: iteration: 121400: Loss: 0.474939, lr: 0.000803
2018-08-11 08:41:22,577:CRITICAL:EmoV2_step40: iteration: 121500: Loss: 0.528117, lr: 0.000808
2018-08-11 08:41:35,721:CRITICAL:EmoV2_step40: iteration: 121600: Loss: 0.622514, lr: 0.000813
2018-08-11 08:41:53,473:CRITICAL:EmoV2_step40: iteration: 121700: Loss: 0.428673, lr: 0.000818
2018-08-11 08:42:06,626:CRITICAL:EmoV2_step40: iteration: 121800: Loss: 0.562101, lr: 0.000823
2018-08-11 08:42:19,781:CRITICAL:EmoV2_step40: iteration: 121900: Loss: 0.313543, lr: 0.000827
2018-08-11 08:42:32,930:CRITICAL:EmoV2_step40: iteration: 122000: Loss: 0.604006, lr: 0.000832
2018-08-11 08:42:37,471:CRITICAL:EmoV2_step40: validate. Iteration: 122000: Accuracy (valence, arousal): 30.088% 19.912%
2018-08-11 08:42:37,472:CRITICAL:EmoV2_step40: validate. Iteration: 122000: Loss: nan
2018-08-11 08:42:50,615:CRITICAL:EmoV2_step40: iteration: 122100: Loss: 0.540015, lr: 0.000837
2018-08-11 08:43:03,769:CRITICAL:EmoV2_step40: iteration: 122200: Loss: 0.722665, lr: 0.000842
2018-08-11 08:43:16,911:CRITICAL:EmoV2_step40: iteration: 122300: Loss: 0.803539, lr: 0.000847
2018-08-11 08:43:30,081:CRITICAL:EmoV2_step40: iteration: 122400: Loss: 0.403677, lr: 0.000851
2018-08-11 08:43:43,233:CRITICAL:EmoV2_step40: iteration: 122500: Loss: 0.574960, lr: 0.000856
2018-08-11 08:43:56,390:CRITICAL:EmoV2_step40: iteration: 122600: Loss: 0.267235, lr: 0.000861
2018-08-11 08:44:09,536:CRITICAL:EmoV2_step40: iteration: 122700: Loss: 0.310346, lr: 0.000866
2018-08-11 08:44:22,693:CRITICAL:EmoV2_step40: iteration: 122800: Loss: 0.448254, lr: 0.000871
2018-08-11 08:44:35,851:CRITICAL:EmoV2_step40: iteration: 122900: Loss: 0.545155, lr: 0.000875
2018-08-11 08:44:53,382:CRITICAL:EmoV2_step40: iteration: 123000: Loss: 0.622370, lr: 0.000880
2018-08-11 08:44:58,026:CRITICAL:EmoV2_step40: validate. Iteration: 123000: Accuracy (valence, arousal): 31.858% 24.336%
2018-08-11 08:44:58,027:CRITICAL:EmoV2_step40: validate. Iteration: 123000: Loss: nan
2018-08-11 08:45:11,178:CRITICAL:EmoV2_step40: iteration: 123100: Loss: 0.410010, lr: 0.000885
2018-08-11 08:45:24,327:CRITICAL:EmoV2_step40: iteration: 123200: Loss: 0.518340, lr: 0.000890
2018-08-11 08:45:37,486:CRITICAL:EmoV2_step40: iteration: 123300: Loss: 0.933997, lr: 0.000895
2018-08-11 08:45:50,630:CRITICAL:EmoV2_step40: iteration: 123400: Loss: 0.605931, lr: 0.000899
2018-08-11 08:46:03,764:CRITICAL:EmoV2_step40: iteration: 123500: Loss: 0.787423, lr: 0.000904
2018-08-11 08:46:16,900:CRITICAL:EmoV2_step40: iteration: 123600: Loss: 0.310068, lr: 0.000909
2018-08-11 08:46:30,047:CRITICAL:EmoV2_step40: iteration: 123700: Loss: 0.376347, lr: 0.000914
2018-08-11 08:46:43,210:CRITICAL:EmoV2_step40: iteration: 123800: Loss: 0.330167, lr: 0.000919
2018-08-11 08:46:56,385:CRITICAL:EmoV2_step40: iteration: 123900: Loss: 0.496570, lr: 0.000923
2018-08-11 08:47:09,555:CRITICAL:EmoV2_step40: iteration: 124000: Loss: 0.652197, lr: 0.000928
2018-08-11 08:47:14,134:CRITICAL:EmoV2_step40: validate. Iteration: 124000: Accuracy (valence, arousal): 34.071% 20.796%
2018-08-11 08:47:14,134:CRITICAL:EmoV2_step40: validate. Iteration: 124000: Loss: nan
2018-08-11 08:47:27,289:CRITICAL:EmoV2_step40: iteration: 124100: Loss: 0.419397, lr: 0.000933
2018-08-11 08:47:45,095:CRITICAL:EmoV2_step40: iteration: 124200: Loss: 0.787601, lr: 0.000938
2018-08-11 08:47:58,242:CRITICAL:EmoV2_step40: iteration: 124300: Loss: 0.510454, lr: 0.000942
2018-08-11 08:48:11,381:CRITICAL:EmoV2_step40: iteration: 124400: Loss: 0.687102, lr: 0.000947
2018-08-11 08:48:24,536:CRITICAL:EmoV2_step40: iteration: 124500: Loss: 0.346514, lr: 0.000952
2018-08-11 08:48:42,136:CRITICAL:EmoV2_step40: iteration: 124600: Loss: 0.471855, lr: 0.000957
2018-08-11 08:48:55,289:CRITICAL:EmoV2_step40: iteration: 124700: Loss: 0.613146, lr: 0.000962
2018-08-11 08:49:08,444:CRITICAL:EmoV2_step40: iteration: 124800: Loss: 0.557484, lr: 0.000966
2018-08-11 08:49:21,606:CRITICAL:EmoV2_step40: iteration: 124900: Loss: 0.541995, lr: 0.000971
2018-08-11 08:49:34,761:CRITICAL:EmoV2_step40: iteration: 125000: Loss: 0.413850, lr: 0.000976
2018-08-11 08:49:39,230:CRITICAL:EmoV2_step40: validate. Iteration: 125000: Accuracy (valence, arousal): 29.646% 19.469%
2018-08-11 08:49:39,231:CRITICAL:EmoV2_step40: validate. Iteration: 125000: Loss: nan
2018-08-11 08:49:52,377:CRITICAL:EmoV2_step40: iteration: 125100: Loss: 0.586507, lr: 0.000981
2018-08-11 08:50:05,526:CRITICAL:EmoV2_step40: iteration: 125200: Loss: 0.423083, lr: 0.000986
2018-08-11 08:50:18,670:CRITICAL:EmoV2_step40: iteration: 125300: Loss: 0.370402, lr: 0.000990
2018-08-11 08:50:31,830:CRITICAL:EmoV2_step40: iteration: 125400: Loss: 1.318201, lr: 0.000995
2018-08-11 08:50:44,987:CRITICAL:EmoV2_step40: iteration: 125500: Loss: 0.350591, lr: 0.000994
2018-08-11 08:50:58,143:CRITICAL:EmoV2_step40: iteration: 125600: Loss: 0.749741, lr: 0.000993
2018-08-11 08:51:11,285:CRITICAL:EmoV2_step40: iteration: 125700: Loss: 0.671457, lr: 0.000992
2018-08-11 08:51:24,439:CRITICAL:EmoV2_step40: iteration: 125800: Loss: 0.449715, lr: 0.000990
2018-08-11 08:51:37,597:CRITICAL:EmoV2_step40: iteration: 125900: Loss: 0.731518, lr: 0.000989
2018-08-11 08:51:50,742:CRITICAL:EmoV2_step40: iteration: 126000: Loss: 0.456664, lr: 0.000988
2018-08-11 08:51:55,385:CRITICAL:EmoV2_step40: validate. Iteration: 126000: Accuracy (valence, arousal): 30.973% 19.912%
2018-08-11 08:51:55,385:CRITICAL:EmoV2_step40: validate. Iteration: 126000: Loss: nan
2018-08-11 08:52:08,545:CRITICAL:EmoV2_step40: iteration: 126100: Loss: 1.166207, lr: 0.000987
2018-08-11 08:52:21,693:CRITICAL:EmoV2_step40: iteration: 126200: Loss: 0.280196, lr: 0.000986
2018-08-11 08:52:39,199:CRITICAL:EmoV2_step40: iteration: 126300: Loss: 0.310568, lr: 0.000984
2018-08-11 08:52:52,342:CRITICAL:EmoV2_step40: iteration: 126400: Loss: 0.321534, lr: 0.000983
2018-08-11 08:53:05,491:CRITICAL:EmoV2_step40: iteration: 126500: Loss: 0.468100, lr: 0.000982
2018-08-11 08:53:18,637:CRITICAL:EmoV2_step40: iteration: 126600: Loss: 0.483422, lr: 0.000981
2018-08-11 08:53:36,406:CRITICAL:EmoV2_step40: iteration: 126700: Loss: 0.333362, lr: 0.000980
2018-08-11 08:53:49,535:CRITICAL:EmoV2_step40: iteration: 126800: Loss: 0.813066, lr: 0.000978
2018-08-11 08:54:02,692:CRITICAL:EmoV2_step40: iteration: 126900: Loss: 0.627787, lr: 0.000977
2018-08-11 08:54:15,830:CRITICAL:EmoV2_step40: iteration: 127000: Loss: 0.463379, lr: 0.000976
2018-08-11 08:54:20,620:CRITICAL:EmoV2_step40: validate. Iteration: 127000: Accuracy (valence, arousal): 33.628% 19.469%
2018-08-11 08:54:20,621:CRITICAL:EmoV2_step40: validate. Iteration: 127000: Loss: nan
2018-08-11 08:54:33,793:CRITICAL:EmoV2_step40: iteration: 127100: Loss: 0.352518, lr: 0.000975
2018-08-11 08:54:46,937:CRITICAL:EmoV2_step40: iteration: 127200: Loss: 0.297488, lr: 0.000974
2018-08-11 08:55:00,076:CRITICAL:EmoV2_step40: iteration: 127300: Loss: 0.990780, lr: 0.000972
2018-08-11 08:55:13,217:CRITICAL:EmoV2_step40: iteration: 127400: Loss: 0.361679, lr: 0.000971
2018-08-11 08:55:26,366:CRITICAL:EmoV2_step40: iteration: 127500: Loss: 0.427080, lr: 0.000970
2018-08-11 08:55:39,521:CRITICAL:EmoV2_step40: iteration: 127600: Loss: 0.292612, lr: 0.000969
2018-08-11 08:55:52,680:CRITICAL:EmoV2_step40: iteration: 127700: Loss: 0.770035, lr: 0.000968
2018-08-11 08:56:05,822:CRITICAL:EmoV2_step40: iteration: 127800: Loss: 0.464703, lr: 0.000966
2018-08-11 08:56:23,365:CRITICAL:EmoV2_step40: iteration: 127900: Loss: 0.315639, lr: 0.000965
2018-08-11 08:56:36,506:CRITICAL:EmoV2_step40: iteration: 128000: Loss: 0.444433, lr: 0.000964
2018-08-11 08:56:41,043:CRITICAL:EmoV2_step40: validate. Iteration: 128000: Accuracy (valence, arousal): 32.743% 15.487%
2018-08-11 08:56:41,043:CRITICAL:EmoV2_step40: validate. Iteration: 128000: Loss: nan
2018-08-11 08:56:54,211:CRITICAL:EmoV2_step40: iteration: 128100: Loss: 0.563613, lr: 0.000963
2018-08-11 08:57:07,362:CRITICAL:EmoV2_step40: iteration: 128200: Loss: 0.592474, lr: 0.000962
2018-08-11 08:57:20,512:CRITICAL:EmoV2_step40: iteration: 128300: Loss: 0.777096, lr: 0.000960
2018-08-11 08:57:33,659:CRITICAL:EmoV2_step40: iteration: 128400: Loss: 0.579783, lr: 0.000959
2018-08-11 08:57:46,801:CRITICAL:EmoV2_step40: iteration: 128500: Loss: 0.382112, lr: 0.000958
2018-08-11 08:57:59,960:CRITICAL:EmoV2_step40: iteration: 128600: Loss: 0.606606, lr: 0.000957
2018-08-11 08:58:13,121:CRITICAL:EmoV2_step40: iteration: 128700: Loss: 0.446115, lr: 0.000956
2018-08-11 08:58:26,271:CRITICAL:EmoV2_step40: iteration: 128800: Loss: 0.277491, lr: 0.000954
2018-08-11 08:58:39,422:CRITICAL:EmoV2_step40: iteration: 128900: Loss: 0.444777, lr: 0.000953
2018-08-11 08:58:52,568:CRITICAL:EmoV2_step40: iteration: 129000: Loss: 0.310802, lr: 0.000952
2018-08-11 08:58:57,171:CRITICAL:EmoV2_step40: validate. Iteration: 129000: Accuracy (valence, arousal): 27.434% 19.027%
2018-08-11 08:58:57,172:CRITICAL:EmoV2_step40: validate. Iteration: 129000: Loss: nan
2018-08-11 08:59:10,329:CRITICAL:EmoV2_step40: iteration: 129100: Loss: 0.607258, lr: 0.000951
2018-08-11 08:59:28,108:CRITICAL:EmoV2_step40: iteration: 129200: Loss: 0.355111, lr: 0.000950
2018-08-11 08:59:41,258:CRITICAL:EmoV2_step40: iteration: 129300: Loss: 0.530752, lr: 0.000948
2018-08-11 08:59:54,397:CRITICAL:EmoV2_step40: iteration: 129400: Loss: 0.847797, lr: 0.000947
2018-08-11 09:00:07,559:CRITICAL:EmoV2_step40: iteration: 129500: Loss: 0.402302, lr: 0.000946
2018-08-11 09:00:25,050:CRITICAL:EmoV2_step40: iteration: 129600: Loss: 1.041479, lr: 0.000945
2018-08-11 09:00:38,187:CRITICAL:EmoV2_step40: iteration: 129700: Loss: 0.583014, lr: 0.000944
2018-08-11 09:00:51,340:CRITICAL:EmoV2_step40: iteration: 129800: Loss: 0.292789, lr: 0.000942
2018-08-11 09:01:04,478:CRITICAL:EmoV2_step40: iteration: 129900: Loss: 0.494031, lr: 0.000941
2018-08-11 09:01:59,033:CRITICAL:EmoV2_step40: iteration: 130000: Loss: 0.825318, lr: 0.000940
2018-08-11 09:02:04,407:CRITICAL:EmoV2_step40: validate. Iteration: 130000: Accuracy (valence, arousal): 28.761% 17.257%
2018-08-11 09:02:04,408:CRITICAL:EmoV2_step40: validate. Iteration: 130000: Loss: nan
2018-08-11 09:02:17,475:CRITICAL:EmoV2_step40: iteration: 130100: Loss: 0.384709, lr: 0.000939
2018-08-11 09:02:30,538:CRITICAL:EmoV2_step40: iteration: 130200: Loss: 0.286630, lr: 0.000938
2018-08-11 09:02:43,605:CRITICAL:EmoV2_step40: iteration: 130300: Loss: 0.258547, lr: 0.000936
2018-08-11 09:02:56,706:CRITICAL:EmoV2_step40: iteration: 130400: Loss: 0.784665, lr: 0.000935
2018-08-11 09:03:09,818:CRITICAL:EmoV2_step40: iteration: 130500: Loss: 0.304579, lr: 0.000934
2018-08-11 09:03:22,922:CRITICAL:EmoV2_step40: iteration: 130600: Loss: 0.399930, lr: 0.000933
2018-08-11 09:03:36,042:CRITICAL:EmoV2_step40: iteration: 130700: Loss: 0.715982, lr: 0.000932
2018-08-11 09:03:49,153:CRITICAL:EmoV2_step40: iteration: 130800: Loss: 0.367943, lr: 0.000930
2018-08-11 09:04:02,289:CRITICAL:EmoV2_step40: iteration: 130900: Loss: 0.561322, lr: 0.000929
2018-08-11 09:04:15,443:CRITICAL:EmoV2_step40: iteration: 131000: Loss: 0.792781, lr: 0.000928
2018-08-11 09:04:24,829:CRITICAL:EmoV2_step40: validate. Iteration: 131000: Accuracy (valence, arousal): 30.973% 18.584%
2018-08-11 09:04:24,830:CRITICAL:EmoV2_step40: validate. Iteration: 131000: Loss: nan
2018-08-11 09:04:37,995:CRITICAL:EmoV2_step40: iteration: 131100: Loss: 0.271577, lr: 0.000927
2018-08-11 09:04:55,505:CRITICAL:EmoV2_step40: iteration: 131200: Loss: 0.630950, lr: 0.000926
2018-08-11 09:05:08,648:CRITICAL:EmoV2_step40: iteration: 131300: Loss: 0.436480, lr: 0.000924
2018-08-11 09:05:21,790:CRITICAL:EmoV2_step40: iteration: 131400: Loss: 0.273101, lr: 0.000923
2018-08-11 09:05:34,927:CRITICAL:EmoV2_step40: iteration: 131500: Loss: 0.410306, lr: 0.000922
2018-08-11 09:05:48,070:CRITICAL:EmoV2_step40: iteration: 131600: Loss: 0.886785, lr: 0.000921
2018-08-11 09:06:05,835:CRITICAL:EmoV2_step40: iteration: 131700: Loss: 0.261976, lr: 0.000920
2018-08-11 09:06:18,986:CRITICAL:EmoV2_step40: iteration: 131800: Loss: 0.636948, lr: 0.000919
2018-08-11 09:06:32,144:CRITICAL:EmoV2_step40: iteration: 131900: Loss: 0.934645, lr: 0.000917
2018-08-11 09:06:45,291:CRITICAL:EmoV2_step40: iteration: 132000: Loss: 0.651535, lr: 0.000916
2018-08-11 09:06:49,967:CRITICAL:EmoV2_step40: validate. Iteration: 132000: Accuracy (valence, arousal): 28.319% 19.027%
2018-08-11 09:06:49,967:CRITICAL:EmoV2_step40: validate. Iteration: 132000: Loss: nan
2018-08-11 09:07:03,118:CRITICAL:EmoV2_step40: iteration: 132100: Loss: 0.368761, lr: 0.000915
2018-08-11 09:07:16,264:CRITICAL:EmoV2_step40: iteration: 132200: Loss: 0.324058, lr: 0.000914
2018-08-11 09:07:29,422:CRITICAL:EmoV2_step40: iteration: 132300: Loss: 0.930322, lr: 0.000913
2018-08-11 09:07:42,582:CRITICAL:EmoV2_step40: iteration: 132400: Loss: 0.469058, lr: 0.000911
2018-08-11 09:07:55,737:CRITICAL:EmoV2_step40: iteration: 132500: Loss: 0.623967, lr: 0.000910
2018-08-11 09:08:08,883:CRITICAL:EmoV2_step40: iteration: 132600: Loss: 0.414325, lr: 0.000909
2018-08-11 09:08:22,031:CRITICAL:EmoV2_step40: iteration: 132700: Loss: 0.304664, lr: 0.000908
2018-08-11 09:08:35,179:CRITICAL:EmoV2_step40: iteration: 132800: Loss: 0.333731, lr: 0.000907
2018-08-11 09:08:52,715:CRITICAL:EmoV2_step40: iteration: 132900: Loss: 0.469427, lr: 0.000905
2018-08-11 09:09:05,878:CRITICAL:EmoV2_step40: iteration: 133000: Loss: 0.821766, lr: 0.000904
2018-08-11 09:09:10,385:CRITICAL:EmoV2_step40: validate. Iteration: 133000: Accuracy (valence, arousal): 32.301% 21.239%
2018-08-11 09:09:10,386:CRITICAL:EmoV2_step40: validate. Iteration: 133000: Loss: nan
2018-08-11 09:09:23,539:CRITICAL:EmoV2_step40: iteration: 133100: Loss: 0.512118, lr: 0.000903
2018-08-11 09:09:36,683:CRITICAL:EmoV2_step40: iteration: 133200: Loss: 0.252923, lr: 0.000902
2018-08-11 09:09:49,838:CRITICAL:EmoV2_step40: iteration: 133300: Loss: 0.337722, lr: 0.000901
2018-08-11 09:10:02,990:CRITICAL:EmoV2_step40: iteration: 133400: Loss: 0.841368, lr: 0.000899
2018-08-11 09:10:16,129:CRITICAL:EmoV2_step40: iteration: 133500: Loss: 0.381688, lr: 0.000898
2018-08-11 09:10:29,274:CRITICAL:EmoV2_step40: iteration: 133600: Loss: 0.745769, lr: 0.000897
2018-08-11 09:10:42,428:CRITICAL:EmoV2_step40: iteration: 133700: Loss: 0.686097, lr: 0.000896
2018-08-11 09:10:55,586:CRITICAL:EmoV2_step40: iteration: 133800: Loss: 0.361957, lr: 0.000895
2018-08-11 09:11:08,729:CRITICAL:EmoV2_step40: iteration: 133900: Loss: 0.767941, lr: 0.000893
2018-08-11 09:11:21,871:CRITICAL:EmoV2_step40: iteration: 134000: Loss: 0.379976, lr: 0.000892
2018-08-11 09:11:26,305:CRITICAL:EmoV2_step40: validate. Iteration: 134000: Accuracy (valence, arousal): 33.186% 20.796%
2018-08-11 09:11:26,306:CRITICAL:EmoV2_step40: validate. Iteration: 134000: Loss: nan
2018-08-11 09:11:39,450:CRITICAL:EmoV2_step40: iteration: 134100: Loss: 0.464154, lr: 0.000891
2018-08-11 09:11:57,210:CRITICAL:EmoV2_step40: iteration: 134200: Loss: 0.666229, lr: 0.000890
2018-08-11 09:12:10,352:CRITICAL:EmoV2_step40: iteration: 134300: Loss: 0.426495, lr: 0.000889
2018-08-11 09:12:23,506:CRITICAL:EmoV2_step40: iteration: 134400: Loss: 0.296549, lr: 0.000887
2018-08-11 09:12:41,086:CRITICAL:EmoV2_step40: iteration: 134500: Loss: 0.682092, lr: 0.000886
2018-08-11 09:12:54,237:CRITICAL:EmoV2_step40: iteration: 134600: Loss: 0.557152, lr: 0.000885
2018-08-11 09:13:07,380:CRITICAL:EmoV2_step40: iteration: 134700: Loss: 0.700171, lr: 0.000884
2018-08-11 09:13:20,539:CRITICAL:EmoV2_step40: iteration: 134800: Loss: 0.428401, lr: 0.000883
2018-08-11 09:13:33,686:CRITICAL:EmoV2_step40: iteration: 134900: Loss: 0.482993, lr: 0.000881
2018-08-11 09:13:46,833:CRITICAL:EmoV2_step40: iteration: 135000: Loss: 0.411941, lr: 0.000880
2018-08-11 09:13:51,598:CRITICAL:EmoV2_step40: validate. Iteration: 135000: Accuracy (valence, arousal): 33.186% 19.912%
2018-08-11 09:13:51,599:CRITICAL:EmoV2_step40: validate. Iteration: 135000: Loss: nan
2018-08-11 09:14:04,750:CRITICAL:EmoV2_step40: iteration: 135100: Loss: 0.578262, lr: 0.000879
2018-08-11 09:14:17,889:CRITICAL:EmoV2_step40: iteration: 135200: Loss: 0.703183, lr: 0.000878
2018-08-11 09:14:31,032:CRITICAL:EmoV2_step40: iteration: 135300: Loss: 0.916636, lr: 0.000877
2018-08-11 09:14:44,187:CRITICAL:EmoV2_step40: iteration: 135400: Loss: 1.098218, lr: 0.000875
2018-08-11 09:14:57,329:CRITICAL:EmoV2_step40: iteration: 135500: Loss: 0.528168, lr: 0.000874
2018-08-11 09:15:10,495:CRITICAL:EmoV2_step40: iteration: 135600: Loss: 0.290307, lr: 0.000873
2018-08-11 09:15:23,653:CRITICAL:EmoV2_step40: iteration: 135700: Loss: 0.329941, lr: 0.000872
2018-08-11 09:15:36,801:CRITICAL:EmoV2_step40: iteration: 135800: Loss: 0.359223, lr: 0.000871
2018-08-11 09:15:49,941:CRITICAL:EmoV2_step40: iteration: 135900: Loss: 0.613891, lr: 0.000869
2018-08-11 09:16:03,090:CRITICAL:EmoV2_step40: iteration: 136000: Loss: 0.488914, lr: 0.000868
2018-08-11 09:16:07,772:CRITICAL:EmoV2_step40: validate. Iteration: 136000: Accuracy (valence, arousal): 34.513% 19.469%
2018-08-11 09:16:07,772:CRITICAL:EmoV2_step40: validate. Iteration: 136000: Loss: nan
2018-08-11 09:16:20,937:CRITICAL:EmoV2_step40: iteration: 136100: Loss: 0.853988, lr: 0.000867
2018-08-11 09:16:38,489:CRITICAL:EmoV2_step40: iteration: 136200: Loss: 0.522959, lr: 0.000866
2018-08-11 09:16:51,638:CRITICAL:EmoV2_step40: iteration: 136300: Loss: 0.321846, lr: 0.000865
2018-08-11 09:17:04,798:CRITICAL:EmoV2_step40: iteration: 136400: Loss: 0.782812, lr: 0.000863
2018-08-11 09:17:17,955:CRITICAL:EmoV2_step40: iteration: 136500: Loss: 0.528112, lr: 0.000862
2018-08-11 09:17:31,121:CRITICAL:EmoV2_step40: iteration: 136600: Loss: 0.581047, lr: 0.000861
2018-08-11 09:17:48,888:CRITICAL:EmoV2_step40: iteration: 136700: Loss: 0.740734, lr: 0.000860
2018-08-11 09:18:02,026:CRITICAL:EmoV2_step40: iteration: 136800: Loss: 0.331365, lr: 0.000859
2018-08-11 09:18:15,167:CRITICAL:EmoV2_step40: iteration: 136900: Loss: 0.376490, lr: 0.000857
2018-08-11 09:18:28,319:CRITICAL:EmoV2_step40: iteration: 137000: Loss: 0.394824, lr: 0.000856
2018-08-11 09:18:32,844:CRITICAL:EmoV2_step40: validate. Iteration: 137000: Accuracy (valence, arousal): 30.088% 20.354%
2018-08-11 09:18:32,845:CRITICAL:EmoV2_step40: validate. Iteration: 137000: Loss: nan
2018-08-11 09:18:45,995:CRITICAL:EmoV2_step40: iteration: 137100: Loss: 0.535052, lr: 0.000855
2018-08-11 09:18:59,146:CRITICAL:EmoV2_step40: iteration: 137200: Loss: 0.440079, lr: 0.000854
2018-08-11 09:19:12,285:CRITICAL:EmoV2_step40: iteration: 137300: Loss: 0.785077, lr: 0.000853
2018-08-11 09:19:25,423:CRITICAL:EmoV2_step40: iteration: 137400: Loss: 0.593092, lr: 0.000851
2018-08-11 09:19:38,567:CRITICAL:EmoV2_step40: iteration: 137500: Loss: 0.418961, lr: 0.000850
2018-08-11 09:19:51,718:CRITICAL:EmoV2_step40: iteration: 137600: Loss: 0.456579, lr: 0.000849
2018-08-11 09:20:04,880:CRITICAL:EmoV2_step40: iteration: 137700: Loss: 0.471781, lr: 0.000848
2018-08-11 09:20:22,416:CRITICAL:EmoV2_step40: iteration: 137800: Loss: 0.562335, lr: 0.000847
2018-08-11 09:20:35,551:CRITICAL:EmoV2_step40: iteration: 137900: Loss: 0.771031, lr: 0.000845
2018-08-11 09:20:48,697:CRITICAL:EmoV2_step40: iteration: 138000: Loss: 0.530479, lr: 0.000844
2018-08-11 09:20:53,507:CRITICAL:EmoV2_step40: validate. Iteration: 138000: Accuracy (valence, arousal): 31.858% 19.027%
2018-08-11 09:20:53,507:CRITICAL:EmoV2_step40: validate. Iteration: 138000: Loss: nan
2018-08-11 09:21:06,658:CRITICAL:EmoV2_step40: iteration: 138100: Loss: 0.530302, lr: 0.000843
2018-08-11 09:21:19,810:CRITICAL:EmoV2_step40: iteration: 138200: Loss: 0.860338, lr: 0.000842
2018-08-11 09:21:32,968:CRITICAL:EmoV2_step40: iteration: 138300: Loss: 0.553058, lr: 0.000841
2018-08-11 09:21:46,128:CRITICAL:EmoV2_step40: iteration: 138400: Loss: 0.717467, lr: 0.000839
2018-08-11 09:21:59,291:CRITICAL:EmoV2_step40: iteration: 138500: Loss: 0.351539, lr: 0.000838
2018-08-11 09:22:12,441:CRITICAL:EmoV2_step40: iteration: 138600: Loss: 0.513148, lr: 0.000837
2018-08-11 09:22:25,584:CRITICAL:EmoV2_step40: iteration: 138700: Loss: 0.392109, lr: 0.000836
2018-08-11 09:22:38,747:CRITICAL:EmoV2_step40: iteration: 138800: Loss: 0.370508, lr: 0.000835
2018-08-11 09:22:51,907:CRITICAL:EmoV2_step40: iteration: 138900: Loss: 0.694785, lr: 0.000833
2018-08-11 09:23:05,064:CRITICAL:EmoV2_step40: iteration: 139000: Loss: 0.276851, lr: 0.000832
2018-08-11 09:23:09,690:CRITICAL:EmoV2_step40: validate. Iteration: 139000: Accuracy (valence, arousal): 31.858% 16.814%
2018-08-11 09:23:09,690:CRITICAL:EmoV2_step40: validate. Iteration: 139000: Loss: nan
2018-08-11 09:23:22,848:CRITICAL:EmoV2_step40: iteration: 139100: Loss: 0.403375, lr: 0.000831
2018-08-11 09:23:40,596:CRITICAL:EmoV2_step40: iteration: 139200: Loss: 1.045722, lr: 0.000830
2018-08-11 09:23:53,733:CRITICAL:EmoV2_step40: iteration: 139300: Loss: 0.670627, lr: 0.000829
2018-08-11 09:24:06,873:CRITICAL:EmoV2_step40: iteration: 139400: Loss: 0.424948, lr: 0.000827
2018-08-11 09:24:24,449:CRITICAL:EmoV2_step40: iteration: 139500: Loss: 0.452348, lr: 0.000826
2018-08-11 09:24:37,595:CRITICAL:EmoV2_step40: iteration: 139600: Loss: 0.729993, lr: 0.000825
2018-08-11 09:24:50,738:CRITICAL:EmoV2_step40: iteration: 139700: Loss: 0.605097, lr: 0.000824
2018-08-11 09:25:03,889:CRITICAL:EmoV2_step40: iteration: 139800: Loss: 0.579212, lr: 0.000823
2018-08-11 09:25:17,025:CRITICAL:EmoV2_step40: iteration: 139900: Loss: 0.344164, lr: 0.000821
2018-08-11 09:26:11,447:CRITICAL:EmoV2_step40: iteration: 140000: Loss: 0.958063, lr: 0.000820
2018-08-11 09:26:16,146:CRITICAL:EmoV2_step40: validate. Iteration: 140000: Accuracy (valence, arousal): 31.416% 17.699%
2018-08-11 09:26:16,147:CRITICAL:EmoV2_step40: validate. Iteration: 140000: Loss: nan
2018-08-11 09:26:29,203:CRITICAL:EmoV2_step40: iteration: 140100: Loss: 0.404419, lr: 0.000819
2018-08-11 09:26:42,260:CRITICAL:EmoV2_step40: iteration: 140200: Loss: 0.303715, lr: 0.000818
2018-08-11 09:26:55,334:CRITICAL:EmoV2_step40: iteration: 140300: Loss: 0.579536, lr: 0.000817
2018-08-11 09:27:08,423:CRITICAL:EmoV2_step40: iteration: 140400: Loss: 0.516823, lr: 0.000815
2018-08-11 09:27:21,521:CRITICAL:EmoV2_step40: iteration: 140500: Loss: 0.403075, lr: 0.000814
2018-08-11 09:27:34,628:CRITICAL:EmoV2_step40: iteration: 140600: Loss: 0.511021, lr: 0.000813
2018-08-11 09:27:47,746:CRITICAL:EmoV2_step40: iteration: 140700: Loss: 0.618063, lr: 0.000812
2018-08-11 09:28:00,846:CRITICAL:EmoV2_step40: iteration: 140800: Loss: 0.351318, lr: 0.000811
2018-08-11 09:28:13,979:CRITICAL:EmoV2_step40: iteration: 140900: Loss: 0.516883, lr: 0.000809
2018-08-11 09:28:27,145:CRITICAL:EmoV2_step40: iteration: 141000: Loss: 0.521893, lr: 0.000808
2018-08-11 09:28:31,763:CRITICAL:EmoV2_step40: validate. Iteration: 141000: Accuracy (valence, arousal): 31.416% 20.354%
2018-08-11 09:28:31,763:CRITICAL:EmoV2_step40: validate. Iteration: 141000: Loss: nan
2018-08-11 09:28:49,318:CRITICAL:EmoV2_step40: iteration: 141100: Loss: 0.760411, lr: 0.000807
2018-08-11 09:29:02,480:CRITICAL:EmoV2_step40: iteration: 141200: Loss: 0.399765, lr: 0.000806
2018-08-11 09:29:15,640:CRITICAL:EmoV2_step40: iteration: 141300: Loss: 0.359331, lr: 0.000805
2018-08-11 09:29:28,788:CRITICAL:EmoV2_step40: iteration: 141400: Loss: 0.389628, lr: 0.000803
2018-08-11 09:29:41,942:CRITICAL:EmoV2_step40: iteration: 141500: Loss: 0.652727, lr: 0.000802
2018-08-11 09:29:55,098:CRITICAL:EmoV2_step40: iteration: 141600: Loss: 0.633109, lr: 0.000801
2018-08-11 09:30:08,251:CRITICAL:EmoV2_step40: iteration: 141700: Loss: 0.479921, lr: 0.000800
2018-08-11 09:30:25,999:CRITICAL:EmoV2_step40: iteration: 141800: Loss: 0.984847, lr: 0.000799
2018-08-11 09:30:39,146:CRITICAL:EmoV2_step40: iteration: 141900: Loss: 0.760413, lr: 0.000797
2018-08-11 09:30:52,291:CRITICAL:EmoV2_step40: iteration: 142000: Loss: 0.695564, lr: 0.000796
2018-08-11 09:30:56,794:CRITICAL:EmoV2_step40: validate. Iteration: 142000: Accuracy (valence, arousal): 33.186% 19.027%
2018-08-11 09:30:56,795:CRITICAL:EmoV2_step40: validate. Iteration: 142000: Loss: nan
2018-08-11 09:31:09,940:CRITICAL:EmoV2_step40: iteration: 142100: Loss: 0.485370, lr: 0.000795
2018-08-11 09:31:23,082:CRITICAL:EmoV2_step40: iteration: 142200: Loss: 0.586664, lr: 0.000794
2018-08-11 09:31:36,235:CRITICAL:EmoV2_step40: iteration: 142300: Loss: 0.324241, lr: 0.000793
2018-08-11 09:31:49,392:CRITICAL:EmoV2_step40: iteration: 142400: Loss: 0.624291, lr: 0.000791
2018-08-11 09:32:02,549:CRITICAL:EmoV2_step40: iteration: 142500: Loss: 0.250429, lr: 0.000790
2018-08-11 09:32:15,704:CRITICAL:EmoV2_step40: iteration: 142600: Loss: 0.581873, lr: 0.000789
2018-08-11 09:32:28,850:CRITICAL:EmoV2_step40: iteration: 142700: Loss: 0.611821, lr: 0.000788
2018-08-11 09:32:46,398:CRITICAL:EmoV2_step40: iteration: 142800: Loss: 0.527593, lr: 0.000787
2018-08-11 09:32:59,565:CRITICAL:EmoV2_step40: iteration: 142900: Loss: 0.416610, lr: 0.000785
2018-08-11 09:33:12,725:CRITICAL:EmoV2_step40: iteration: 143000: Loss: 0.688758, lr: 0.000784
2018-08-11 09:33:17,168:CRITICAL:EmoV2_step40: validate. Iteration: 143000: Accuracy (valence, arousal): 32.743% 18.584%
2018-08-11 09:33:17,168:CRITICAL:EmoV2_step40: validate. Iteration: 143000: Loss: nan
2018-08-11 09:33:30,327:CRITICAL:EmoV2_step40: iteration: 143100: Loss: 0.588721, lr: 0.000783
2018-08-11 09:33:43,481:CRITICAL:EmoV2_step40: iteration: 143200: Loss: 0.298107, lr: 0.000782
2018-08-11 09:33:56,635:CRITICAL:EmoV2_step40: iteration: 143300: Loss: 0.510215, lr: 0.000781
2018-08-11 09:34:09,795:CRITICAL:EmoV2_step40: iteration: 143400: Loss: 0.699942, lr: 0.000779
2018-08-11 09:34:22,963:CRITICAL:EmoV2_step40: iteration: 143500: Loss: 0.455492, lr: 0.000778
2018-08-11 09:34:36,134:CRITICAL:EmoV2_step40: iteration: 143600: Loss: 0.274077, lr: 0.000777
2018-08-11 09:34:49,296:CRITICAL:EmoV2_step40: iteration: 143700: Loss: 0.243815, lr: 0.000776
2018-08-11 09:35:02,439:CRITICAL:EmoV2_step40: iteration: 143800: Loss: 0.662831, lr: 0.000775
2018-08-11 09:35:15,606:CRITICAL:EmoV2_step40: iteration: 143900: Loss: 0.379027, lr: 0.000773
2018-08-11 09:35:28,752:CRITICAL:EmoV2_step40: iteration: 144000: Loss: 0.449393, lr: 0.000772
2018-08-11 09:35:33,440:CRITICAL:EmoV2_step40: validate. Iteration: 144000: Accuracy (valence, arousal): 31.858% 17.699%
2018-08-11 09:35:33,441:CRITICAL:EmoV2_step40: validate. Iteration: 144000: Loss: nan
2018-08-11 09:35:46,591:CRITICAL:EmoV2_step40: iteration: 144100: Loss: 0.516861, lr: 0.000771
2018-08-11 09:35:59,739:CRITICAL:EmoV2_step40: iteration: 144200: Loss: 0.219347, lr: 0.000770
2018-08-11 09:36:17,475:CRITICAL:EmoV2_step40: iteration: 144300: Loss: 0.682223, lr: 0.000769
2018-08-11 09:36:35,004:CRITICAL:EmoV2_step40: iteration: 144400: Loss: 0.803176, lr: 0.000768
2018-08-11 09:36:48,149:CRITICAL:EmoV2_step40: iteration: 144500: Loss: 0.382849, lr: 0.000766
2018-08-11 09:37:01,288:CRITICAL:EmoV2_step40: iteration: 144600: Loss: 0.433065, lr: 0.000765
2018-08-11 09:37:14,426:CRITICAL:EmoV2_step40: iteration: 144700: Loss: 0.649291, lr: 0.000764
2018-08-11 09:37:27,575:CRITICAL:EmoV2_step40: iteration: 144800: Loss: 0.376414, lr: 0.000763
2018-08-11 09:37:40,741:CRITICAL:EmoV2_step40: iteration: 144900: Loss: 0.543090, lr: 0.000762
2018-08-11 09:37:53,882:CRITICAL:EmoV2_step40: iteration: 145000: Loss: 0.483090, lr: 0.000760
2018-08-11 09:37:58,332:CRITICAL:EmoV2_step40: validate. Iteration: 145000: Accuracy (valence, arousal): 32.743% 15.487%
2018-08-11 09:37:58,332:CRITICAL:EmoV2_step40: validate. Iteration: 145000: Loss: nan
2018-08-11 09:38:11,477:CRITICAL:EmoV2_step40: iteration: 145100: Loss: 0.510598, lr: 0.000759
2018-08-11 09:38:24,636:CRITICAL:EmoV2_step40: iteration: 145200: Loss: 1.061889, lr: 0.000758
2018-08-11 09:38:37,814:CRITICAL:EmoV2_step40: iteration: 145300: Loss: 0.490589, lr: 0.000757
2018-08-11 09:38:50,969:CRITICAL:EmoV2_step40: iteration: 145400: Loss: 0.609449, lr: 0.000756
2018-08-11 09:39:04,125:CRITICAL:EmoV2_step40: iteration: 145500: Loss: 0.529232, lr: 0.000754
2018-08-11 09:39:17,275:CRITICAL:EmoV2_step40: iteration: 145600: Loss: 0.417782, lr: 0.000753
2018-08-11 09:39:30,420:CRITICAL:EmoV2_step40: iteration: 145700: Loss: 0.465842, lr: 0.000752
2018-08-11 09:39:43,573:CRITICAL:EmoV2_step40: iteration: 145800: Loss: 0.284317, lr: 0.000751
2018-08-11 09:39:56,719:CRITICAL:EmoV2_step40: iteration: 145900: Loss: 0.612072, lr: 0.000750
2018-08-11 09:40:09,866:CRITICAL:EmoV2_step40: iteration: 146000: Loss: 0.413113, lr: 0.000748
2018-08-11 09:40:14,377:CRITICAL:EmoV2_step40: validate. Iteration: 146000: Accuracy (valence, arousal): 31.416% 19.912%
2018-08-11 09:40:14,377:CRITICAL:EmoV2_step40: validate. Iteration: 146000: Loss: nan
2018-08-11 09:40:31,917:CRITICAL:EmoV2_step40: iteration: 146100: Loss: 0.402997, lr: 0.000747
2018-08-11 09:40:45,062:CRITICAL:EmoV2_step40: iteration: 146200: Loss: 0.758282, lr: 0.000746
2018-08-11 09:40:58,210:CRITICAL:EmoV2_step40: iteration: 146300: Loss: 0.393749, lr: 0.000745
2018-08-11 09:41:11,356:CRITICAL:EmoV2_step40: iteration: 146400: Loss: 0.512090, lr: 0.000744
2018-08-11 09:41:24,494:CRITICAL:EmoV2_step40: iteration: 146500: Loss: 0.401516, lr: 0.000742
2018-08-11 09:41:37,632:CRITICAL:EmoV2_step40: iteration: 146600: Loss: 0.652932, lr: 0.000741
2018-08-11 09:41:50,785:CRITICAL:EmoV2_step40: iteration: 146700: Loss: 0.405597, lr: 0.000740
2018-08-11 09:42:08,575:CRITICAL:EmoV2_step40: iteration: 146800: Loss: 0.511633, lr: 0.000739
2018-08-11 09:42:21,712:CRITICAL:EmoV2_step40: iteration: 146900: Loss: 0.613082, lr: 0.000738
2018-08-11 09:42:34,857:CRITICAL:EmoV2_step40: iteration: 147000: Loss: 0.329149, lr: 0.000736
2018-08-11 09:42:39,288:CRITICAL:EmoV2_step40: validate. Iteration: 147000: Accuracy (valence, arousal): 34.956% 17.699%
2018-08-11 09:42:39,289:CRITICAL:EmoV2_step40: validate. Iteration: 147000: Loss: nan
2018-08-11 09:42:52,448:CRITICAL:EmoV2_step40: iteration: 147100: Loss: 0.292723, lr: 0.000735
2018-08-11 09:43:05,587:CRITICAL:EmoV2_step40: iteration: 147200: Loss: 0.374896, lr: 0.000734
2018-08-11 09:43:18,731:CRITICAL:EmoV2_step40: iteration: 147300: Loss: 0.680860, lr: 0.000733
2018-08-11 09:43:31,885:CRITICAL:EmoV2_step40: iteration: 147400: Loss: 0.231550, lr: 0.000732
2018-08-11 09:43:45,027:CRITICAL:EmoV2_step40: iteration: 147500: Loss: 0.619567, lr: 0.000730
2018-08-11 09:43:58,165:CRITICAL:EmoV2_step40: iteration: 147600: Loss: 0.509315, lr: 0.000729
2018-08-11 09:44:15,696:CRITICAL:EmoV2_step40: iteration: 147700: Loss: 0.375746, lr: 0.000728
2018-08-11 09:44:28,843:CRITICAL:EmoV2_step40: iteration: 147800: Loss: 0.437712, lr: 0.000727
2018-08-11 09:44:41,982:CRITICAL:EmoV2_step40: iteration: 147900: Loss: 0.446754, lr: 0.000726
2018-08-11 09:44:55,118:CRITICAL:EmoV2_step40: iteration: 148000: Loss: 0.822110, lr: 0.000724
2018-08-11 09:44:59,699:CRITICAL:EmoV2_step40: validate. Iteration: 148000: Accuracy (valence, arousal): 33.628% 18.142%
2018-08-11 09:44:59,700:CRITICAL:EmoV2_step40: validate. Iteration: 148000: Loss: nan
2018-08-11 09:45:12,850:CRITICAL:EmoV2_step40: iteration: 148100: Loss: 0.578616, lr: 0.000723
2018-08-11 09:45:25,996:CRITICAL:EmoV2_step40: iteration: 148200: Loss: 0.707044, lr: 0.000722
2018-08-11 09:45:39,166:CRITICAL:EmoV2_step40: iteration: 148300: Loss: 0.856431, lr: 0.000721
2018-08-11 09:45:52,328:CRITICAL:EmoV2_step40: iteration: 148400: Loss: 0.393439, lr: 0.000720
2018-08-11 09:46:05,485:CRITICAL:EmoV2_step40: iteration: 148500: Loss: 0.677349, lr: 0.000718
2018-08-11 09:46:18,637:CRITICAL:EmoV2_step40: iteration: 148600: Loss: 0.319982, lr: 0.000717
2018-08-11 09:46:31,781:CRITICAL:EmoV2_step40: iteration: 148700: Loss: 0.258097, lr: 0.000716
2018-08-11 09:46:44,931:CRITICAL:EmoV2_step40: iteration: 148800: Loss: 0.975963, lr: 0.000715
2018-08-11 09:46:58,084:CRITICAL:EmoV2_step40: iteration: 148900: Loss: 0.341147, lr: 0.000714
2018-08-11 09:47:11,229:CRITICAL:EmoV2_step40: iteration: 149000: Loss: 0.365996, lr: 0.000712
2018-08-11 09:47:15,762:CRITICAL:EmoV2_step40: validate. Iteration: 149000: Accuracy (valence, arousal): 30.531% 17.257%
2018-08-11 09:47:15,763:CRITICAL:EmoV2_step40: validate. Iteration: 149000: Loss: nan
2018-08-11 09:47:28,932:CRITICAL:EmoV2_step40: iteration: 149100: Loss: 0.460987, lr: 0.000711
2018-08-11 09:47:42,094:CRITICAL:EmoV2_step40: iteration: 149200: Loss: 0.587952, lr: 0.000710
2018-08-11 09:47:59,861:CRITICAL:EmoV2_step40: iteration: 149300: Loss: 0.362389, lr: 0.000709
2018-08-11 09:48:17,408:CRITICAL:EmoV2_step40: iteration: 149400: Loss: 0.501901, lr: 0.000708
2018-08-11 09:48:30,537:CRITICAL:EmoV2_step40: iteration: 149500: Loss: 0.356743, lr: 0.000706
2018-08-11 09:48:43,673:CRITICAL:EmoV2_step40: iteration: 149600: Loss: 0.343415, lr: 0.000705
2018-08-11 09:48:56,826:CRITICAL:EmoV2_step40: iteration: 149700: Loss: 0.579473, lr: 0.000704
2018-08-11 09:49:09,963:CRITICAL:EmoV2_step40: iteration: 149800: Loss: 0.306207, lr: 0.000703
2018-08-11 09:49:23,107:CRITICAL:EmoV2_step40: iteration: 149900: Loss: 0.617077, lr: 0.000702
2018-08-11 09:50:18,253:CRITICAL:EmoV2_step40: iteration: 150000: Loss: 0.353226, lr: 0.000700
2018-08-11 09:50:23,239:CRITICAL:EmoV2_step40: validate. Iteration: 150000: Accuracy (valence, arousal): 30.531% 17.699%
2018-08-11 09:50:23,239:CRITICAL:EmoV2_step40: validate. Iteration: 150000: Loss: nan
2018-08-11 09:50:36,298:CRITICAL:EmoV2_step40: iteration: 150100: Loss: 0.632601, lr: 0.000699
2018-08-11 09:50:49,347:CRITICAL:EmoV2_step40: iteration: 150200: Loss: 0.248650, lr: 0.000698
2018-08-11 09:51:02,421:CRITICAL:EmoV2_step40: iteration: 150300: Loss: 0.536436, lr: 0.000697
2018-08-11 09:51:15,508:CRITICAL:EmoV2_step40: iteration: 150400: Loss: 0.272624, lr: 0.000696
2018-08-11 09:51:28,601:CRITICAL:EmoV2_step40: iteration: 150500: Loss: 0.406990, lr: 0.000694
2018-08-11 09:51:41,690:CRITICAL:EmoV2_step40: iteration: 150600: Loss: 0.362639, lr: 0.000693
2018-08-11 09:51:54,792:CRITICAL:EmoV2_step40: iteration: 150700: Loss: 0.761927, lr: 0.000692
2018-08-11 09:52:07,893:CRITICAL:EmoV2_step40: iteration: 150800: Loss: 0.492212, lr: 0.000691
2018-08-11 09:52:21,042:CRITICAL:EmoV2_step40: iteration: 150900: Loss: 0.397969, lr: 0.000690
2018-08-11 09:52:38,583:CRITICAL:EmoV2_step40: iteration: 151000: Loss: 0.727015, lr: 0.000688
2018-08-11 09:52:43,226:CRITICAL:EmoV2_step40: validate. Iteration: 151000: Accuracy (valence, arousal): 31.858% 19.912%
2018-08-11 09:52:43,226:CRITICAL:EmoV2_step40: validate. Iteration: 151000: Loss: nan
2018-08-11 09:52:56,373:CRITICAL:EmoV2_step40: iteration: 151100: Loss: 0.626631, lr: 0.000687
2018-08-11 09:53:09,518:CRITICAL:EmoV2_step40: iteration: 151200: Loss: 0.314523, lr: 0.000686
2018-08-11 09:53:22,673:CRITICAL:EmoV2_step40: iteration: 151300: Loss: 0.397690, lr: 0.000685
2018-08-11 09:53:35,820:CRITICAL:EmoV2_step40: iteration: 151400: Loss: 0.349018, lr: 0.000684
2018-08-11 09:53:48,976:CRITICAL:EmoV2_step40: iteration: 151500: Loss: 0.321642, lr: 0.000682
2018-08-11 09:54:02,124:CRITICAL:EmoV2_step40: iteration: 151600: Loss: 0.745626, lr: 0.000681
2018-08-11 09:54:15,276:CRITICAL:EmoV2_step40: iteration: 151700: Loss: 0.794454, lr: 0.000680
2018-08-11 09:54:33,071:CRITICAL:EmoV2_step40: iteration: 151800: Loss: 0.669216, lr: 0.000679
2018-08-11 09:54:46,224:CRITICAL:EmoV2_step40: iteration: 151900: Loss: 0.656474, lr: 0.000678
2018-08-11 09:54:59,368:CRITICAL:EmoV2_step40: iteration: 152000: Loss: 0.499214, lr: 0.000676
2018-08-11 09:55:04,023:CRITICAL:EmoV2_step40: validate. Iteration: 152000: Accuracy (valence, arousal): 33.628% 18.584%
2018-08-11 09:55:04,024:CRITICAL:EmoV2_step40: validate. Iteration: 152000: Loss: nan
2018-08-11 09:55:17,163:CRITICAL:EmoV2_step40: iteration: 152100: Loss: 0.478935, lr: 0.000675
2018-08-11 09:55:30,311:CRITICAL:EmoV2_step40: iteration: 152200: Loss: 0.391072, lr: 0.000674
2018-08-11 09:55:43,456:CRITICAL:EmoV2_step40: iteration: 152300: Loss: 0.307765, lr: 0.000673
2018-08-11 09:55:56,602:CRITICAL:EmoV2_step40: iteration: 152400: Loss: 0.360272, lr: 0.000672
2018-08-11 09:56:09,755:CRITICAL:EmoV2_step40: iteration: 152500: Loss: 0.622813, lr: 0.000670
2018-08-11 09:56:22,910:CRITICAL:EmoV2_step40: iteration: 152600: Loss: 0.427964, lr: 0.000669
2018-08-11 09:56:40,429:CRITICAL:EmoV2_step40: iteration: 152700: Loss: 0.754331, lr: 0.000668
2018-08-11 09:56:53,577:CRITICAL:EmoV2_step40: iteration: 152800: Loss: 0.429293, lr: 0.000667
2018-08-11 09:57:06,742:CRITICAL:EmoV2_step40: iteration: 152900: Loss: 0.522204, lr: 0.000666
2018-08-11 09:57:19,909:CRITICAL:EmoV2_step40: iteration: 153000: Loss: 0.287676, lr: 0.000664
2018-08-11 09:57:24,731:CRITICAL:EmoV2_step40: validate. Iteration: 153000: Accuracy (valence, arousal): 30.088% 19.469%
2018-08-11 09:57:24,732:CRITICAL:EmoV2_step40: validate. Iteration: 153000: Loss: nan
2018-08-11 09:57:37,890:CRITICAL:EmoV2_step40: iteration: 153100: Loss: 0.568577, lr: 0.000663
2018-08-11 09:57:51,043:CRITICAL:EmoV2_step40: iteration: 153200: Loss: 0.302069, lr: 0.000662
2018-08-11 09:58:04,191:CRITICAL:EmoV2_step40: iteration: 153300: Loss: 0.442364, lr: 0.000661
2018-08-11 09:58:17,343:CRITICAL:EmoV2_step40: iteration: 153400: Loss: 0.397178, lr: 0.000660
2018-08-11 09:58:30,491:CRITICAL:EmoV2_step40: iteration: 153500: Loss: 0.357798, lr: 0.000658
2018-08-11 09:58:43,643:CRITICAL:EmoV2_step40: iteration: 153600: Loss: 0.731825, lr: 0.000657
2018-08-11 09:58:56,788:CRITICAL:EmoV2_step40: iteration: 153700: Loss: 0.604526, lr: 0.000656
2018-08-11 09:59:09,934:CRITICAL:EmoV2_step40: iteration: 153800: Loss: 0.264524, lr: 0.000655
2018-08-11 09:59:23,077:CRITICAL:EmoV2_step40: iteration: 153900: Loss: 0.399408, lr: 0.000654
2018-08-11 09:59:36,247:CRITICAL:EmoV2_step40: iteration: 154000: Loss: 0.380121, lr: 0.000652
2018-08-11 09:59:40,840:CRITICAL:EmoV2_step40: validate. Iteration: 154000: Accuracy (valence, arousal): 32.301% 19.912%
2018-08-11 09:59:40,840:CRITICAL:EmoV2_step40: validate. Iteration: 154000: Loss: nan
2018-08-11 09:59:53,998:CRITICAL:EmoV2_step40: iteration: 154100: Loss: 0.462215, lr: 0.000651
2018-08-11 10:00:07,164:CRITICAL:EmoV2_step40: iteration: 154200: Loss: 0.402379, lr: 0.000650
2018-08-11 10:00:29,325:CRITICAL:EmoV2_step40: iteration: 154300: Loss: 0.514463, lr: 0.000649
2018-08-11 10:00:42,464:CRITICAL:EmoV2_step40: iteration: 154400: Loss: 0.366712, lr: 0.000648
2018-08-11 10:00:55,599:CRITICAL:EmoV2_step40: iteration: 154500: Loss: 0.658128, lr: 0.000646
2018-08-11 10:01:08,741:CRITICAL:EmoV2_step40: iteration: 154600: Loss: 0.362452, lr: 0.000645
2018-08-11 10:01:21,887:CRITICAL:EmoV2_step40: iteration: 154700: Loss: 0.446077, lr: 0.000644
2018-08-11 10:01:35,025:CRITICAL:EmoV2_step40: iteration: 154800: Loss: 0.378195, lr: 0.000643
2018-08-11 10:01:48,164:CRITICAL:EmoV2_step40: iteration: 154900: Loss: 0.533319, lr: 0.000642
2018-08-11 10:02:01,324:CRITICAL:EmoV2_step40: iteration: 155000: Loss: 0.785430, lr: 0.000640
2018-08-11 10:02:05,929:CRITICAL:EmoV2_step40: validate. Iteration: 155000: Accuracy (valence, arousal): 31.858% 20.354%
2018-08-11 10:02:05,929:CRITICAL:EmoV2_step40: validate. Iteration: 155000: Loss: nan
2018-08-11 10:02:19,083:CRITICAL:EmoV2_step40: iteration: 155100: Loss: 0.465084, lr: 0.000639
2018-08-11 10:02:32,232:CRITICAL:EmoV2_step40: iteration: 155200: Loss: 0.637333, lr: 0.000638
2018-08-11 10:02:45,393:CRITICAL:EmoV2_step40: iteration: 155300: Loss: 0.490879, lr: 0.000637
2018-08-11 10:02:58,543:CRITICAL:EmoV2_step40: iteration: 155400: Loss: 0.385702, lr: 0.000636
2018-08-11 10:03:11,695:CRITICAL:EmoV2_step40: iteration: 155500: Loss: 0.558411, lr: 0.000634
2018-08-11 10:03:24,844:CRITICAL:EmoV2_step40: iteration: 155600: Loss: 0.327960, lr: 0.000633
2018-08-11 10:03:37,988:CRITICAL:EmoV2_step40: iteration: 155700: Loss: 0.657122, lr: 0.000632
2018-08-11 10:03:51,150:CRITICAL:EmoV2_step40: iteration: 155800: Loss: 0.419823, lr: 0.000631
2018-08-11 10:04:04,301:CRITICAL:EmoV2_step40: iteration: 155900: Loss: 0.515593, lr: 0.000630
2018-08-11 10:04:21,833:CRITICAL:EmoV2_step40: iteration: 156000: Loss: 0.620368, lr: 0.000628
2018-08-11 10:04:26,398:CRITICAL:EmoV2_step40: validate. Iteration: 156000: Accuracy (valence, arousal): 33.628% 17.699%
2018-08-11 10:04:26,399:CRITICAL:EmoV2_step40: validate. Iteration: 156000: Loss: nan
2018-08-11 10:04:39,544:CRITICAL:EmoV2_step40: iteration: 156100: Loss: 0.389253, lr: 0.000627
2018-08-11 10:04:52,702:CRITICAL:EmoV2_step40: iteration: 156200: Loss: 0.276798, lr: 0.000626
2018-08-11 10:05:05,845:CRITICAL:EmoV2_step40: iteration: 156300: Loss: 0.363317, lr: 0.000625
2018-08-11 10:05:18,991:CRITICAL:EmoV2_step40: iteration: 156400: Loss: 0.651157, lr: 0.000624
2018-08-11 10:05:32,146:CRITICAL:EmoV2_step40: iteration: 156500: Loss: 0.504627, lr: 0.000622
2018-08-11 10:05:45,296:CRITICAL:EmoV2_step40: iteration: 156600: Loss: 0.462410, lr: 0.000621
2018-08-11 10:05:58,436:CRITICAL:EmoV2_step40: iteration: 156700: Loss: 0.518459, lr: 0.000620
2018-08-11 10:06:16,154:CRITICAL:EmoV2_step40: iteration: 156800: Loss: 0.335612, lr: 0.000619
2018-08-11 10:06:29,307:CRITICAL:EmoV2_step40: iteration: 156900: Loss: 0.655170, lr: 0.000618
2018-08-11 10:06:42,463:CRITICAL:EmoV2_step40: iteration: 157000: Loss: 0.336661, lr: 0.000617
2018-08-11 10:06:46,969:CRITICAL:EmoV2_step40: validate. Iteration: 157000: Accuracy (valence, arousal): 30.088% 18.142%
2018-08-11 10:06:46,969:CRITICAL:EmoV2_step40: validate. Iteration: 157000: Loss: nan
2018-08-11 10:07:00,123:CRITICAL:EmoV2_step40: iteration: 157100: Loss: 0.338953, lr: 0.000615
2018-08-11 10:07:13,283:CRITICAL:EmoV2_step40: iteration: 157200: Loss: 0.348796, lr: 0.000614
2018-08-11 10:07:26,429:CRITICAL:EmoV2_step40: iteration: 157300: Loss: 0.572642, lr: 0.000613
2018-08-11 10:07:39,574:CRITICAL:EmoV2_step40: iteration: 157400: Loss: 0.406478, lr: 0.000612
2018-08-11 10:07:52,710:CRITICAL:EmoV2_step40: iteration: 157500: Loss: 0.420582, lr: 0.000611
2018-08-11 10:08:10,219:CRITICAL:EmoV2_step40: iteration: 157600: Loss: 0.673741, lr: 0.000609
2018-08-11 10:08:23,366:CRITICAL:EmoV2_step40: iteration: 157700: Loss: 0.328613, lr: 0.000608
2018-08-11 10:08:36,521:CRITICAL:EmoV2_step40: iteration: 157800: Loss: 0.345188, lr: 0.000607
2018-08-11 10:08:49,673:CRITICAL:EmoV2_step40: iteration: 157900: Loss: 0.508461, lr: 0.000606
2018-08-11 10:09:02,824:CRITICAL:EmoV2_step40: iteration: 158000: Loss: 0.495875, lr: 0.000605
2018-08-11 10:09:07,365:CRITICAL:EmoV2_step40: validate. Iteration: 158000: Accuracy (valence, arousal): 32.743% 18.142%
2018-08-11 10:09:07,365:CRITICAL:EmoV2_step40: validate. Iteration: 158000: Loss: nan
2018-08-11 10:09:20,515:CRITICAL:EmoV2_step40: iteration: 158100: Loss: 0.345424, lr: 0.000603
2018-08-11 10:09:33,661:CRITICAL:EmoV2_step40: iteration: 158200: Loss: 0.458081, lr: 0.000602
2018-08-11 10:09:46,812:CRITICAL:EmoV2_step40: iteration: 158300: Loss: 0.884962, lr: 0.000601
2018-08-11 10:09:59,971:CRITICAL:EmoV2_step40: iteration: 158400: Loss: 0.344768, lr: 0.000600
2018-08-11 10:10:13,129:CRITICAL:EmoV2_step40: iteration: 158500: Loss: 0.503627, lr: 0.000599
2018-08-11 10:10:26,293:CRITICAL:EmoV2_step40: iteration: 158600: Loss: 0.718711, lr: 0.000597
2018-08-11 10:10:39,446:CRITICAL:EmoV2_step40: iteration: 158700: Loss: 0.561495, lr: 0.000596
2018-08-11 10:10:52,592:CRITICAL:EmoV2_step40: iteration: 158800: Loss: 0.471984, lr: 0.000595
2018-08-11 10:11:05,744:CRITICAL:EmoV2_step40: iteration: 158900: Loss: 0.520429, lr: 0.000594
2018-08-11 10:11:18,891:CRITICAL:EmoV2_step40: iteration: 159000: Loss: 0.577551, lr: 0.000593
2018-08-11 10:11:23,596:CRITICAL:EmoV2_step40: validate. Iteration: 159000: Accuracy (valence, arousal): 31.858% 20.354%
2018-08-11 10:11:23,597:CRITICAL:EmoV2_step40: validate. Iteration: 159000: Loss: nan
2018-08-11 10:11:36,758:CRITICAL:EmoV2_step40: iteration: 159100: Loss: 0.379053, lr: 0.000591
2018-08-11 10:11:54,317:CRITICAL:EmoV2_step40: iteration: 159200: Loss: 0.856704, lr: 0.000590
2018-08-11 10:12:12,030:CRITICAL:EmoV2_step40: iteration: 159300: Loss: 0.374268, lr: 0.000589
2018-08-11 10:12:25,164:CRITICAL:EmoV2_step40: iteration: 159400: Loss: 0.279757, lr: 0.000588
2018-08-11 10:12:38,298:CRITICAL:EmoV2_step40: iteration: 159500: Loss: 0.376223, lr: 0.000587
2018-08-11 10:12:51,454:CRITICAL:EmoV2_step40: iteration: 159600: Loss: 0.541836, lr: 0.000585
2018-08-11 10:13:04,604:CRITICAL:EmoV2_step40: iteration: 159700: Loss: 0.305254, lr: 0.000584
